{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75427881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from src.roma_dspy.modules import Executor, Atomizer, Planner, Aggregator\n",
    "from src.roma_dspy.engine.solve import RecursiveSolver\n",
    "from src.roma_dspy.engine.visualizer import ExecutionVisualizer\n",
    "from src.roma_dspy.engine.solve import event_solve, solve\n",
    "\n",
    "executor_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "atomizer_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "planner_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "aggregator_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "\n",
    "# Initialize modules\n",
    "atomizer = Atomizer(lm=atomizer_lm)\n",
    "planner = Planner(lm=planner_lm)\n",
    "executor = Executor(lm=executor_lm)\n",
    "aggregator = Aggregator(lm=aggregator_lm)\n",
    "\n",
    "# Create solver\n",
    "solver = RecursiveSolver(\n",
    "    atomizer,\n",
    "    planner,\n",
    "    executor,\n",
    "    aggregator,\n",
    "    max_depth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test task - use a simpler task for clearer output\n",
    "task_goal = \"Write me a blog post about the benefits of using DSPy.\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ROMA-DSPy Detailed Execution Visualization\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüéØ Goal: {task_goal}\\n\")\n",
    "\n",
    "# Execute task\n",
    "print(\"‚öôÔ∏è  Executing task...\\n\")\n",
    "result = solver.solve(task_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DAG for detailed visualization\n",
    "dag = solver.last_dag\n",
    "\n",
    "if dag:\n",
    "    # Show the full execution report\n",
    "    visualizer = ExecutionVisualizer()\n",
    "    print(visualizer.get_full_execution_report(result, dag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6705477",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(visualizer.get_execution_tree_with_details(result, dag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07946a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROMA-DSPy Detailed Execution Visualization\n",
      "================================================================================\n",
      "\n",
      "üéØ Goal: Write me a blog post about the benefits of using DSPy.\n",
      "\n",
      "‚öôÔ∏è  Executing task...\n",
      "\n",
      "\n",
      "‚úÖ Result: # Unlocking LLM Potential: Why DSPy is a Game-Changer for AI Development\n",
      "\n",
      "## I. Introduction\n",
      "\n",
      "Are you tired of wrestling with complex prompt engineering, struggling to get consistent and reliable outputs from your Large Language Models (LLMs)? What if there was a way to program LLMs more systematically, like traditional software, and achieve significantly better results with less effort?\n",
      "\n",
      "Enter DSPy, a groundbreaking framework designed to optimize and program LLMs. Unlike traditional prompt engineering, which often relies on trial-and-error, DSPy treats LLMs as components within a larger program, allowing developers to compose, compile, and optimize complex LLM pipelines. It provides a systematic approach to building robust and efficient LLM-powered applications, moving beyond simple prompting to a more structured and performant paradigm.\n",
      "\n",
      "In an era where LLMs are becoming central to countless applications, the ability to reliably and efficiently develop with them is paramount. DSPy addresses this critical need by offering a principled way to improve LLM performance, reduce development time, and build more scalable and maintainable AI systems. It empowers developers to unlock the full potential of LLMs, transforming them from unpredictable black boxes into programmable, optimizable agents.\n",
      "\n",
      "## II. The Core Benefits of DSPy\n",
      "\n",
      "### 1. Programmatic Control and Structured Development\n",
      "\n",
      "DSPy fundamentally shifts the paradigm from \"prompt engineering\" to \"program engineering.\" It allows you to treat LLMs not as black boxes to be coaxed with magic words, but as functions within a larger, structured program. This means:\n",
      "\n",
      "*   **Traditional Software Engineering Practices:** You can apply familiar software development principles like modularity, abstraction, and version control to your LLM applications.\n",
      "*   **Explicit Modules and Signatures:** DSPy replaces ad-hoc, unstructured prompts with explicit `dspy.Signature` definitions and modular components (like `dspy.Predict` or `dspy.ChainOfThought`). This clearly defines the inputs and outputs of each LLM call, making your LLM interactions readable, maintainable, and testable.\n",
      "*   **Clear Logic:** It allows for a clear definition of inputs, outputs, and intermediate steps within LLM applications, making complex reasoning pipelines easier to understand and debug.\n",
      "\n",
      "### 2. Automatic Optimization and Performance Gains\n",
      "\n",
      "This is where DSPy truly shines. Instead of manually tweaking prompts, DSPy introduces \"optimizers\" (also called Teleprompters or Compilers) that automatically learn how to improve your LLM programs.\n",
      "\n",
      "*   **Automated Prompt and Weight Optimization:** DSPy \"compiles\" LLM programs to automatically optimize prompts, few-shot examples, and even model weights for specific tasks and datasets.\n",
      "*   **Reduced Manual Effort:** It significantly reduces the need for manual, trial-and-error prompt engineering, saving countless hours of development time.\n",
      "*   **Improved Accuracy and Robustness:** By systematically optimizing the program, DSPy improves task accuracy, robustness, and generalization across different LLMs and datasets. It learns to generate better prompts, demonstrations, and reasoning steps automatically, leading to more consistent and reliable outputs.\n",
      "\n",
      "### 3. Modularity and Composable Pipelines\n",
      "\n",
      "DSPy encourages a modular approach to building LLM applications, much like building with LEGO bricks.\n",
      "\n",
      "*   **Break Down Complexity:** It enables breaking down complex LLM tasks into smaller, manageable, and reusable modules. For example, one module might extract entities, another might summarize, and a third might generate a response.\n",
      "*   **Sophisticated Reasoning:** You can easily chain and combine these modules to build sophisticated, multi-step reasoning pipelines, such as a `dspy.ReAct` agent that plans, acts, and observes.\n",
      "*   **Reusability:** This promotes the reusability of LLM components across different applications and projects, simplifying development, debugging, and maintenance.\n",
      "\n",
      "### 4. Systematic Evaluation and Debugging\n",
      "\n",
      "Moving beyond subjective assessments, DSPy provides a robust framework for objective evaluation.\n",
      "\n",
      "*   **Data-Driven Improvement:** It provides a robust framework for defining metrics and systematically evaluating LLM program performance against a dataset.\n",
      "*   **Identify Bottlenecks:** This facilitates data-driven testing and comparison of different LLM approaches and configurations, helping you identify bottlenecks, errors, and areas for improvement within your LLM pipelines.\n",
      "*   **Objective Optimization:** DSPy moves beyond subjective assessment to enable objective, quantifiable optimization of LLM applications, ensuring your improvements are real and measurable.\n",
      "\n",
      "### 5. Cost-Effectiveness and Efficiency\n",
      "\n",
      "Beyond performance, DSPy also offers tangible economic benefits.\n",
      "\n",
      "*   **Reduced API Costs:** Optimized prompts often lead to fewer tokens used per inference, directly reducing API costs, especially for high-volume applications.\n",
      "*   **Less Rework:** Improved accuracy and reliability reduce the need for re-runs, human correction, or fallback mechanisms, saving time and resources.\n",
      "*   **Leverage Smaller Models:** DSPy can enable the effective use of smaller, more cost-efficient LLMs for certain tasks by optimizing their performance to match or even exceed larger models.\n",
      "*   **Accelerated Development:** Most importantly, it significantly reduces development time and effort compared to manual, iterative prompt engineering, allowing teams to build and deploy faster.\n",
      "\n",
      "## III. Conclusion\n",
      "\n",
      "In summary, embracing DSPy offers a transformative path to building robust and efficient LLM applications. It empowers developers to achieve unparalleled performance, make data-driven decisions, and foster a culture of continuous innovation in AI development. By leveraging its capabilities, organizations can not only overcome current challenges in prompt engineering but also secure a significant competitive advantage in an ever-evolving AI landscape.\n",
      "\n",
      "The future of LLM development is here, and it's powered by DSPy. Don't just adapt to the complexities of LLMs; master them. Start your journey towards a more efficient, insightful, and innovative LLM-powered future today.\n"
     ]
    }
   ],
   "source": [
    "# Use async/await pattern - most compatible with Jupyter\n",
    "import asyncio\n",
    "\n",
    "async def run_task():\n",
    "    task_goal = \"Write me a blog post about the benefits of using DSPy.\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ROMA-DSPy Detailed Execution Visualization\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüéØ Goal: {task_goal}\\n\")\n",
    "\n",
    "    # Execute task\n",
    "    print(\"‚öôÔ∏è  Executing task...\\n\")\n",
    "    result = await solver.async_event_solve(task_goal, concurrency=8)\n",
    "\n",
    "    print(f\"\\n‚úÖ Result: {result.result}\")\n",
    "    return result\n",
    "\n",
    "# Run it\n",
    "new_result = await run_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba29d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
