{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75427881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roma_dspy import RecursiveSolverModule, RecursiveSolver, Executor, Atomizer, Planner, Aggregator\n",
    "import dspy \n",
    "\n",
    "executor_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "atomizer_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "planner_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "aggregator_lm = dspy.LM(\"openrouter/google/gemini-2.5-flash\", cache=False)\n",
    "\n",
    "# Initialize modules\n",
    "atomizer = Atomizer(lm=atomizer_lm)\n",
    "planner = Planner(lm=planner_lm)\n",
    "executor = Executor(lm=executor_lm)\n",
    "aggregator = Aggregator(lm=aggregator_lm)\n",
    "\n",
    "# Create solver\n",
    "solver = RecursiveSolver(\n",
    "    atomizer,\n",
    "    planner,\n",
    "    executor,\n",
    "    aggregator,\n",
    "    max_depth=2\n",
    ")\n",
    "\n",
    "dspy_module = RecursiveSolverModule(solver=solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_goal = \"Write me a blog post about the benefits of using DSPy.\"\n",
    "\n",
    "root = await dspy_module.aforward(task_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb15237f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m================================================================================\u001b[0m\n",
      "\u001b[1m📊 HIERARCHICAL TASK DECOMPOSITION TREE\u001b[0m\n",
      "\u001b[36m================================================================================\u001b[0m\n",
      "\n",
      "└── ✅ \u001b[33m[D0/2]\u001b[0m \u001b[94mWrite me a blog post about the benefits of using DSPy.\u001b[0m \u001b[35m📝PLAN\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    ⏱️  \u001b[32m11.08s\u001b[0m\n",
      "    🔍 atomizer: 1.13s \u001b[36m[291/86 tokens, $0.000302]\u001b[0m\n",
      "    🔄 aggregator: 9.95s \u001b[36m[3096/1803 tokens, $0.005436]\u001b[0m\n",
      "    \u001b[93m💰 Node Total: 5276 tokens, $0.005739\u001b[0m\n",
      "    ├── ✅ \u001b[33m[D1/2]\u001b[0m \u001b[94mResearch DSPy to understand its core concepts, functional...\u001b[0m \u001b[35m📝PLAN\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   ⏱️  \u001b[32m6.22s\u001b[0m\n",
      "    │   🔍 atomizer: 1.04s \u001b[36m[297/135 tokens, $0.000427]\u001b[0m\n",
      "    │   🔄 aggregator: 5.18s \u001b[36m[2922/979 tokens, $0.003324]\u001b[0m\n",
      "    │   \u001b[93m💰 Node Total: 4333 tokens, $0.003751\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mFind the official DSPy documentation and any high-level o...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m4.87s\u001b[0m\n",
      "    │   │   ⚡ executor: 4.87s \u001b[36m[293/432 tokens, $0.001168]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 725 tokens, $0.001168\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mRead through the official DSPy documentation (e.g., 'Intr...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m3.21s\u001b[0m\n",
      "    │   │   ⚡ executor: 3.21s \u001b[36m[320/527 tokens, $0.001414]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 847 tokens, $0.001414\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mIdentify and list the main functionalities offered by DSP...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m4.11s\u001b[0m\n",
      "    │   │   ⚡ executor: 4.11s \u001b[36m[300/656 tokens, $0.001730]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 956 tokens, $0.001730\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mDetermine and articulate the primary benefits of using DS...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m5.71s\u001b[0m\n",
      "    │   │   ⚡ executor: 5.71s \u001b[36m[304/929 tokens, $0.002414]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 1233 tokens, $0.002414\u001b[0m\n",
      "    │   └── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mSynthesize all gathered information on DSPy's core concep...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │       ⏱️  \u001b[32m5.08s\u001b[0m\n",
      "    │       ⚡ executor: 5.08s \u001b[36m[301/1041 tokens, $0.002693]\u001b[0m\n",
      "    │       \u001b[93m💰 Node Total: 1342 tokens, $0.002693\u001b[0m\n",
      "    ├── ✅ \u001b[33m[D1/2]\u001b[0m \u001b[94mCreate a detailed outline for the blog post, including an...\u001b[0m \u001b[35m📝PLAN\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   ⏱️  \u001b[32m9.44s\u001b[0m\n",
      "    │   🔍 atomizer: 1.24s \u001b[36m[314/152 tokens, $0.000474]\u001b[0m\n",
      "    │   🔄 aggregator: 8.21s \u001b[36m[1894/2248 tokens, $0.006188]\u001b[0m\n",
      "    │   \u001b[93m💰 Node Total: 4608 tokens, $0.006662\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mIdentify the core benefits and key features of DSPy that ...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m4.97s\u001b[0m\n",
      "    │   │   ⚡ executor: 4.97s \u001b[36m[299/1013 tokens, $0.002622]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 1312 tokens, $0.002622\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mDraft an introduction for the blog post outline, includin...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m4.77s\u001b[0m\n",
      "    │   │   ⚡ executor: 4.77s \u001b[36m[298/306 tokens, $0.000854]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 604 tokens, $0.000854\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mFor each identified benefit of DSPy, create a dedicated s...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m5.86s\u001b[0m\n",
      "    │   │   ⚡ executor: 5.86s \u001b[36m[314/1288 tokens, $0.003314]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 1602 tokens, $0.003314\u001b[0m\n",
      "    │   ├── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mDraft a conclusion for the blog post outline, summarizing...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │   │   ⏱️  \u001b[32m2.04s\u001b[0m\n",
      "    │   │   ⚡ executor: 2.04s \u001b[36m[302/285 tokens, $0.000803]\u001b[0m\n",
      "    │   │   \u001b[93m💰 Node Total: 587 tokens, $0.000803\u001b[0m\n",
      "    │   └── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mReview and refine the entire blog post outline for clarit...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "    │       ⏱️  \u001b[32m1.14s\u001b[0m\n",
      "    │       ⚡ executor: 1.14s \u001b[36m[313/110 tokens, $0.000369]\u001b[0m\n",
      "    │       \u001b[93m💰 Node Total: 423 tokens, $0.000369\u001b[0m\n",
      "    └── ✅ \u001b[33m[D1/2]\u001b[0m \u001b[94mWrite the full blog post content based on the approved ou...\u001b[0m \u001b[35m📝PLAN\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "        ⏱️  \u001b[32m4.97s\u001b[0m\n",
      "        🔍 atomizer: 1.17s \u001b[36m[304/102 tokens, $0.000346]\u001b[0m\n",
      "        🔄 aggregator: 3.80s \u001b[36m[1396/1090 tokens, $0.003144]\u001b[0m\n",
      "        \u001b[93m💰 Node Total: 2892 tokens, $0.003490\u001b[0m\n",
      "        └── ✅ \u001b[33m[D2/2]\u001b[0m \u001b[94mWrite the full blog post content based on the approved ou...\u001b[0m \u001b[35m⚡EXECUTE\u001b[0m [\u001b[92mCOMPLETED\u001b[0m]\n",
      "            ⏱️  \u001b[32m15.81s\u001b[0m\n",
      "            ⚡ executor: 15.81s \u001b[36m[296/1155 tokens, $0.002976]\u001b[0m\n",
      "            \u001b[93m💰 Node Total: 1451 tokens, $0.002976\u001b[0m\n",
      "\n",
      "\u001b[36m================================================================================\u001b[0m\n",
      "\u001b[1m💰 TREE TOTALS\u001b[0m\n",
      "  Total Prompt Tokens: 17,194\n",
      "  Total Completion Tokens: 22,079\n",
      "  Total Tokens: 39,273\n",
      "\u001b[92m  Total Cost: $0.060356\u001b[0m\n",
      "\n",
      "\u001b[1m📈 EXECUTION STATISTICS\u001b[0m\n",
      "\n",
      "Task Status Distribution:\n",
      "  ✅ COMPLETED: 15\n",
      "\n",
      "Depth Distribution:\n",
      "  Level 0: █ (1 tasks)\n",
      "  Level 1: ███ (3 tasks)\n",
      "  Level 2: ███████████ (11 tasks)\n",
      "\n",
      "Total Tasks: 15\n",
      "Subgraphs Created: 1\n",
      "Execution Complete: ✅ Yes\n"
     ]
    }
   ],
   "source": [
    "from roma_dspy.visualizer import TreeVisualizer, StatisticsVisualizer, HierarchicalVisualizer# Get the DAG for detailed visualization\n",
    "\n",
    "# Show the full execution report\n",
    "tree = TreeVisualizer(use_colors=True, show_timing=True)\n",
    "print(tree.visualize(root.completed_task, dag=dspy_module._solver.last_dag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e6346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    goal='Write me a blog post about the benefits of using DSPy.',\n",
       "    completed_task=TaskNode(task_id='fead3493-f10d-4b3f-9a62-49089c943e27', parent_id=None, goal='Write me a blog post about the benefits of using DSPy.', depth=0, max_depth=2, task_type=<TaskType.THINK: 'THINK'>, node_type=<NodeType.PLAN: 'PLAN'>, status=<TaskStatus.COMPLETED: 'COMPLETED'>, result='# Unlocking LLM Potential: Why DSPy is a Game-Changer for AI Development\\n\\n## I. Introduction\\n\\nAre you tired of wrestling with complex prompt engineering, struggling to get consistent and reliable outputs from your Large Language Models (LLMs)? What if there was a way to program LLMs more systematically, like traditional software, and achieve significantly better results with less effort?\\n\\nEnter DSPy, a groundbreaking framework designed to optimize and program LLMs. Unlike traditional prompt engineering, which often relies on trial-and-error, DSPy treats LLMs as components within a larger program, allowing developers to compose, compile, and optimize complex LLM pipelines. It provides a systematic approach to building robust and efficient LLM-powered applications, moving beyond simple prompting to a more structured and performant paradigm.\\n\\nIn an era where LLMs are becoming central to countless applications, the ability to reliably and efficiently develop with them is paramount. DSPy addresses this critical need by offering a principled way to improve LLM performance, reduce development time, and build more scalable and maintainable AI systems. It empowers developers to unlock the full potential of LLMs, transforming them from unpredictable black boxes into programmable, optimizable agents.\\n\\n## II. The Core Benefits of DSPy\\n\\n### 1. Programmatic Control and Structured Development\\n\\nDSPy fundamentally shifts the paradigm from \"prompt engineering\" to \"program engineering.\" It allows you to treat LLMs not as black boxes to be coaxed with magic words, but as functions within a larger, structured program. This means:\\n\\n*   **Traditional Software Engineering Practices:** You can apply familiar software development principles like modularity, abstraction, and version control to your LLM applications.\\n*   **Explicit Modules and Signatures:** DSPy replaces ad-hoc, unstructured prompts with explicit `dspy.Signature` definitions and modular components (like `dspy.Predict` or `dspy.ChainOfThought`). This clearly defines the inputs and outputs of each LLM call, making your LLM interactions readable, maintainable, and testable.\\n*   **Clear Logic:** It allows for a clear definition of inputs, outputs, and intermediate steps within LLM applications, making complex reasoning pipelines easier to understand and debug.\\n\\n### 2. Automatic Optimization and Performance Gains\\n\\nThis is where DSPy truly shines. Instead of manually tweaking prompts, DSPy introduces \"optimizers\" (also called Teleprompters or Compilers) that automatically learn how to improve your LLM programs.\\n\\n*   **Automated Prompt and Weight Optimization:** DSPy \"compiles\" LLM programs to automatically optimize prompts, few-shot examples, and even model weights for specific tasks and datasets.\\n*   **Reduced Manual Effort:** It significantly reduces the need for manual, trial-and-error prompt engineering, saving countless hours of development time.\\n*   **Improved Accuracy and Robustness:** By systematically optimizing the program, DSPy improves task accuracy, robustness, and generalization across different LLMs and datasets. It learns to generate better prompts, demonstrations, and reasoning steps automatically, leading to more consistent and reliable outputs.\\n\\n### 3. Modularity and Composable Pipelines\\n\\nDSPy encourages a modular approach to building LLM applications, much like building with LEGO bricks.\\n\\n*   **Break Down Complexity:** It enables breaking down complex LLM tasks into smaller, manageable, and reusable modules. For example, one module might extract entities, another might summarize, and a third might generate a response.\\n*   **Sophisticated Reasoning:** You can easily chain and combine these modules to build sophisticated, multi-step reasoning pipelines, such as a `dspy.ReAct` agent that plans, acts, and observes.\\n*   **Reusability:** This promotes the reusability of LLM components across different applications and projects, simplifying development, debugging, and maintenance.\\n\\n### 4. Systematic Evaluation and Debugging\\n\\nMoving beyond subjective assessments, DSPy provides a robust framework for objective evaluation.\\n\\n*   **Data-Driven Improvement:** It provides a robust framework for defining metrics and systematically evaluating LLM program performance against a dataset.\\n*   **Identify Bottlenecks:** This facilitates data-driven testing and comparison of different LLM approaches and configurations, helping you identify bottlenecks, errors, and areas for improvement within your LLM pipelines.\\n*   **Objective Optimization:** DSPy moves beyond subjective assessment to enable objective, quantifiable optimization of LLM applications, ensuring your improvements are real and measurable.\\n\\n### 5. Cost-Effectiveness and Efficiency\\n\\nBeyond performance, DSPy also offers tangible economic benefits.\\n\\n*   **Reduced API Costs:** Optimized prompts often lead to fewer tokens used per inference, directly reducing API costs, especially for high-volume applications.\\n*   **Less Rework:** Improved accuracy and reliability reduce the need for re-runs, human correction, or fallback mechanisms, saving time and resources.\\n*   **Leverage Smaller Models:** DSPy can enable the effective use of smaller, more cost-efficient LLMs for certain tasks by optimizing their performance to match or even exceed larger models.\\n*   **Accelerated Development:** Most importantly, it significantly reduces development time and effort compared to manual, iterative prompt engineering, allowing teams to build and deploy faster.\\n\\n## III. Conclusion\\n\\nIn summary, embracing DSPy offers a transformative path to building robust and efficient LLM applications. It empowers developers to achieve unparalleled performance, make data-driven decisions, and foster a culture of continuous innovation in AI development. By leveraging its capabilities, organizations can not only overcome current challenges in prompt engineering but also secure a significant competitive advantage in an ever-evolving AI landscape.\\n\\nThe future of LLM development is here, and it\\'s powered by DSPy. Don\\'t just adapt to the complexities of LLMs; master them. Start your journey towards a more efficient, insightful, and innovative LLM-powered future today.', dependencies=frozenset(), children=frozenset(), execution_history={'atomizer': ModuleResult(module_name='atomizer', input='Write me a blog post about the benefits of using DSPy.', output={'is_atomic': False, 'node_type': 'PLAN'}, timestamp=datetime.datetime(2025, 9, 29, 15, 58, 39, 769085), duration=1.131679, error=None, metadata={}, token_metrics=TokenMetrics(prompt_tokens=291, completion_tokens=86, total_tokens=377, cost=0.00030230000000000003, model='openrouter/google/gemini-2.5-flash'), messages=[{'role': 'system', 'content': 'Your input fields are:\\n1. `goal` (str): Task to atomize\\nYour output fields are:\\n1. `reasoning` (str): \\n2. `is_atomic` (bool): True if task can be executed directly\\n3. `node_type` (NodeType): Type of node to process (PLAN or EXECUTE)\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## goal ## ]]\\n{goal}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## is_atomic ## ]]\\n{is_atomic}        # note: the value you produce must be True or False\\n\\n[[ ## node_type ## ]]\\n{node_type}        # note: the value you produce must be one of: PLAN; EXECUTE\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Signature for task atomization.'}, {'role': 'user', 'content': '[[ ## goal ## ]]\\nWrite me a blog post about the benefits of using DSPy.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## is_atomic ## ]]` (must be formatted as a valid Python bool), then `[[ ## node_type ## ]]` (must be formatted as a valid Python NodeType), and then ending with the marker for `[[ ## completed ## ]]`.'}]), 'aggregator': ModuleResult(module_name='aggregator', input={'original_goal': 'Write me a blog post about the benefits of using DSPy.', 'subtask_count': 3}, output='# Unlocking LLM Potential: Why DSPy is a Game-Changer for AI Development\\n\\n## I. Introduction\\n\\nAre you tired of wrestling with complex prompt engineering, struggling to get consistent and reliable outputs from your Large Language Models (LLMs)? What if there was a way to program LLMs more systematically, like traditional software, and achieve significantly better results with less effort?\\n\\nEnter DSPy, a groundbreaking framework designed to optimize and program LLMs. Unlike traditional prompt engineering, which often relies on trial-and-error, DSPy treats LLMs as components within a larger program, allowing developers to compose, compile, and optimize complex LLM pipelines. It provides a systematic approach to building robust and efficient LLM-powered applications, moving beyond simple prompting to a more structured and performant paradigm.\\n\\nIn an era where LLMs are becoming central to countless applications, the ability to reliably and efficiently develop with them is paramount. DSPy addresses this critical need by offering a principled way to improve LLM performance, reduce development time, and build more scalable and maintainable AI systems. It empowers developers to unlock the full potential of LLMs, transforming them from unpredictable black boxes into programmable, optimizable agents.\\n\\n## II. The Core Benefits of DSPy\\n\\n### 1. Programmatic Control and Structured Development\\n\\nDSPy fundamentally shifts the paradigm from \"prompt engineering\" to \"program engineering.\" It allows you to treat LLMs not as black boxes to be coaxed with magic words, but as functions within a larger, structured program. This means:\\n\\n*   **Traditional Software Engineering Practices:** You can apply familiar software development principles like modularity, abstraction, and version control to your LLM applications.\\n*   **Explicit Modules and Signatures:** DSPy replaces ad-hoc, unstructured prompts with explicit `dspy.Signature` definitions and modular components (like `dspy.Predict` or `dspy.ChainOfThought`). This clearly defines the inputs and outputs of each LLM call, making your LLM interactions readable, maintainable, and testable.\\n*   **Clear Logic:** It allows for a clear definition of inputs, outputs, and intermediate steps within LLM applications, making complex reasoning pipelines easier to understand and debug.\\n\\n### 2. Automatic Optimization and Performance Gains\\n\\nThis is where DSPy truly shines. Instead of manually tweaking prompts, DSPy introduces \"optimizers\" (also called Teleprompters or Compilers) that automatically learn how to improve your LLM programs.\\n\\n*   **Automated Prompt and Weight Optimization:** DSPy \"compiles\" LLM programs to automatically optimize prompts, few-shot examples, and even model weights for specific tasks and datasets.\\n*   **Reduced Manual Effort:** It significantly reduces the need for manual, trial-and-error prompt engineering, saving countless hours of development time.\\n*   **Improved Accuracy and Robustness:** By systematically optimizing the program, DSPy improves task accuracy, robustness, and generalization across different LLMs and datasets. It learns to generate better prompts, demonstrations, and reasoning steps automatically, leading to more consistent and reliable outputs.\\n\\n### 3. Modularity and Composable Pipelines\\n\\nDSPy encourages a modular approach to building LLM applications, much like building with LEGO bricks.\\n\\n*   **Break Down Complexity:** It enables breaking down complex LLM tasks into smaller, manageable, and reusable modules. For example, one module might extract entities, another might summarize, and a third might generate a response.\\n*   **Sophisticated Reasoning:** You can easily chain and combine these modules to build sophisticated, multi-step reasoning pipelines, such as a `dspy.ReAct` agent that plans, acts, and observes.\\n*   **Reusability:** This promotes the reusability of LLM components across different applications and projects, simplifying development, debugging, and maintenance.\\n\\n### 4. Systematic Evaluation and Debugging\\n\\nMoving beyond subjective assessments, DSPy provides a robust framework for objective evaluation.\\n\\n*   **Data-Driven Improvement:** It provides a robust framework for defining metrics and systematically evaluating LLM program performance against a dataset.\\n*   **Identify Bottlenecks:** This facilitates data-driven testing and comparison of different LLM approaches and configurations, helping you identify bottlenecks, errors, and areas for improvement within your LLM pipelines.\\n*   **Objective Optimization:** DSPy moves beyond subjective assessment to enable objective, quantifiable optimization of LLM applications, ensuring your improvements are real and measurable.\\n\\n### 5. Cost-Effectiveness and Efficiency\\n\\nBeyond performance, DSPy also offers tangible economic benefits.\\n\\n*   **Reduced API Costs:** Optimized prompts often lead to fewer tokens used per inference, directly reducing API costs, especially for high-volume applications.\\n*   **Less Rework:** Improved accuracy and reliability reduce the need for re-runs, human correction, or fallback mechanisms, saving time and resources.\\n*   **Leverage Smaller Models:** DSPy can enable the effective use of smaller, more cost-efficient LLMs for certain tasks by optimizing their performance to match or even exceed larger models.\\n*   **Accelerated Development:** Most importantly, it significantly reduces development time and effort compared to manual, iterative prompt engineering, allowing teams to build and deploy faster.\\n\\n## III. Conclusion\\n\\nIn summary, embracing DSPy offers a transformative path to building robust and efficient LLM applications. It empowers developers to achieve unparalleled performance, make data-driven decisions, and foster a culture of continuous innovation in AI development. By leveraging its capabilities, organizations can not only overcome current challenges in prompt engineering but also secure a significant competitive advantage in an ever-evolving AI landscape.\\n\\nThe future of LLM development is here, and it\\'s powered by DSPy. Don\\'t just adapt to the complexities of LLMs; master them. Start your journey towards a more efficient, insightful, and innovative LLM-powered future today.', timestamp=datetime.datetime(2025, 9, 29, 15, 59, 13, 260162), duration=9.948538, error=None, metadata={}, token_metrics=TokenMetrics(prompt_tokens=3096, completion_tokens=1803, total_tokens=4899, cost=0.0054363, model='openrouter/google/gemini-2.5-flash'), messages=[{'role': 'system', 'content': 'Your input fields are:\\n1. `original_goal` (str): Original goal of the task\\n2. `subtasks_results` (list[SubTask]): List of subtask results to synthesize\\nYour output fields are:\\n1. `reasoning` (str): \\n2. `synthesized_result` (str): Final synthesized output\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## original_goal ## ]]\\n{original_goal}\\n\\n[[ ## subtasks_results ## ]]\\n{subtasks_results}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## synthesized_result ## ]]\\n{synthesized_result}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Aggregator synthesis result.\\n        \\n        Contains the synthesis of multiple subtask results into a cohesive output.'}, {'role': 'user', 'content': '[[ ## original_goal ## ]]\\nWrite me a blog post about the benefits of using DSPy.\\n\\n[[ ## subtasks_results ## ]]\\n[{\"goal\": \"Research DSPy to understand its core concepts, functionalities, and primary benefits for LLM development.\", \"task_type\": \"THINK\", \"dependencies\": [], \"result\": \"DSPy is a framework designed to programmatically build and optimize applications using Large Language Models (LLMs). It shifts the paradigm from manual \\\\\"prompt engineering\\\\\" to \\\\\"program engineering,\\\\\" enabling developers to construct robust, efficient, and explainable LLM-powered systems.\\\\n\\\\n**Core Concepts:**\\\\n\\\\n1.  **Programmatic Prompting & Signatures:** DSPy allows developers to define the *structure* of their LLM programs using Python, specifying the inputs and outputs of each LLM call through `dspy.Signature`. This abstracts away the complexities of prompt generation, making LLM interactions more readable, maintainable, and testable.\\\\n2.  **Modules:** Similar to layers in neural networks, DSPy provides modular components (e.g., `dspy.Predict`, `dspy.ChainOfThought`, `dspy.ReAct`) that encapsulate specific LLM operations. Developers compose these modules to build complex, multi-step reasoning pipelines, promoting reusability and clear program logic.\\\\n3.  **Optimizers (Teleprompters/Compilers):** This is a distinguishing feature. Optimizers are algorithms that *learn* how to compile and optimize DSPy programs for specific tasks and datasets. Instead of manual prompt tuning, developers provide an evaluation metric, and optimizers (like `dspy.BootstrapFewShot` or `dspy.BayesianSignatureOptimizer`) automatically search for the best prompting strategies, few-shot examples, and module configurations to maximize that metric. This automates prompt engineering and model tuning.\\\\n\\\\n**Main Functionalities:**\\\\n\\\\n*   **Building Modular LLM Pipelines:** DSPy facilitates the construction of complex, multi-step LLM applications by composing smaller, verifiable modules.\\\\n*   **Automated Prompt and Weight Optimization:** It automatically generates and refines prompts, few-shot examples, and can even optimize model weights for smaller models, using compilers to improve performance based on defined metrics.\\\\n*   **Systematic Evaluation:** DSPy provides a framework for robustly evaluating LLM programs and pipelines, allowing for data-driven iteration and improvement.\\\\n*   **Unified LLM/RM Interface:** It offers a consistent API to interact with various LLMs and Retrieval Models (RMs), abstracting away provider-specific details.\\\\n\\\\n**Primary Benefits:**\\\\n\\\\n*   **Enhanced Robustness:** By programmatically optimizing prompts and using modular designs, DSPy reduces the brittleness of LLM applications, leading to more consistent and reliable performance.\\\\n*   **Significant Efficiency Gains:** It drastically reduces development time by automating prompt engineering and optimization, allowing developers to focus on logic and evaluation. This also leads to improved performance and potentially lower API costs due to optimized token usage.\\\\n*   **Simplified Development:** DSPy offers a high-level, declarative programming model that makes LLM development feel more like traditional software engineering. It separates application logic from prompting details, improving code readability, maintainability, and debuggability.\\\\n*   **Accelerated Iteration and Experimentation:** The integrated evaluation and optimization loop enables rapid iteration and systematic improvement of LLM applications.\\\\n*   **Abstraction and Flexibility:** A unified interface for various LLMs allows for easy model swapping and experimentation without extensive code changes.\\\\n\\\\nIn essence, DSPy transforms LLM development from an art of ad-hoc prompt engineering into a systematic, robust, and efficient engineering discipline, making it easier to build high-quality and reliable LLM-powered applications.\"}, {\"goal\": \"Create a detailed outline for the blog post, including an introduction, sections for each identified benefit of DSPy, and a conclusion. The outline should specify key points for each section.\", \"task_type\": \"THINK\", \"dependencies\": [], \"result\": \"## Blog Post Outline: Unlocking LLM Potential with DSPy\\\\n\\\\n### I. Introduction\\\\n\\\\n**Hook:** Are you tired of wrestling with complex prompt engineering, struggling to get consistent and reliable outputs from your Large Language Models (LLMs)? What if there was a way to program LLMs more systematically, like traditional software, and achieve significantly better results with less effort?\\\\n\\\\n**Overview:** Enter DSPy, a groundbreaking framework designed to optimize and program LLMs. Unlike traditional prompt engineering, which often relies on trial-and-error, DSPy treats LLMs as components within a larger program, allowing developers to compose, compile, and optimize complex LLM pipelines. It provides a systematic approach to building robust and efficient LLM-powered applications, moving beyond simple prompting to a more structured and performant paradigm.\\\\n\\\\n**Importance:** In an era where LLMs are becoming central to countless applications, the ability to reliably and efficiently develop with them is paramount. DSPy addresses this critical need by offering a principled way to improve LLM performance, reduce development time, and build more scalable and maintainable AI systems. It empowers developers to unlock the full potential of LLMs, transforming them from unpredictable black boxes into programmable, optimizable agents.\\\\n\\\\n### II. The Core Benefits of DSPy\\\\n\\\\n---\\\\n\\\\n### 1. Programmatic Control and Structured Development\\\\n\\\\n*   Treats Large Language Models (LLMs) as functions, enabling traditional software engineering practices.\\\\n*   Replaces ad-hoc, unstructured prompt engineering with explicit, testable modules.\\\\n*   Allows for clear definition of inputs, outputs, and intermediate steps within LLM applications.\\\\n*   Facilitates version control, code reviews, and collaborative development of LLM-powered systems.\\\\n\\\\n### 2. Automatic Optimization and Performance Gains\\\\n\\\\n*   \\\\\"Compiles\\\\\" LLM programs to automatically optimize prompts and model weights for specific tasks.\\\\n*   Significantly reduces the need for manual, trial-and-error prompt engineering.\\\\n*   Improves task accuracy, robustness, and generalization across different LLMs and datasets.\\\\n*   Can fine-tune smaller models or adapt larger models more effectively to achieve desired performance.\\\\n*   Learns to generate better prompts, demonstrations, and reasoning steps automatically.\\\\n\\\\n### 3. Modularity and Composable Pipelines\\\\n\\\\n*   Enables breaking down complex LLM tasks into smaller, manageable, and reusable modules.\\\\n*   Supports chaining and combining these modules to build sophisticated, multi-step reasoning pipelines.\\\\n*   Promotes reusability of LLM components across different applications and projects.\\\\n*   Simplifies the development, debugging, and maintenance of complex LLM-powered systems.\\\\n\\\\n### 4. Systematic Evaluation and Debugging\\\\n\\\\n*   Provides a robust framework for defining metrics and systematically evaluating LLM program performance.\\\\n*   Facilitates data-driven testing and comparison of different LLM approaches and configurations.\\\\n*   Helps identify bottlenecks, errors, and areas for improvement within LLM pipelines.\\\\n*   Moves beyond subjective assessment to enable objective, quantifiable optimization of LLM applications.\\\\n\\\\n### 5. Cost-Effectiveness and Efficiency\\\\n\\\\n*   Optimized prompts often lead to fewer tokens used per inference, reducing API costs.\\\\n*   Improved accuracy and reliability reduce the need for re-runs, human correction, or fallback mechanisms.\\\\n*   Can enable the effective use of smaller, more cost-efficient LLMs for certain tasks.\\\\n*   Significantly reduces development time and effort compared to manual, iterative prompt engineering.\\\\n\\\\n### III. Conclusion\\\\n\\\\nIn summary, embracing DSPy offers a transformative path to building robust and efficient LLM applications, empowering developers to achieve unparalleled performance, make data-driven decisions, and foster a culture of continuous innovation in AI development. By leveraging its capabilities, organizations can not only overcome current challenges in prompt engineering but also secure a significant competitive advantage in an ever-evolving AI landscape. The future of LLM development is here, and it\\'s powered by DSPy. Don\\'t just adapt to the complexities of LLMs; master them. Start your journey towards a more efficient, insightful, and innovative LLM-powered future today.\"}, {\"goal\": \"Write the full blog post content based on the approved outline, ensuring a clear, engaging, and informative tone suitable for a blog.\", \"task_type\": \"THINK\", \"dependencies\": [], \"result\": \"# The Future of AI in Healthcare: Revolutionizing Patient Care\\\\n\\\\n## Introduction: The Dawn of a New Era in Medicine\\\\n\\\\nImagine a world where diseases are detected earlier, treatments are personalized to your unique genetic makeup, and medical research accelerates at an unprecedented pace. This isn\\'t science fiction; it\\'s the promise of Artificial Intelligence (AI) in healthcare. For decades, healthcare professionals have strived for more efficient, accurate, and accessible care. Now, with the rapid advancements in AI, we stand at the precipice of a revolution that could fundamentally transform how we approach health and wellness. From diagnostics to drug discovery, AI is not just a tool; it\\'s becoming an indispensable partner in the quest for better patient outcomes.\\\\n\\\\n## AI\\'s Impact on Diagnostics and Early Detection\\\\n\\\\nOne of the most profound impacts of AI is in its ability to enhance diagnostics and facilitate early disease detection. Traditional diagnostic methods, while effective, can sometimes be time-consuming and prone to human error. AI algorithms, particularly those leveraging machine learning and deep learning, can analyze vast amounts of medical data – including medical images (X-rays, MRIs, CT scans), pathology slides, and patient records – with remarkable speed and accuracy. \\\\n\\\\nFor instance, AI-powered systems are already demonstrating superior performance in identifying subtle anomalies in mammograms that might be missed by the human eye, leading to earlier detection of breast cancer. Similarly, in ophthalmology, AI can detect early signs of diabetic retinopathy, a leading cause of blindness, allowing for timely intervention. This capability not only improves diagnostic precision but also frees up clinicians to focus on more complex cases and direct patient interaction.\\\\n\\\\n## Personalized Treatment Plans: Tailoring Care to the Individual\\\\n\\\\nHealthcare is increasingly moving towards a personalized approach, and AI is at the forefront of this shift. Every patient is unique, with different genetic predispositions, lifestyles, and responses to medication. AI can analyze a patient\\'s comprehensive health data – including their genetic profile, medical history, lifestyle factors, and even real-time physiological data from wearables – to create highly personalized treatment plans.\\\\n\\\\nThis means moving beyond a \\'one-size-fits-all\\' approach. For cancer patients, AI can help predict which therapies will be most effective based on the tumor\\'s genetic mutations. In chronic disease management, AI can monitor patient adherence to medication, suggest dietary adjustments, and even predict potential complications before they arise. This level of personalization not only improves treatment efficacy but also minimizes adverse side effects, leading to a better quality of life for patients.\\\\n\\\\n## Revolutionizing Drug Discovery and Development\\\\n\\\\nThe process of discovering and developing new drugs is notoriously long, expensive, and often fraught with failure. It can take over a decade and billions of dollars to bring a single drug to market. AI is poised to dramatically accelerate this process.\\\\n\\\\nAI algorithms can sift through massive databases of chemical compounds, predict their interactions with biological targets, and identify potential drug candidates much faster than traditional methods. They can also simulate how drugs will behave in the human body, reducing the need for extensive and costly laboratory experiments. Furthermore, AI can analyze existing research papers and clinical trial data to uncover new insights and repurpose existing drugs for new indications. This acceleration in drug discovery promises to bring life-saving treatments to patients much more quickly and efficiently.\\\\n\\\\n## The Ethical Considerations and Challenges Ahead\\\\n\\\\nWhile the potential benefits of AI in healthcare are immense, it\\'s crucial to address the ethical considerations and challenges that come with its widespread adoption. \\\\n\\\\n*   **Data Privacy and Security:** AI systems rely on vast amounts of sensitive patient data. Ensuring the privacy and security of this information is paramount to maintaining public trust.\\\\n*   **Bias in Algorithms:** If AI models are trained on biased datasets, they can perpetuate or even amplify existing health disparities. Developing fair and unbiased algorithms is a critical challenge.\\\\n*   **Accountability and Transparency:** When an AI system makes a diagnostic error or recommends a treatment, who is accountable? The \\'black box\\' nature of some AI models also raises questions about transparency and explainability.\\\\n*   **Job Displacement:** While AI will create new roles, there are concerns about the potential displacement of certain healthcare jobs.\\\\n\\\\nAddressing these challenges requires careful regulation, interdisciplinary collaboration, and ongoing public discourse to ensure that AI is developed and implemented responsibly and ethically.\\\\n\\\\n## Conclusion: A Healthier Future Powered by AI\\\\n\\\\nAI is not just a technological trend; it\\'s a transformative force that is reshaping the landscape of healthcare. From enhancing diagnostic accuracy and personalizing treatments to accelerating drug discovery, its potential to improve patient care is undeniable. While challenges related to ethics, data privacy, and implementation remain, the collaborative efforts of researchers, clinicians, policymakers, and technology developers are paving the way for a healthier, more equitable future.\\\\n\\\\nThe integration of AI into healthcare promises a future where diseases are caught earlier, treatments are more effective, and every individual receives care tailored to their unique needs. The journey has just begun, and the future of medicine looks brighter than ever with AI as its guiding light.\"}]\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## synthesized_result ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}])}, state_transitions=[StateTransition(from_state='PENDING', to_state='ATOMIZING', timestamp=datetime.datetime(2025, 9, 29, 19, 58, 38, 636916, tzinfo=datetime.timezone.utc), reason=None, metadata={}), StateTransition(from_state='ATOMIZING', to_state='PLANNING', timestamp=datetime.datetime(2025, 9, 29, 19, 58, 39, 769202, tzinfo=datetime.timezone.utc), reason=None, metadata={}), StateTransition(from_state='PLANNING', to_state='PLAN_DONE', timestamp=datetime.datetime(2025, 9, 29, 19, 58, 41, 651246, tzinfo=datetime.timezone.utc), reason=None, metadata={}), StateTransition(from_state='PLAN_DONE', to_state='AGGREGATING', timestamp=datetime.datetime(2025, 9, 29, 19, 59, 3, 311560, tzinfo=datetime.timezone.utc), reason=None, metadata={}), StateTransition(from_state='AGGREGATING', to_state='COMPLETED', timestamp=datetime.datetime(2025, 9, 29, 19, 59, 13, 260232, tzinfo=datetime.timezone.utc), reason=None, metadata={})], metrics=NodeMetrics(atomizer_duration=1.131679, planner_duration=None, executor_duration=None, aggregator_duration=9.948538, total_duration=11.080217, retry_count=0, subtasks_created=3, max_depth_reached=0), subgraph_id='a07c96e0-9a45-46e2-bd28-03467dac689a_sub_fead3493-f10d-4b3f-9a62-49089c943e27', created_at=datetime.datetime(2025, 9, 29, 19, 58, 38, 636536, tzinfo=datetime.timezone.utc), started_at=None, completed_at=datetime.datetime(2025, 9, 29, 19, 59, 13, 260245, tzinfo=datetime.timezone.utc), version=9),\n",
       "    status=<TaskStatus.COMPLETED: 'COMPLETED'>,\n",
       "    result_text='# Unlocking LLM Potential: Why DSPy is a Game-Changer for AI Development\\n\\n## I. Introduction\\n\\nAre you tired of wrestling with complex prompt engineering, struggling to get consistent and reliable outputs from your Large Language Models (LLMs)? What if there was a way to program LLMs more systematically, like traditional software, and achieve significantly better results with less effort?\\n\\nEnter DSPy, a groundbreaking framework designed to optimize and program LLMs. Unlike traditional prompt engineering, which often relies on trial-and-error, DSPy treats LLMs as components within a larger program, allowing developers to compose, compile, and optimize complex LLM pipelines. It provides a systematic approach to building robust and efficient LLM-powered applications, moving beyond simple prompting to a more structured and performant paradigm.\\n\\nIn an era where LLMs are becoming central to countless applications, the ability to reliably and efficiently develop with them is paramount. DSPy addresses this critical need by offering a principled way to improve LLM performance, reduce development time, and build more scalable and maintainable AI systems. It empowers developers to unlock the full potential of LLMs, transforming them from unpredictable black boxes into programmable, optimizable agents.\\n\\n## II. The Core Benefits of DSPy\\n\\n### 1. Programmatic Control and Structured Development\\n\\nDSPy fundamentally shifts the paradigm from \"prompt engineering\" to \"program engineering.\" It allows you to treat LLMs not as black boxes to be coaxed with magic words, but as functions within a larger, structured program. This means:\\n\\n*   **Traditional Software Engineering Practices:** You can apply familiar software development principles like modularity, abstraction, and version control to your LLM applications.\\n*   **Explicit Modules and Signatures:** DSPy replaces ad-hoc, unstructured prompts with explicit `dspy.Signature` definitions and modular components (like `dspy.Predict` or `dspy.ChainOfThought`). This clearly defines the inputs and outputs of each LLM call, making your LLM interactions readable, maintainable, and testable.\\n*   **Clear Logic:** It allows for a clear definition of inputs, outputs, and intermediate steps within LLM applications, making complex reasoning pipelines easier to understand and debug.\\n\\n### 2. Automatic Optimization and Performance Gains\\n\\nThis is where DSPy truly shines. Instead of manually tweaking prompts, DSPy introduces \"optimizers\" (also called Teleprompters or Compilers) that automatically learn how to improve your LLM programs.\\n\\n*   **Automated Prompt and Weight Optimization:** DSPy \"compiles\" LLM programs to automatically optimize prompts, few-shot examples, and even model weights for specific tasks and datasets.\\n*   **Reduced Manual Effort:** It significantly reduces the need for manual, trial-and-error prompt engineering, saving countless hours of development time.\\n*   **Improved Accuracy and Robustness:** By systematically optimizing the program, DSPy improves task accuracy, robustness, and generalization across different LLMs and datasets. It learns to generate better prompts, demonstrations, and reasoning steps automatically, leading to more consistent and reliable outputs.\\n\\n### 3. Modularity and Composable Pipelines\\n\\nDSPy encourages a modular approach to building LLM applications, much like building with LEGO bricks.\\n\\n*   **Break Down Complexity:** It enables breaking down complex LLM tasks into smaller, manageable, and reusable modules. For example, one module might extract entities, another might summarize, and a third might generate a response.\\n*   **Sophisticated Reasoning:** You can easily chain and combine these modules to build sophisticated, multi-step reasoning pipelines, such as a `dspy.ReAct` agent that plans, acts, and observes.\\n*   **Reusability:** This promotes the reusability of LLM components across different applications and projects, simplifying development, debugging, and maintenance.\\n\\n### 4. Systematic Evaluation and Debugging\\n\\nMoving beyond subjective assessments, DSPy provides a robust framework for objective evaluation.\\n\\n*   **Data-Driven Improvement:** It provides a robust framework for defining metrics and systematically evaluating LLM program performance against a dataset.\\n*   **Identify Bottlenecks:** This facilitates data-driven testing and comparison of different LLM approaches and configurations, helping you identify bottlenecks, errors, and areas for improvement within your LLM pipelines.\\n*   **Objective Optimization:** DSPy moves beyond subjective assessment to enable objective, quantifiable optimization of LLM applications, ensuring your improvements are real and measurable.\\n\\n### 5. Cost-Effectiveness and Efficiency\\n\\nBeyond performance, DSPy also offers tangible economic benefits.\\n\\n*   **Reduced API Costs:** Optimized prompts often lead to fewer tokens used per inference, directly reducing API costs, especially for high-volume applications.\\n*   **Less Rework:** Improved accuracy and reliability reduce the need for re-runs, human correction, or fallback mechanisms, saving time and resources.\\n*   **Leverage Smaller Models:** DSPy can enable the effective use of smaller, more cost-efficient LLMs for certain tasks by optimizing their performance to match or even exceed larger models.\\n*   **Accelerated Development:** Most importantly, it significantly reduces development time and effort compared to manual, iterative prompt engineering, allowing teams to build and deploy faster.\\n\\n## III. Conclusion\\n\\nIn summary, embracing DSPy offers a transformative path to building robust and efficient LLM applications. It empowers developers to achieve unparalleled performance, make data-driven decisions, and foster a culture of continuous innovation in AI development. By leveraging its capabilities, organizations can not only overcome current challenges in prompt engineering but also secure a significant competitive advantage in an ever-evolving AI landscape.\\n\\nThe future of LLM development is here, and it\\'s powered by DSPy. Don\\'t just adapt to the complexities of LLMs; master them. Start your journey towards a more efficient, insightful, and innovative LLM-powered future today.'\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6b31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
