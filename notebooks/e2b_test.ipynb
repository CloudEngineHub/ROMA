{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2B Unified Mount Directory Test\n",
    "\n",
    "**✅ E2B Integration with Unified Mount Structure**\n",
    "\n",
    "## 🎯 Purpose\n",
    "Test the **unified mount directory structure** across all environments:\n",
    "- **Local**: `$HOME/data/sentient/`\n",
    "- **Docker**: `/data/sentient/`  \n",
    "- **E2B**: `/data/sentient/`\n",
    "\n",
    "## 📁 Expected Structure\n",
    "```\n",
    "/data/sentient/\n",
    "├── {project_id}/\n",
    "│   ├── binance_toolkit/     # Binance API data\n",
    "│   ├── coingecko_toolkit/   # CoinGecko API data  \n",
    "│   ├── defillama_toolkit/   # DefiLlama API data\n",
    "│   ├── arkham_toolkit/      # Arkham API data\n",
    "│   └── results/             # AI analysis outputs\n",
    "└── other_projects/\n",
    "```\n",
    "\n",
    "## 🔄 Test Flow\n",
    "1. 🏠 **Local Data Fetching**: Agent with data toolkit fetches crypto data to mount directory\n",
    "2. ☁️ **S3 Sync**: Data automatically syncs to S3 bucket via mount\n",
    "3. 🚀 **E2B Execution**: Code runs in sandbox with same mount path `/data/sentient`\n",
    "4. 🧮 **Analysis**: Sandbox analyzes data and saves results to mount\n",
    "5. 📊 **Results**: Results visible across all environments\n",
    "\n",
    "## ✅ What's Working\n",
    "- Unified `/data/sentient` mount path across environments\n",
    "- S3 bucket mounted with goofys ✅ **WORKING!**\n",
    "- Cross-environment data persistence ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Environment Setup and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Ready for fresh imports\n"
     ]
    }
   ],
   "source": [
    "# Clean imports and memory\n",
    "import sys\n",
    "import importlib\n",
    "import gc\n",
    "\n",
    "# Remove any cached modules to ensure fresh imports\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'sentient' in m.lower() or 'agno' in m.lower()]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "print(\"🔄 Ready for fresh imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Dependencies and Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Environment Configuration Check:\n",
      "==================================================\n",
      "✅ E2B_API_KEY: e2b_d5d5...bd00\n",
      "✅ AWS_ACCESS_KEY_ID: AKIA6GBM...QUL3\n",
      "✅ AWS_SECRET_ACCESS_KEY: K/Ux894V...X8Xp\n",
      "✅ S3_BUCKET_NAME: ***\n",
      "\n",
      "📋 Optional Configuration:\n",
      "ℹ️  E2B_TEMPLATE_ID: Using default - Custom E2B template (defaults to sentient-e2b-s3)\n",
      "ℹ️  E2B_TIMEOUT: Using default - E2B sandbox timeout (defaults to 300s)\n",
      "✅ AWS_REGION: us-west-2\n",
      "\n",
      "🎉 All required environment variables are configured!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "required_env_vars = {\n",
    "    'E2B_API_KEY': 'E2B API key for sandbox access',\n",
    "    'AWS_ACCESS_KEY_ID': 'AWS access key for S3 integration',\n",
    "    'AWS_SECRET_ACCESS_KEY': 'AWS secret key for S3 integration',\n",
    "    'S3_BUCKET_NAME': 'S3 bucket name for data storage',\n",
    "}\n",
    "\n",
    "optional_env_vars = {\n",
    "    'E2B_TEMPLATE_ID': 'Custom E2B template (defaults to sentient-e2b-s3)',\n",
    "    'E2B_TIMEOUT': 'E2B sandbox timeout (defaults to 300s)',\n",
    "    'AWS_REGION': 'AWS region (defaults to us-east-1)',\n",
    "}\n",
    "\n",
    "print(\"🔍 Environment Configuration Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check required variables\n",
    "missing_required = []\n",
    "for var, description in required_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        masked_value = f\"{value[:8]}...{value[-4:]}\" if len(value) > 12 else \"***\"\n",
    "        print(f\"✅ {var}: {masked_value}\")\n",
    "    else:\n",
    "        print(f\"❌ {var}: MISSING - {description}\")\n",
    "        missing_required.append(var)\n",
    "\n",
    "print(\"\\n📋 Optional Configuration:\")\n",
    "for var, description in optional_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        print(f\"✅ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"ℹ️  {var}: Using default - {description}\")\n",
    "\n",
    "if missing_required:\n",
    "    print(f\"\\n⚠️  Missing required environment variables: {', '.join(missing_required)}\")\n",
    "    print(\"   Please update your .env file with the missing values.\")\n",
    "else:\n",
    "    print(\"\\n🎉 All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agno dependencies imported successfully\n",
      "✅ SentientResearchAgent toolkits imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import agno and E2B tools\n",
    "try:\n",
    "    from agno.agent import Agent as AgnoAgent\n",
    "    from agno.models.litellm import LiteLLM\n",
    "    from agno.tools.e2b import E2BTools\n",
    "    from agno.tools.reasoning import ReasoningTools\n",
    "    print(\"✅ Agno dependencies imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import agno dependencies: {e}\")\n",
    "    print(\"   Please install agno: pip install agno-ai\")\n",
    "\n",
    "# Import SentientResearchAgent toolkits for comparison\n",
    "try:\n",
    "    from sentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit import BinanceToolkit\n",
    "    from sentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit import CoinGeckoToolkit\n",
    "    print(\"✅ SentientResearchAgent toolkits imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import SentientResearchAgent toolkits: {e}\")\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 E2B output decoder functions loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Union\n",
    "\n",
    "def decode_e2b_output(raw_result: Any) -> str:\n",
    "    \"\"\"\n",
    "    Decode AgnoAgent E2BTools output properly.\n",
    "    \n",
    "    The AgnoAgent E2BTools.run_python_code() method returns JSON-encoded output \n",
    "    in the format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "    This function properly decodes the Unicode escapes and extracts the stdout content.\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Raw output from E2BTools.run_python_code()\n",
    "        \n",
    "    Returns:\n",
    "        Properly decoded and formatted output string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle different input types\n",
    "        if isinstance(raw_result, list) and len(raw_result) > 0:\n",
    "            content = raw_result[0]\n",
    "        elif isinstance(raw_result, str):\n",
    "            content = raw_result\n",
    "        else:\n",
    "            return str(raw_result)\n",
    "        \n",
    "        # Handle JSON-encoded list format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "        if isinstance(content, str) and content.startswith('[\"') and content.endswith('\"]'):\n",
    "            try:\n",
    "                parsed_list = json.loads(content)\n",
    "                if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "                    content = parsed_list[0]\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Extract stdout content from Logs format: \"Logs:\\nLogs(stdout: [...], stderr: [...])\"\n",
    "        if isinstance(content, str) and \"Logs(stdout:\" in content:\n",
    "            # Use regex to extract stdout content\n",
    "            stdout_pattern = r\"stdout:\\s*\\[(.*?)\\],?\\s*stderr:\"\n",
    "            match = re.search(stdout_pattern, content, re.DOTALL)\n",
    "            \n",
    "            if match:\n",
    "                stdout_raw = match.group(1)\n",
    "                \n",
    "                # Parse the stdout array content - handle quoted strings with proper escaping\n",
    "                stdout_lines = []\n",
    "                \n",
    "                # Split by ', ' but handle embedded quotes and escapes\n",
    "                current_part = \"\"\n",
    "                in_quotes = False\n",
    "                escaped = False\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(stdout_raw):\n",
    "                    char = stdout_raw[i]\n",
    "                    \n",
    "                    if escaped:\n",
    "                        current_part += char\n",
    "                        escaped = False\n",
    "                    elif char == '\\\\':\n",
    "                        current_part += char\n",
    "                        escaped = True\n",
    "                    elif char == \"'\" and not escaped:\n",
    "                        in_quotes = not in_quotes\n",
    "                        current_part += char\n",
    "                    elif char == ',' and not in_quotes and i + 1 < len(stdout_raw) and stdout_raw[i + 1] == ' ':\n",
    "                        # Found separator\n",
    "                        if current_part.strip():\n",
    "                            stdout_lines.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "                        i += 1  # Skip the space after comma\n",
    "                    else:\n",
    "                        current_part += char\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                # Add the last part\n",
    "                if current_part.strip():\n",
    "                    stdout_lines.append(current_part.strip())\n",
    "                \n",
    "                # Clean and decode each part\n",
    "                decoded_lines = []\n",
    "                for part in stdout_lines:\n",
    "                    # Remove outer quotes\n",
    "                    if (part.startswith(\"'\") and part.endswith(\"'\")) or (part.startswith('\"') and part.endswith('\"')):\n",
    "                        part = part[1:-1]\n",
    "                    \n",
    "                    # Decode JSON escapes properly\n",
    "                    try:\n",
    "                        # Handle JSON string encoding with proper Unicode support\n",
    "                        decoded_part = json.loads(f'\"{part}\"')\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Fallback: manual decode common escapes\n",
    "                        decoded_part = part\n",
    "                        decoded_part = decoded_part.replace('\\\\n', '\\n')\n",
    "                        decoded_part = decoded_part.replace('\\\\t', '\\t')\n",
    "                        decoded_part = decoded_part.replace('\\\\\"', '\"')\n",
    "                        decoded_part = decoded_part.replace(\"\\\\'\", \"'\")\n",
    "                        # Handle Unicode escapes manually\n",
    "                        decoded_part = decode_unicode_escapes(decoded_part)\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                \n",
    "                # Join all stdout lines\n",
    "                full_output = ''.join(decoded_lines)\n",
    "                return full_output\n",
    "        \n",
    "        # If not in Logs format, try direct JSON decode\n",
    "        if isinstance(content, str):\n",
    "            try:\n",
    "                decoded = json.loads(f'\"{content}\"')\n",
    "                return decode_unicode_escapes(decoded)\n",
    "            except json.JSONDecodeError:\n",
    "                return decode_unicode_escapes(content)\n",
    "        \n",
    "        return str(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Error decoding E2B output: {e}]\\n\\nRaw output:\\n{str(raw_result)}\"\n",
    "\n",
    "\n",
    "def decode_unicode_escapes(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Decode Unicode escape sequences in text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that may contain Unicode escapes like \\\\ud83c\\\\udfd7\\\\ufe0f\n",
    "        \n",
    "    Returns:\n",
    "        Text with Unicode escapes properly decoded\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    try:\n",
    "        # Handle Unicode escapes using encode/decode\n",
    "        return text.encode().decode('unicode_escape')\n",
    "    except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "        # Fallback: manual decode using codecs\n",
    "        try:\n",
    "            import codecs\n",
    "            return codecs.decode(text, 'unicode_escape')\n",
    "        except:\n",
    "            # Final fallback: return as-is\n",
    "            return text\n",
    "\n",
    "print(\"🔧 E2B output decoder functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-21 19:50:44.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_cache_system\u001b[0m:\u001b[36m270\u001b[0m - \u001b[34m\u001b[1mInitialized generic cache system with TTL: 3600s\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.151\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[34m\u001b[1mInitialized DataHTTPClient with 30.0s timeout\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.152\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_standard_configuration\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1mInitialized standard configuration: timeout=30.0s, retries=3, cache_ttl=3600s\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.153\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m100\u001b[0m - \u001b[33m\u001b[1mS3 mounting enabled but directory $HOME/data/sentient not found, using fallback: notebooks/data/e2b_test\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_detect_e2b_context\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mS3 integration detected with bucket: roma-shared\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mData helpers initialized - Project: default, Toolkit: binance, Dir: notebooks/data/e2b_test/default/binance, S3: True\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m300\u001b[0m - \u001b[34m\u001b[1mInitialized Multi-Market BinanceToolkit with default market 'spot' and 3 symbols\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'spot' with base URL: https://api.binance.us\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.154\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'usdm' with base URL: https://fapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'coinm' with base URL: https://dapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36msetup_endpoints\u001b[0m:\u001b[36m419\u001b[0m - \u001b[34m\u001b[1mSetup 3 authenticated endpoints\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.162\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_get_client\u001b[0m:\u001b[36m175\u001b[0m - \u001b[34m\u001b[1mCreated HTTP client for endpoint 'spot'\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/exchangeInfo (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 LOCAL DATA FETCHING & S3 SYNC TEST\n",
      "=============================================\n",
      "📊 Step 1: Fetching crypto data locally...\n",
      "✅ BinanceToolkit initialized with local dir: ./notebooks/data/e2b_test\n",
      "  Fetching current prices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-21 19:50:44.456\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_cache_data\u001b[0m:\u001b[36m311\u001b[0m - \u001b[34m\u001b[1mCached set data (246 items) for key 'symbols_spot'\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.456\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36mreload_symbols\u001b[0m:\u001b[36m509\u001b[0m - \u001b[1mLoaded 246 symbols for Binance Spot Trading\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.581\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-21 19:50:44.714\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ BTCUSDT: $112409.04\n",
      "  ✅ ETHUSD: $4234.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-21 19:50:44.850\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/klines (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ SOLUSDT: $181.54\n",
      "  Fetching historical data...\n",
      "  ✅ BTC 24h data: 24 klines\n",
      "\n",
      "💾 Step 2: Saving data locally for S3 sync...\n",
      "✅ Data saved locally: ./notebooks/data/e2b_test/crypto_dataset.json (15306 bytes)\n",
      "✅ Prices CSV saved: ./notebooks/data/e2b_test/current_prices.csv (185 bytes)\n",
      "\n",
      "☁️ Step 3: Verifying S3 sync via E2B sandbox...\n",
      "   Waiting 5 seconds for potential S3 sync...\n",
      "❌ Local data fetching test failed: name 'direct_success' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/dk/m1n_82n10g9fnf63ct8vl2540000gn/T/ipykernel_22457/3609255658.py\", line 121, in test_local_data_fetch_and_sync\n",
      "    if not direct_success:\n",
      "           ^^^^^^^^^^^^^^\n",
      "NameError: name 'direct_success' is not defined\n"
     ]
    }
   ],
   "source": [
    "async def test_local_data_fetch_and_sync():\n",
    "    \"\"\"Test local data fetching with toolkits and S3 sync verification.\"\"\"\n",
    "    print(\"🏠 LOCAL DATA FETCHING & S3 SYNC TEST\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Fetch data locally using our toolkits\n",
    "        print(\"📊 Step 1: Fetching crypto data locally...\")\n",
    "        \n",
    "        # Initialize local data toolkit (similar to what agents would use)\n",
    "        local_data_dir = \"./notebooks/data/e2b_test\"\n",
    "        os.makedirs(local_data_dir, exist_ok=True)\n",
    "        \n",
    "        binance_toolkit = BinanceToolkit(\n",
    "            symbols=['BTCUSDT', 'ETHUSD', 'SOLUSDT'],\n",
    "            default_market_type=\"spot\",\n",
    "            data_dir=local_data_dir,\n",
    "            parquet_threshold=500\n",
    "        )\n",
    "        print(f\"✅ BinanceToolkit initialized with local dir: {local_data_dir}\")\n",
    "        \n",
    "        # Get current prices and save locally (fix: handle API response format)\n",
    "        print(\"  Fetching current prices...\")\n",
    "        prices_data = {}\n",
    "        symbols = ['BTCUSDT', 'ETHUSD', 'SOLUSDT']\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                price_response = await binance_toolkit.get_current_price(symbol)\n",
    "                # Handle different response formats\n",
    "                if isinstance(price_response, dict):\n",
    "                    if 'price' in price_response:\n",
    "                        price_value = price_response['price']\n",
    "                    elif 'data' in price_response and isinstance(price_response['data'], dict):\n",
    "                        price_value = price_response['data'].get('price', 'N/A')\n",
    "                    elif 'data' in price_response and isinstance(price_response['data'], list) and len(price_response['data']) > 0:\n",
    "                        price_value = price_response['data'][0].get('price', 'N/A')\n",
    "                    else:\n",
    "                        # Check for any field that might contain price\n",
    "                        price_value = str(price_response)[:50] + \"...\" if len(str(price_response)) > 50 else str(price_response)\n",
    "                else:\n",
    "                    price_value = str(price_response)\n",
    "                \n",
    "                prices_data[symbol] = {'price': price_value, 'raw_response': str(price_response)[:200]}\n",
    "                print(f\"  ✅ {symbol}: ${price_value}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ {symbol}: Error - {e}\")\n",
    "                prices_data[symbol] = {\"error\": str(e)}\n",
    "        \n",
    "        # Get some historical data (fix: handle DataFrame serialization)\n",
    "        print(\"  Fetching historical data...\")\n",
    "        try:\n",
    "            btc_klines_response = await binance_toolkit.get_klines('BTCUSDT', interval='1h', limit=24)\n",
    "            \n",
    "            # Convert DataFrame to serializable format\n",
    "            if isinstance(btc_klines_response, dict):\n",
    "                btc_klines = {}\n",
    "                for key, value in btc_klines_response.items():\n",
    "                    if hasattr(value, 'to_dict'):  # DataFrame\n",
    "                        btc_klines[key] = value.to_dict('records')\n",
    "                    elif hasattr(value, 'tolist'):  # NumPy array\n",
    "                        btc_klines[key] = value.tolist()\n",
    "                    else:\n",
    "                        btc_klines[key] = value\n",
    "            else:\n",
    "                btc_klines = {\"raw_response\": str(btc_klines_response)[:500]}\n",
    "                \n",
    "            data_count = len(btc_klines.get('data', [])) if 'data' in btc_klines else 'unknown'\n",
    "            print(f\"  ✅ BTC 24h data: {data_count} klines\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ BTC historical data: Error - {e}\")\n",
    "            btc_klines = {\"error\": str(e)}\n",
    "        \n",
    "        # Create comprehensive dataset (ensure all data is JSON serializable)\n",
    "        dataset = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data_source': 'binance_api',\n",
    "            'fetch_location': 'local_notebook',\n",
    "            'current_prices': prices_data,\n",
    "            'btc_24h_klines': btc_klines,\n",
    "            'symbols_count': len(symbols),\n",
    "            'local_storage_path': local_data_dir\n",
    "        }\n",
    "        \n",
    "        # Step 2: Save data locally (this should sync to S3 via mounted filesystem)\n",
    "        print(f\"\\n💾 Step 2: Saving data locally for S3 sync...\")\n",
    "        local_file = os.path.join(local_data_dir, 'crypto_dataset.json')\n",
    "        \n",
    "        with open(local_file, 'w') as f:\n",
    "            json.dump(dataset, f, indent=2)\n",
    "        \n",
    "        local_size = os.path.getsize(local_file)\n",
    "        print(f\"✅ Data saved locally: {local_file} ({local_size} bytes)\")\n",
    "        \n",
    "        # Also save as CSV for easier analysis  \n",
    "        if prices_data:\n",
    "            import pandas as pd\n",
    "            prices_df = pd.DataFrame([\n",
    "                {\n",
    "                    'symbol': symbol,\n",
    "                    'price': str(data.get('price', 'N/A')),\n",
    "                    'timestamp': dataset['timestamp'],\n",
    "                    'error': data.get('error', None),\n",
    "                    'has_data': 'price' in data and data.get('error') is None\n",
    "                }\n",
    "                for symbol, data in prices_data.items()\n",
    "            ])\n",
    "            \n",
    "            csv_file = os.path.join(local_data_dir, 'current_prices.csv')\n",
    "            prices_df.to_csv(csv_file, index=False)\n",
    "            csv_size = os.path.getsize(csv_file)\n",
    "            print(f\"✅ Prices CSV saved: {csv_file} ({csv_size} bytes)\")\n",
    "        \n",
    "        # Step 3: Wait for S3 sync and verify in sandbox\n",
    "        print(f\"\\n☁️ Step 3: Verifying S3 sync via E2B sandbox...\")\n",
    "        \n",
    "        # Give some time for S3 sync (if using mounted filesystem)\n",
    "        print(\"   Waiting 5 seconds for potential S3 sync...\")\n",
    "        await asyncio.sleep(5)\n",
    "        \n",
    "        if not direct_success:\n",
    "            print(\"⚠️ Skipping S3 verification - direct E2B test failed\")\n",
    "            return False\n",
    "        \n",
    "        # Check if data is accessible from E2B sandbox using human-readable template name\n",
    "        template_name = \"sentient-e2b-s3\"\n",
    "        e2b_toolkit = E2BTools(sandbox_options={\"template\": template_name})\n",
    "        \n",
    "        # This code will run in the E2B sandbox to check S3 data availability\n",
    "        s3_verification_code = f'''\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "print(\"S3 DATA SYNC VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check potential S3 mount paths where our data might appear\n",
    "s3_paths = [\n",
    "    \"/home/user/s3-bucket\",\n",
    "    \"/workspace/s3-bucket\", \n",
    "    \"/workspace/data\",\n",
    "    \"/workspace/results\"\n",
    "]\n",
    "\n",
    "found_data = False\n",
    "dataset_file = None\n",
    "\n",
    "# Look for our dataset in mounted S3 paths\n",
    "for s3_path in s3_paths:\n",
    "    if os.path.exists(s3_path):\n",
    "        print(f\"Searching in: {{s3_path}}\")\n",
    "        \n",
    "        # Search for our crypto dataset\n",
    "        search_patterns = [\n",
    "            os.path.join(s3_path, \"**\", \"crypto_dataset.json\"),\n",
    "            os.path.join(s3_path, \"**\", \"current_prices.csv\"),\n",
    "            os.path.join(s3_path, \"crypto_dataset.json\"),\n",
    "            os.path.join(s3_path, \"current_prices.csv\")\n",
    "        ]\n",
    "        \n",
    "        for pattern in search_patterns:\n",
    "            matches = glob.glob(pattern, recursive=True)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    print(f\"FOUND data file: {{match}}\")\n",
    "                    size = os.path.getsize(match)\n",
    "                    print(f\"   Size: {{size}} bytes\")\n",
    "                    \n",
    "                    if match.endswith('crypto_dataset.json'):\n",
    "                        dataset_file = match\n",
    "                        found_data = True\n",
    "                        \n",
    "                        # Try to read and verify the data\n",
    "                        try:\n",
    "                            with open(match, 'r') as f:\n",
    "                                data = json.load(f)\n",
    "                            print(f\"   Data readable: {{len(data)}} fields\")\n",
    "                            print(f\"   Timestamp: {{data.get('timestamp', 'unknown')}}\")\n",
    "                            print(f\"   Symbols: {{data.get('symbols_count', 'unknown')}}\")\n",
    "                            print(f\"   Source: {{data.get('data_source', 'unknown')}}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"   Read error: {{e}}\")\n",
    "        \n",
    "        if not found_data:\n",
    "            # List what's actually in the S3 mount\n",
    "            try:\n",
    "                contents = os.listdir(s3_path)\n",
    "                print(f\"Directory {{s3_path}}: {{len(contents)}} items\")\n",
    "                if contents:\n",
    "                    # Show all items (no truncation)\n",
    "                    for item in contents:\n",
    "                        item_path = os.path.join(s3_path, item)\n",
    "                        is_dir = os.path.isdir(item_path)\n",
    "                        size = \"dir\" if is_dir else f\"{{os.path.getsize(item_path)}}b\"\n",
    "                        print(f\"   - {{item}} ({{size}})\")\n",
    "                else:\n",
    "                    print(f\"   (empty)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Cannot list {{s3_path}}: {{e}}\")\n",
    "    else:\n",
    "        print(f\"Path {{s3_path}}: does not exist\")\n",
    "\n",
    "if found_data:\n",
    "    print()\n",
    "    print(\"SUCCESS: Local data is accessible in E2B sandbox!\")\n",
    "    print(f\"Dataset location in sandbox: {{dataset_file}}\")\n",
    "    print(\"File path translation working!\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"Local data not yet synced to S3 or not accessible from sandbox\")\n",
    "    print(\"This could be normal if:\")\n",
    "    print(\"- S3 sync takes time\")\n",
    "    print(\"- Files are in different S3 paths\") \n",
    "    print(\"- Local data needs to be stored in S3-mounted directory\")\n",
    "\n",
    "print()\n",
    "print(\"S3 sync verification completed\")\n",
    "'''\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(s3_verification_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"✅ S3 verification completed\")\n",
    "        print(\"📊 Sandbox output:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        print(f\"\\n🎉 Local data fetching and sync test completed!\")\n",
    "        print(f\"📁 Local files created: {local_data_dir}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Local data fetching test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "local_sync_success = await test_local_data_fetch_and_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "async def test_cross_environment_data_sync():\n    \\\"\\\"\\\"Test data sync between local and E2B environments using unified mount.\\\"\\\"\\\"\\n    print(\\\"🔄 CROSS-ENVIRONMENT DATA SYNC TEST\\\")\\n    print(\\\"=\\\" * 45)\\n    \\n    try:\\n        # Test project setup\\n        test_project_id = f\\\"sync_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\\\"\\n        mount_dir = \\\"/data/sentient\\\"  # Unified mount directory\\n        \\n        print(f\\\"🧪 Test Project ID: {test_project_id}\\\")\\n        print(f\\\"📁 Mount Directory: {mount_dir}\\\")\\n        \\n        # Step 1: Create test data locally (simulating toolkit data)\\n        print(\\\"\\\\n📊 Step 1: Creating test data locally...\\\")\\n        \\n        local_data_dir = f\\\"./notebooks/data/{test_project_id}\\\"\\n        os.makedirs(local_data_dir, exist_ok=True)\\n        \\n        # Create sample toolkit data\\n        test_data = {\\n            'project_id': test_project_id,\\n            'timestamp': datetime.now().isoformat(),\\n            'data_source': 'local_simulation',\\n            'crypto_prices': {\\n                'BTC': 45000.50,\\n                'ETH': 3200.75,\\n                'SOL': 105.25\\n            },\\n            'analysis_ready': True\\n        }\\n        \\n        # Save locally\\n        local_file = os.path.join(local_data_dir, 'test_data.json')\\n        with open(local_file, 'w') as f:\\n            json.dump(test_data, f, indent=2)\\n        \\n        local_size = os.path.getsize(local_file)\\n        print(f\\\"✅ Test data created: {local_file} ({local_size} bytes)\\\")\\n        \\n        # Step 2: Test E2B sandbox access to mount directory\\n        print(\\\"\\\\n☁️ Step 2: Testing E2B sandbox mount access...\\\")\\n        \\n        # Initialize E2B\\n        template_name = \\\"sentient-e2b-s3\\\"\\n        e2b_toolkit = E2BTools(sandbox_options={\\\"template\\\": template_name})\\n        \\n        # Test code to run in E2B sandbox\\n        sync_test_code = f\\\"\\\"\\\"\\nimport os\\nimport json\\nfrom datetime import datetime\\n\\nMOUNT_DIR = \\\"{mount_dir}\\\"\\nPROJECT_ID = \\\"{test_project_id}\\\"\\n\\nprint(\\\"🔄 CROSS-ENVIRONMENT SYNC VERIFICATION\\\")\\nprint(\\\"=\\\" * 45)\\nprint(f\\\"Project: {{PROJECT_ID}}\\\")\\nprint(f\\\"Mount: {{MOUNT_DIR}}\\\")\\n\\n# Check if mount directory exists\\nif os.path.exists(MOUNT_DIR):\\n    print(f\\\"✅ Mount directory exists: {{MOUNT_DIR}}\\\")\\n    \\n    # Create test project structure in E2B\\n    project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\\n    os.makedirs(project_dir, exist_ok=True)\\n    \\n    # Create toolkit directories\\n    toolkit_dirs = ['binance_toolkit', 'coingecko_toolkit', 'results']\\n    created_dirs = []\\n    \\n    for toolkit in toolkit_dirs:\\n        toolkit_dir = os.path.join(project_dir, toolkit)\\n        os.makedirs(toolkit_dir, exist_ok=True)\\n        created_dirs.append(toolkit)\\n        \\n        # Create a test file in each directory\\n        test_file = os.path.join(toolkit_dir, 'e2b_test.json')\\n        e2b_data = {{\\n            'created_in': 'e2b_sandbox',\\n            'toolkit': toolkit,\\n            'project_id': PROJECT_ID,\\n            'timestamp': datetime.now().isoformat(),\\n            'mount_path': MOUNT_DIR\\n        }}\\n        \\n        with open(test_file, 'w') as f:\\n            json.dump(e2b_data, f, indent=2)\\n        \\n        print(f\\\"✅ Created: {{toolkit_dir}}/e2b_test.json\\\")\\n    \\n    # Summary\\n    print(f\\\"\\\\n📊 Created {{len(created_dirs)}} toolkit directories:\\\")\\n    for d in created_dirs:\\n        print(f\\\"   📁 {{d}}\\\")\\n    \\n    # Test write/read operations\\n    results_file = os.path.join(project_dir, 'results', 'sync_verification.json')\\n    verification_data = {{\\n        'test': 'cross_environment_sync',\\n        'created_in': 'e2b_sandbox',\\n        'project_id': PROJECT_ID,\\n        'timestamp': datetime.now().isoformat(),\\n        'status': 'success',\\n        'message': 'E2B can create and write files to mount directory'\\n    }}\\n    \\n    with open(results_file, 'w') as f:\\n        json.dump(verification_data, f, indent=2)\\n    \\n    print(f\\\"\\\\n💾 Verification file created: {{results_file}}\\\")\\n    \\n    # List project contents\\n    if os.path.exists(project_dir):\\n        contents = os.listdir(project_dir)\\n        print(f\\\"\\\\n📋 Project structure created: {{len(contents)}} directories\\\")\\n        for item in contents:\\n            item_path = os.path.join(project_dir, item)\\n            if os.path.isdir(item_path):\\n                files = os.listdir(item_path)\\n                print(f\\\"   📁 {{item}}: {{len(files)}} files\\\")\\n    \\n    print(\\\"\\\\n✅ Cross-environment sync test completed successfully!\\\")\\n    print(\\\"🎉 Data created in E2B will be visible in other environments\\\")\\n    \\nelse:\\n    print(f\\\"❌ Mount directory not found: {{MOUNT_DIR}}\\\")\\n    print(\\\"   Check E2B template configuration\\\")\\n    print(\\\"   Expected mount: /data/sentient\\\")\\n\\\"\\\"\\\"\\n        \\n        result = e2b_toolkit.run_python_code(sync_test_code)\\n        print(\\\"✅ E2B sync test completed\\\")\\n        print(\\\"📊 E2B Output:\\\")\\n        print(\\\"-\\\" * 50)\\n        print(result)\\n        print(\\\"-\\\" * 50)\\n        \\n        print(f\\\"\\\\n🎉 Cross-environment data sync test completed!\\\")\\n        print(f\\\"📁 Local test data: {local_data_dir}\\\")\\n        print(f\\\"☁️ E2B created structure in: {mount_dir}/{test_project_id}\\\")\\n        \\n        return True\\n        \\n    except Exception as e:\\n        print(f\\\"❌ Cross-environment sync test failed: {e}\\\")\\n        import traceback\\n        traceback.print_exc()\\n        return False\\n\\n# Run the test\\nsync_success = await test_cross_environment_data_sync()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_data_analysis_with_mount():\n",
    "    \\\"\\\"\\\"Test data analysis using the unified mount directory.\\\"\\\"\\\"\\n    print(\\\"💰 DATA ANALYSIS WITH UNIFIED MOUNT\\\")\\n    print(\\\"=\\\" * 45)\\n    \\n    try:\\n        # Test project setup\\n        test_project_id = f\\\"analysis_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\\\"\\n        print(f\\\"🧪 Test Project ID: {test_project_id}\\\")\\n        \\n        # Initialize E2B\\n        template_name = \\\"sentient-e2b-s3\\\"\\n        e2b_toolkit = E2BTools(sandbox_options={\\\"template\\\": template_name})\\n        \\n        print(\\\"\\\\n📊 Step 1: Create sample data and analyze in E2B\\\")\\n        \\n        analysis_code = f\\\"\\\"\\\"\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nimport os\\nfrom datetime import datetime, timedelta\\n\\nMOUNT_DIR = \\\"/data/sentient\\\"\\nPROJECT_ID = \\\"{test_project_id}\\\"\\n\\nprint(\\\"💰 CRYPTO DATA ANALYSIS\\\")\\nprint(\\\"=\\\" * 30)\\nprint(f\\\"Project: {{PROJECT_ID}}\\\")\\nprint(f\\\"Mount: {{MOUNT_DIR}}\\\")\\n\\n# Create sample crypto data (simulating toolkit output)\\nsymbols = ['BTC', 'ETH', 'SOL', 'ADA', 'DOT']\\nnp.random.seed(42)  # Reproducible results\\n\\n# Generate realistic crypto price data\\nbase_prices = {{'BTC': 45000, 'ETH': 3000, 'SOL': 100, 'ADA': 0.5, 'DOT': 25}}\\n\\n# Generate 30 days of hourly data\\nhours = 24 * 30  # 30 days\\ntimestamps = [datetime.now() - timedelta(hours=i) for i in range(hours, 0, -1)]\\n\\ncrypto_data = []\\nfor symbol in symbols:\\n    base_price = base_prices[symbol]\\n    \\n    for i, timestamp in enumerate(timestamps):\\n        # Simulate price movement with trend and volatility\\n        trend = 0.001 * np.sin(i / 24)  # Daily cycle\\n        volatility = np.random.normal(0, 0.02)  # 2% volatility\\n        price = base_price * (1 + trend + volatility) ** (i / hours)\\n        \\n        crypto_data.append({{\\n            'timestamp': timestamp.isoformat(),\\n            'symbol': symbol,\\n            'price': round(price, 6),\\n            'volume': np.random.uniform(1000000, 10000000)\\n        }})\\n\\n# Convert to DataFrame\\ndf = pd.DataFrame(crypto_data)\\nprint(f\\\"📊 Generated data: {{len(df)}} records for {{len(symbols)}} symbols\\\")\\n\\n# Perform analysis\\nprint(\\\"\\\\n🔍 Analysis Results:\\\")\\n\\n# Current prices\\ncurrent_prices = df.groupby('symbol')['price'].last()\\nprint(\\\"\\\\n💰 Current Prices:\\\")\\nfor symbol, price in current_prices.items():\\n    print(f\\\"  {{symbol}}: ${{price:,.2f}}\\\")\\n\\n# 24h changes\\ndf['date'] = pd.to_datetime(df['timestamp']).dt.date\\ndaily_prices = df.groupby(['symbol', 'date'])['price'].last().reset_index()\\nlatest_date = daily_prices['date'].max()\\nprevious_date = daily_prices['date'].max() - timedelta(days=1)\\n\\nchanges_24h = {{}}\\nfor symbol in symbols:\\n    latest = daily_prices[(daily_prices['symbol'] == symbol) & (daily_prices['date'] == latest_date)]['price'].iloc[0]\\n    previous = daily_prices[(daily_prices['symbol'] == symbol) & (daily_prices['date'] == previous_date)]['price'].iloc[0]\\n    change_pct = ((latest - previous) / previous) * 100\\n    changes_24h[symbol] = change_pct\\n\\nprint(\\\"\\\\n📈 24h Changes:\\\")\\nfor symbol, change in changes_24h.items():\\n    emoji = \\\"🟢\\\" if change > 0 else \\\"🔴\\\" if change < 0 else \\\"⚪\\\"\\n    print(f\\\"  {{emoji}} {{symbol}}: {{change:+.2f}}%\\\")\\n\\n# Save results to mount directory\\nif os.path.exists(MOUNT_DIR):\\n    results_dir = os.path.join(MOUNT_DIR, PROJECT_ID, \\\"results\\\")\\n    os.makedirs(results_dir, exist_ok=True)\\n    \\n    # Save analysis results\\n    analysis_results = {{\\n        'timestamp': datetime.now().isoformat(),\\n        'project_id': PROJECT_ID,\\n        'analysis_type': 'crypto_market_analysis',\\n        'current_prices': dict(current_prices),\\n        'changes_24h': changes_24h,\\n        'data_points': len(df),\\n        'symbols': symbols\\n    }}\\n    \\n    results_file = os.path.join(results_dir, 'analysis_results.json')\\n    with open(results_file, 'w') as f:\\n        json.dump(analysis_results, f, indent=2)\\n    \\n    print(f\\\"\\\\n💾 Results saved: {{results_file}}\\\")\\n    \\n    # Save raw data\\n    data_file = os.path.join(results_dir, 'crypto_data.csv')\\n    df.to_csv(data_file, index=False)\\n    \\n    print(f\\\"💾 Data saved: {{data_file}} ({{len(df)}} records)\\\")\\n    \\n    # Create data summary for toolkits\\n    for symbol in symbols:\\n        symbol_dir = os.path.join(MOUNT_DIR, PROJECT_ID, f\\\"{{symbol.lower()}}_toolkit\\\")\\n        os.makedirs(symbol_dir, exist_ok=True)\\n        \\n        symbol_data = df[df['symbol'] == symbol].copy()\\n        symbol_file = os.path.join(symbol_dir, f\\\"{{symbol.lower()}}_prices.json\\\")\\n        \\n        symbol_summary = {{\\n            'symbol': symbol,\\n            'current_price': float(current_prices[symbol]),\\n            'change_24h': changes_24h[symbol],\\n            'data_points': len(symbol_data),\\n            'price_range': {{\\n                'min': float(symbol_data['price'].min()),\\n                'max': float(symbol_data['price'].max()),\\n                'avg': float(symbol_data['price'].mean())\\n            }}\\n        }}\\n        \\n        with open(symbol_file, 'w') as f:\\n            json.dump(symbol_summary, f, indent=2)\\n    \\n    print(f\\\"\\\\n📁 Project structure created in {{MOUNT_DIR}}/{{PROJECT_ID}}\\\")\\n    print(\\\"✅ Analysis completed successfully!\\\")\\n    \\n    # Verify structure\\n    project_path = os.path.join(MOUNT_DIR, PROJECT_ID)\\n    if os.path.exists(project_path):\\n        contents = os.listdir(project_path)\\n        print(f\\\"\\\\n📋 Project contents: {{len(contents)}} directories\\\")\\n        for item in contents:\\n            item_path = os.path.join(project_path, item)\\n            if os.path.isdir(item_path):\\n                files = os.listdir(item_path)\\n                print(f\\\"   📁 {{item}}: {{len(files)}} files\\\")\\nelse:\\n    print(\\\"❌ Mount directory not available\\\")\\n    \\nprint(\\\"\\\\n🎉 Data analysis with mount completed!\\\")\\n\\\"\\\"\\\"\\n        \\n        result = e2b_toolkit.run_python_code(analysis_code)\\n        print(f\\\"📊 Analysis result:\\\\n{result}\\\")\\n        \\n        print(\\\"\\\\n🎉 Data analysis with unified mount completed!\\\")\\n        return True\\n        \\n    except Exception as e:\\n        print(f\\\"❌ Data analysis test failed: {e}\\\")\\n        import traceback\\n        traceback.print_exc()\\n        return False\\n\\n# Run the test\\ndata_analysis_success = await test_data_analysis_with_mount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Test 3: Agent-Based Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_agent_code_execution():\n",
    "    \"\"\"Test agent-based code execution with E2B tools and data analysis.\"\"\"\n",
    "    print(\"🤖 AGENT-BASED CODE EXECUTION TEST\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Initialize model\n",
    "        model = LiteLLM(id=\"openrouter/google/gemini-2.5-flash:nitro\")\n",
    "        \n",
    "        # Initialize E2B toolkit with human-readable template name\n",
    "        template_name = \"sentient-e2b-s3\"\n",
    "        e2b_tools = E2BTools(sandbox_options={\"template\": template_name})\n",
    "        reasoning_tools = ReasoningTools()\n",
    "        \n",
    "        # Create data analysis agent\n",
    "        data_agent = AgnoAgent(\n",
    "            model=model,\n",
    "            tools=[e2b_tools, reasoning_tools],\n",
    "            name=\"DataAnalysisAgent\",\n",
    "            system_message=\"\"\"\n",
    "You are an expert data analyst with access to secure E2B sandbox environments for code execution.\n",
    "\n",
    "Key capabilities:\n",
    "1. E2B sandbox for secure Python code execution (template: sentient-e2b-s3)\n",
    "2. S3 storage mounted at /workspace/data and /workspace/results (goofys)\n",
    "3. Data analysis libraries: pandas, numpy, matplotlib, seaborn\n",
    "4. Structured reasoning tools\n",
    "\n",
    "File Path Translation Rules:\n",
    "- Local files sync to S3 bucket automatically\n",
    "- In E2B sandbox, access S3 data via: /home/user/s3-bucket/ or /workspace/\n",
    "- Save analysis results to /workspace/results/ for persistence\n",
    "\n",
    "When analyzing data:\n",
    "1. Use E2B sandbox for all computations\n",
    "2. Access data files from mounted S3 paths\n",
    "3. Save results to /workspace/results/ for sync back to local\n",
    "4. Provide clear explanations of findings\n",
    "5. Include proper error handling\n",
    "\n",
    "Always be thorough and explain your analysis steps.\n",
    "\"\"\"\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Data analysis agent initialized with E2B tools\")\n",
    "        \n",
    "        # Test agent with crypto data analysis task\n",
    "        print(\"\\n📊 Testing agent with crypto data analysis...\")\n",
    "        \n",
    "        analysis_task = f\"\"\"\n",
    "Please perform a comprehensive cryptocurrency data analysis using the E2B sandbox.\n",
    "\n",
    "Tasks:\n",
    "1. Check what data is available in the mounted S3 storage paths:\n",
    "   - /home/user/s3-bucket/\n",
    "   - /workspace/data/\n",
    "   - /workspace/results/\n",
    "   \n",
    "2. Generate sample crypto market data for BTC, ETH, SOL if no real data found\n",
    "\n",
    "3. Perform technical analysis:\n",
    "   - Calculate price changes and volatility\n",
    "   - Compute moving averages\n",
    "   - Identify trends and patterns\n",
    "   \n",
    "4. Create visualizations (save as PNG files)\n",
    "\n",
    "5. Save comprehensive analysis results to:\n",
    "   - /workspace/results/crypto_analysis_report.json\n",
    "   - /workspace/results/analysis_summary.csv\n",
    "\n",
    "6. Provide actionable insights and risk assessment\n",
    "\n",
    "Use only the E2B sandbox for all computations. Template: {template_name}\n",
    "Current timestamp: {datetime.now().isoformat()}\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"🔄 Agent task: {analysis_task[:200]}...\")\n",
    "        print(\"\\n🤖 Agent is working (this may take a few minutes)...\")\n",
    "        \n",
    "        # Execute the task with timeout\n",
    "        try:\n",
    "            response = await asyncio.wait_for(\n",
    "                data_agent.arun(analysis_task), \n",
    "                timeout=300  # 5 minute timeout\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n🤖 Agent Response:\\n{response}\")\n",
    "            \n",
    "            # Verify results were created in E2B sandbox\n",
    "            print(\"\\n🔍 Verifying agent results in E2B sandbox...\")\n",
    "            verification_code = '''\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "print(\"🔍 AGENT RESULTS VERIFICATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check for expected output files\n",
    "expected_files = [\n",
    "    \"/workspace/results/crypto_analysis_report.json\",\n",
    "    \"/workspace/results/analysis_summary.csv\"\n",
    "]\n",
    "\n",
    "results_found = 0\n",
    "for file_path in expected_files:\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"✅ Found: {file_path} ({size} bytes)\")\n",
    "        results_found += 1\n",
    "        \n",
    "        # Try to read JSON files\n",
    "        if file_path.endswith('.json'):\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                print(f\"   📊 JSON structure: {list(data.keys()) if isinstance(data, dict) else type(data).__name__}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ JSON read error: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ Missing: {file_path}\")\n",
    "\n",
    "# Check for any PNG files (visualizations)\n",
    "png_files = glob.glob(\"/workspace/results/*.png\")\n",
    "if png_files:\n",
    "    print(f\"\\\\n📈 Visualizations found: {len(png_files)} PNG files\")\n",
    "    for png in png_files:\n",
    "        size = os.path.getsize(png)\n",
    "        print(f\"   - {os.path.basename(png)}: {size} bytes\")\n",
    "else:\n",
    "    print(\"\\\\n📈 No PNG visualizations found\")\n",
    "\n",
    "# List all files in results directory\n",
    "results_dir = \"/workspace/results\"\n",
    "if os.path.exists(results_dir):\n",
    "    all_files = os.listdir(results_dir)\n",
    "    print(f\"\\\\n📁 All files in {results_dir}: {len(all_files)} items\")\n",
    "    for f in all_files[:10]:  # Show first 10\n",
    "        path = os.path.join(results_dir, f)\n",
    "        size = os.path.getsize(path) if os.path.isfile(path) else \"dir\"\n",
    "        print(f\"   - {f}: {size}\")\n",
    "    if len(all_files) > 10:\n",
    "        print(f\"   ... and {len(all_files)-10} more\")\n",
    "\n",
    "print(f\"\\\\n📊 Agent Results Summary: {results_found}/{len(expected_files)} expected files found\")\n",
    "print(\"✅ Verification completed\")\n",
    "'''\n",
    "            \n",
    "            verification_result = e2b_tools.run_python_code(verification_code)\n",
    "            if verification_result and verification_result.get('status') == 'success':\n",
    "                print(\"✅ Results verification completed\")\n",
    "                print(\"📊 Verification output:\", verification_result.get('output', 'No output'))\n",
    "            else:\n",
    "                print(\"❌ Results verification failed:\", verification_result)\n",
    "            \n",
    "            print(\"\\n🎉 Agent-based code execution test completed!\")\n",
    "            return True\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(\"❌ Agent task timed out after 5 minutes\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Agent-based code execution test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "agent_success = await test_agent_code_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏠 Test 5: Local Dependencies Setup Check\n",
    "\n",
    "Since this notebook runs locally, we should check if goofys and other S3 dependencies are available locally for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "def check_local_dependencies():\n",
    "    \"\"\"Check if local dependencies are available for E2B template building.\"\"\"\n",
    "    print(\"🏠 LOCAL DEPENDENCIES CHECK\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    dependencies = {\n",
    "        'e2b': 'E2B CLI for template management',\n",
    "        'docker': 'Docker for building E2B templates',\n",
    "        'aws': 'AWS CLI for S3 operations',\n",
    "        'goofys': 'FUSE filesystem for S3 mounting (optional for local testing)',\n",
    "        's3fs': 'Alternative S3 filesystem (optional for local testing)'\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for tool, description in dependencies.items():\n",
    "        path = shutil.which(tool)\n",
    "        if path:\n",
    "            try:\n",
    "                # Get version info\n",
    "                if tool == 'e2b':\n",
    "                    result = subprocess.run([tool, '--version'], capture_output=True, text=True, timeout=5)\n",
    "                elif tool == 'docker':\n",
    "                    result = subprocess.run([tool, '--version'], capture_output=True, text=True, timeout=5)\n",
    "                elif tool == 'aws':\n",
    "                    result = subprocess.run([tool, '--version'], capture_output=True, text=True, timeout=5)\n",
    "                else:\n",
    "                    result = subprocess.run([tool, '--help'], capture_output=True, text=True, timeout=5)\n",
    "                \n",
    "                version = result.stdout.split('\\n')[0] if result.stdout else result.stderr.split('\\n')[0]\n",
    "                results[tool] = {'available': True, 'path': path, 'version': version}\n",
    "                print(f\"✅ {tool}: {path}\")\n",
    "                if version.strip():\n",
    "                    print(f\"   Version: {version.strip()}\")\n",
    "            except Exception as e:\n",
    "                results[tool] = {'available': True, 'path': path, 'error': str(e)}\n",
    "                print(f\"⚠️  {tool}: {path} (version check failed: {e})\")\n",
    "        else:\n",
    "            results[tool] = {'available': False}\n",
    "            print(f\"❌ {tool}: not found - {description}\")\n",
    "    \n",
    "    # Installation suggestions\n",
    "    print(\"\\n📋 Installation suggestions:\")\n",
    "    install_commands = {\n",
    "        'e2b': 'npm install -g @e2b/cli',\n",
    "        'docker': 'Install Docker Desktop from https://docker.com',\n",
    "        'aws': 'pip install awscli',\n",
    "        'goofys': 'Download from https://github.com/kahing/goofys/releases',\n",
    "        's3fs': 'brew install s3fs (macOS) or apt-get install s3fs (Ubuntu)'\n",
    "    }\n",
    "    \n",
    "    for tool, available_info in results.items():\n",
    "        if not available_info['available']:\n",
    "            print(f\"  {tool}: {install_commands.get(tool, 'See documentation')}\")\n",
    "    \n",
    "    # Check E2B authentication if available\n",
    "    if results.get('e2b', {}).get('available'):\n",
    "        print(\"\\n🔐 E2B Authentication Check:\")\n",
    "        try:\n",
    "            result = subprocess.run(['e2b', 'auth', 'whoami'], capture_output=True, text=True, timeout=10)\n",
    "            if result.returncode == 0:\n",
    "                print(f\"✅ E2B authenticated: {result.stdout.strip()}\")\n",
    "            else:\n",
    "                print(f\"❌ E2B not authenticated: {result.stderr.strip()}\")\n",
    "                print(f\"   Run: e2b auth login\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  E2B auth check failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "local_deps = check_local_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Test 6: E2B Template Building (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_e2b_template_building():\n",
    "    \"\"\"Test E2B template building if dependencies are available.\"\"\"\n",
    "    print(\"🔧 E2B TEMPLATE BUILDING TEST\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if not local_deps.get('e2b', {}).get('available'):\n",
    "        print(\"⚠️  E2B CLI not available - skipping template building test\")\n",
    "        return False\n",
    "        \n",
    "    if not local_deps.get('docker', {}).get('available'):\n",
    "        print(\"⚠️  Docker not available - skipping template building test\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Check if our custom template exists\n",
    "        template_id = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        print(f\"\\n🔍 Checking if template '{template_id}' exists...\")\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            ['e2b', 'template', 'list'], \n",
    "            capture_output=True, text=True, timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            templates = result.stdout\n",
    "            if template_id in templates:\n",
    "                print(f\"✅ Template '{template_id}' exists\")\n",
    "                print(\"\\n📋 Available templates:\")\n",
    "                for line in templates.split('\\n'):\n",
    "                    if line.strip():\n",
    "                        print(f\"  {line}\")\n",
    "            else:\n",
    "                print(f\"❌ Template '{template_id}' not found\")\n",
    "                print(\"\\n🏗️  To build the template, run:\")\n",
    "                print(f\"  cd docker/e2b-sandbox\")\n",
    "                print(f\"  e2b template build -n {template_id}\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to list templates: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Template check failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Only run if we have the basic tools\n",
    "if local_deps.get('e2b', {}).get('available'):\n",
    "    template_success = test_e2b_template_building()\n",
    "else:\n",
    "    print(\"⚠️  Skipping template building test - E2B CLI not available\")\n",
    "    template_success = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary():\n",
    "    \\\"\\\"\\\"Print a summary of all tests.\\\"\\\"\\\"\\n    print(\\\"📊 E2B UNIFIED MOUNT TEST SUMMARY\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    tests = [\\n        (\\\"Unified Mount Directory\\\", mount_success if 'mount_success' in locals() else False),\\n        (\\\"Data Analysis with Mount\\\", data_analysis_success if 'data_analysis_success' in locals() else False),\\n    ]\\n    \\n    passed = 0\\n    total = 0\\n    \\n    for test_name, result in tests:\\n        if result is None:\\n            status = \\\"⏭️  SKIPPED\\\"\\n        elif result:\\n            status = \\\"✅ PASSED\\\"\\n            passed += 1\\n            total += 1\\n        else:\\n            status = \\\"❌ FAILED\\\"\\n            total += 1\\n        \\n        print(f\\\"{status} {test_name}\\\")\\n    \\n    print(f\\\"\\\\n📈 Results: {passed}/{total} tests passed\\\")\\n    \\n    if passed == total and total > 0:\\n        print(\\\"🎉 All tests passed! Unified mount directory is working correctly.\\\")\\n        print(\\\"✅ Cross-environment data persistence is functional\\\")\\n        print(\\\"✅ Project structure creation is working\\\")\\n    elif passed > 0:\\n        print(f\\\"⚠️  Some tests failed. Check the output above for details.\\\")\\n    else:\\n        print(f\\\"❌ Tests failed. Check your E2B template and mount configuration.\\\")\\n    \\n    # Recommendations\\n    print(\\\"\\\\n💡 Key Points:\\\")\\n    print(\\\"  📁 Mount directory: /data/sentient (unified across all environments)\\\")\\n    print(\\\"  🗂️  Project structure: /data/sentient/{project_id}/{toolkit_name|results}/\\\")\\n    print(\\\"  🔄 Data flows: Local → S3 → E2B sandbox (same paths)\\\")\\n    print(\\\"  📊 Results: Saved to /data/sentient/{project_id}/results/\\\")\\n    \\n    print(\\\"\\\\n🔗 Next Steps:\\\")\\n    print(\\\"  1. Verify mount is active locally: ls $HOME/data/sentient\\\")\\n    print(\\\"  2. Check S3 sync: aws s3 ls s3://your-bucket\\\")\\n    print(\\\"  3. Use BaseDataToolkit with CURRENT_PROJECT_ID env var\\\")\\n    print(\\\"  4. Results will appear in all environments at same paths\\\")\\n\\nprint_test_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def print_test_summary():\n    \\\"\\\"\\\"Print a summary of all tests.\\\"\\\"\\\"\\n    print(\\\"📊 E2B UNIFIED MOUNT TEST SUMMARY\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    tests = [\\n        (\\\"Unified Mount Directory\\\", mount_success if 'mount_success' in globals() else False),\\n        (\\\"Cross-Environment Data Sync\\\", sync_success if 'sync_success' in globals() else False),\\n        (\\\"Data Analysis with Mount\\\", data_analysis_success if 'data_analysis_success' in globals() else False),\\n    ]\\n    \\n    passed = 0\\n    total = 0\\n    \\n    for test_name, result in tests:\\n        if result is None:\\n            status = \\\"⏭️  SKIPPED\\\"\\n        elif result:\\n            status = \\\"✅ PASSED\\\"\\n            passed += 1\\n            total += 1\\n        else:\\n            status = \\\"❌ FAILED\\\"\\n            total += 1\\n        \\n        print(f\\\"{status} {test_name}\\\")\\n    \\n    print(f\\\"\\\\n📈 Results: {passed}/{total} tests passed\\\")\\n    \\n    if passed == total and total > 0:\\n        print(\\\"🎉 All tests passed! Unified mount directory is working correctly.\\\")\\n        print(\\\"✅ Cross-environment data persistence is functional\\\")\\n        print(\\\"✅ Project structure creation is working\\\")\\n    elif passed > 0:\\n        print(f\\\"⚠️  Some tests failed. Check the output above for details.\\\")\\n    else:\\n        print(f\\\"❌ Tests failed. Check your E2B template and mount configuration.\\\")\\n    \\n    # Recommendations\\n    print(\\\"\\\\n💡 Key Points:\\\")\\n    print(\\\"  📁 Mount directory: /data/sentient (unified across all environments)\\\")\\n    print(\\\"  🗂️  Project structure: /data/sentient/{project_id}/{toolkit_name|results}/\\\")\\n    print(\\\"  🔄 Data flows: Local → S3 → E2B sandbox (same paths)\\\")\\n    print(\\\"  📊 Results: Saved to /data/sentient/{project_id}/results/\\\")\\n    \\n    print(\\\"\\\\n🔗 Next Steps:\\\")\\n    print(\\\"  1. Verify mount is active locally: ls $HOME/data/sentient\\\")\\n    print(\\\"  2. Check S3 sync: aws s3 ls s3://your-bucket\\\")\\n    print(\\\"  3. Use BaseDataToolkit with CURRENT_PROJECT_ID env var\\\")\\n    print(\\\"  4. Results will appear in all environments at same paths\\\")\\n\\nprint_test_summary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug E2B output truncation - let's examine what E2BTools actually returns\n",
    "print(\"🔍 DEBUGGING E2B OUTPUT TRUNCATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "async def debug_e2b_output():\n",
    "    try:\n",
    "        # Initialize E2B with our template\n",
    "        from agno.tools.e2b import E2BTools\n",
    "        template_name = \"sentient-e2b-s3\"\n",
    "        e2b_toolkit = E2BTools(sandbox_options={\"template\": template_name})\n",
    "        \n",
    "        print(f\"🎯 Using template: {template_name}\")\n",
    "        \n",
    "        # Simple test code that should produce known output\n",
    "        test_code = '''\n",
    "print(\"🔍 DEBUG OUTPUT INVESTIGATION\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Line 1: This is a test line\")\n",
    "print(\"Line 2: Testing emoji display 🏗️ ✅ 📁\")\n",
    "print(\"Line 3: Unicode characters: 🚀 💰 🎉\")\n",
    "print(\"Line 4: Long line with lots of content to see if truncation happens here with more text\")\n",
    "print(\"Line 5: More content to fill output\")\n",
    "print(\"Line 6: Even more content\")\n",
    "print(\"Line 7: Keep adding lines\")\n",
    "print(\"Line 8: To test truncation\")\n",
    "print(\"Line 9: Nearly done\")\n",
    "print(\"Line 10: Final line - END OF OUTPUT\")\n",
    "'''\n",
    "        \n",
    "        print(\"📝 Executing test code in E2B sandbox...\")\n",
    "        result = e2b_toolkit.run_python_code(test_code)\n",
    "        \n",
    "        print(\"\\n📊 RAW RESULT ANALYSIS:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Type: {type(result)}\")\n",
    "        print(f\"Length: {len(str(result))} characters\")\n",
    "        print(f\"First 200 chars: {str(result)[:200]}...\")\n",
    "        print(f\"Last 200 chars: ...{str(result)[-200:]}\")\n",
    "        \n",
    "        print(\"\\n📋 DETAILED STRUCTURE:\")\n",
    "        if isinstance(result, list):\n",
    "            print(f\"List length: {len(result)}\")\n",
    "            for i, item in enumerate(result):\n",
    "                print(f\"  Item {i}: {type(item)} - {len(str(item))} chars\")\n",
    "                if isinstance(item, str) and \"stdout:\" in item:\n",
    "                    # Try to extract just the stdout part\n",
    "                    import re\n",
    "                    stdout_match = re.search(r\"stdout:\\s*\\[(.*?)\\]\", item, re.DOTALL)\n",
    "                    if stdout_match:\n",
    "                        stdout_content = stdout_match.group(1)\n",
    "                        print(f\"    Stdout length: {len(stdout_content)} chars\")\n",
    "                        print(f\"    Stdout preview: {stdout_content[:100]}...\")\n",
    "        \n",
    "        print(\"\\n🔧 DECODED OUTPUT:\")\n",
    "        print(\"-\" * 40)\n",
    "        decoded = decode_e2b_output(result)\n",
    "        print(f\"Decoded length: {len(decoded)} characters\")\n",
    "        print(\"Decoded content:\")\n",
    "        print(decoded)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if the output contains all expected lines\n",
    "        expected_lines = [f\"Line {i}:\" for i in range(1, 11)]\n",
    "        found_lines = []\n",
    "        for line_prefix in expected_lines:\n",
    "            if line_prefix in decoded:\n",
    "                found_lines.append(line_prefix)\n",
    "        \n",
    "        print(f\"\\n📈 TRUNCATION ANALYSIS:\")\n",
    "        print(f\"Expected lines: {len(expected_lines)}\")\n",
    "        print(f\"Found lines: {len(found_lines)}\")\n",
    "        print(f\"Missing lines: {set(expected_lines) - set(found_lines)}\")\n",
    "        \n",
    "        if len(found_lines) < len(expected_lines):\n",
    "            print(\"❌ OUTPUT IS TRUNCATED!\")\n",
    "            print(\"🔍 Truncation occurs before all lines are included\")\n",
    "        else:\n",
    "            print(\"✅ All expected lines found - no truncation detected\")\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Debug failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the debug\n",
    "debug_result = await debug_e2b_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to access the underlying E2B client directly to bypass AgnoAgent wrapper\n",
    "print(\"🔗 DIRECT E2B CLIENT ACCESS TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "async def test_direct_e2b_client():\n",
    "    try:\n",
    "        # Try to import e2b directly\n",
    "        from e2b_code_interpreter import Sandbox\n",
    "        \n",
    "        # Create direct E2B client\n",
    "        template_id = \"sentient-e2b-s3\"\n",
    "        print(f\"🎯 Connecting directly to template: {template_id}\")\n",
    "        \n",
    "        # Create sandbox directly\n",
    "        sandbox = Sandbox(template=template_id)\n",
    "        \n",
    "        print(\"✅ Direct E2B sandbox created\")\n",
    "        \n",
    "        # Test the same code that showed truncation\n",
    "        test_code = '''\n",
    "print(\"🔍 DIRECT E2B CLIENT TEST\")\n",
    "print(\"=\" * 30)\n",
    "print(\"Line 1: This is a test line\")\n",
    "print(\"Line 2: Testing emoji display 🏗️ ✅ 📁\")\n",
    "print(\"Line 3: Unicode characters: 🚀 💰 🎉\")\n",
    "print(\"Line 4: Long line with lots of content to see if truncation happens here\")\n",
    "print(\"Line 5: More content to fill output\")\n",
    "print(\"Line 6: Even more content\")\n",
    "print(\"Line 7: Keep adding lines\")\n",
    "print(\"Line 8: To test truncation\")\n",
    "print(\"Line 9: Nearly done\")\n",
    "print(\"Line 10: Final line - END OF OUTPUT\")\n",
    "'''\n",
    "        \n",
    "        print(\"📝 Executing code via direct E2B client...\")\n",
    "        result = sandbox.run_code(test_code)\n",
    "        \n",
    "        print(\"\\n📊 DIRECT E2B RESULT:\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Type: {type(result)}\")\n",
    "        print(f\"Dir: {[attr for attr in dir(result) if not attr.startswith('_')][:10]}\")\n",
    "        \n",
    "        # Check if result has stdout/stderr attributes\n",
    "        if hasattr(result, 'stdout'):\n",
    "            print(f\"Stdout: {result.stdout}\")\n",
    "        if hasattr(result, 'stderr'):\n",
    "            print(f\"Stderr: {result.stderr}\")\n",
    "        if hasattr(result, 'output'):\n",
    "            print(f\"Output: {result.output}\")\n",
    "        if hasattr(result, 'logs'):\n",
    "            print(f\"Logs: {result.logs}\")\n",
    "        \n",
    "        # Check the actual content\n",
    "        output_content = None\n",
    "        if hasattr(result, 'stdout') and result.stdout:\n",
    "            output_content = result.stdout\n",
    "        elif hasattr(result, 'output') and result.output:\n",
    "            output_content = result.output\n",
    "        else:\n",
    "            output_content = str(result)\n",
    "        \n",
    "        print(f\"\\n📋 OUTPUT CONTENT:\")\n",
    "        print(f\"Length: {len(output_content)} characters\")\n",
    "        print(\"Content:\")\n",
    "        print(output_content)\n",
    "        \n",
    "        # Count lines to check for truncation\n",
    "        lines = output_content.split('\\n')\n",
    "        expected_lines = [f\"Line {i}:\" for i in range(1, 11)]\n",
    "        found_lines = [line for line in lines if any(exp in line for exp in expected_lines)]\n",
    "        \n",
    "        print(f\"\\n📈 TRUNCATION ANALYSIS (Direct Client):\")\n",
    "        print(f\"Total output lines: {len(lines)}\")\n",
    "        print(f\"Expected test lines: {len(expected_lines)}\")\n",
    "        print(f\"Found test lines: {len(found_lines)}\")\n",
    "        \n",
    "        if len(found_lines) < len(expected_lines):\n",
    "            print(\"❌ TRUNCATION CONFIRMED IN DIRECT E2B CLIENT!\")\n",
    "        else:\n",
    "            print(\"✅ No truncation in direct E2B client\")\n",
    "        \n",
    "        # Clean up\n",
    "        sandbox.kill()\n",
    "        print(\"🧹 Sandbox cleaned up\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ Cannot import e2b directly - not available\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Direct E2B test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run direct E2B test\n",
    "direct_e2b_result = await test_direct_e2b_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced E2B output decoder with better display handling\n",
    "def enhanced_decode_e2b_output(raw_result: Any, max_display_length: int = None) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced decoder for E2B output with optional display length limiting.\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Raw output from E2BTools.run_python_code()\n",
    "        max_display_length: Optional max characters to display (None = no limit)\n",
    "        \n",
    "    Returns:\n",
    "        Properly decoded and formatted output string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the original decoder\n",
    "        decoded = decode_e2b_output(raw_result)\n",
    "        \n",
    "        # Apply display length limit if specified\n",
    "        if max_display_length and len(decoded) > max_display_length:\n",
    "            truncated = decoded[:max_display_length]\n",
    "            lines_count = decoded.count('\\n')\n",
    "            truncated_lines_count = truncated.count('\\n')\n",
    "            \n",
    "            return f\"{truncated}\\n\\n[...OUTPUT TRUNCATED FOR DISPLAY...]\\n\" \\\n",
    "                   f\"[Full output: {len(decoded)} characters, {lines_count} lines]\\n\" \\\n",
    "                   f\"[Displayed: {len(truncated)} characters, {truncated_lines_count} lines]\"\n",
    "        \n",
    "        return decoded\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Error in enhanced decoder: {e}]\\n\\nFalling back to raw result:\\n{str(raw_result)}\"\n",
    "\n",
    "\n",
    "def analyze_e2b_output(raw_result: Any) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze E2B output structure and content for debugging.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'raw_type': type(raw_result).__name__,\n",
    "        'raw_length': len(str(raw_result)),\n",
    "        'has_content': False,\n",
    "        'decoded_length': 0,\n",
    "        'line_count': 0,\n",
    "        'contains_unicode': False,\n",
    "        'truncation_detected': False,\n",
    "        'content_preview': '',\n",
    "        'full_content': ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        decoded = decode_e2b_output(raw_result)\n",
    "        analysis['has_content'] = bool(decoded.strip())\n",
    "        analysis['decoded_length'] = len(decoded)\n",
    "        analysis['line_count'] = decoded.count('\\n')\n",
    "        analysis['contains_unicode'] = any(ord(char) > 127 for char in decoded)\n",
    "        analysis['content_preview'] = decoded[:200] + \"...\" if len(decoded) > 200 else decoded\n",
    "        analysis['full_content'] = decoded\n",
    "        \n",
    "        # Check for common truncation indicators\n",
    "        truncation_indicators = ['...', '[truncated]', 'OUTPUT_LIMIT', 'MAX_LENGTH']\n",
    "        analysis['truncation_detected'] = any(indicator.lower() in decoded.lower() \n",
    "                                            for indicator in truncation_indicators)\n",
    "        \n",
    "    except Exception as e:\n",
    "        analysis['error'] = str(e)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Test the enhanced decoder\n",
    "print(\"🔧 ENHANCED E2B OUTPUT DECODER TEST\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Test with a sample from our debug results\n",
    "if 'debug_result' in locals() and debug_result:\n",
    "    print(\"✅ Using debug_result for enhanced decoder test\")\n",
    "    \n",
    "    # Analyze the output\n",
    "    analysis = analyze_e2b_output(debug_result)\n",
    "    \n",
    "    print(\"📊 OUTPUT ANALYSIS:\")\n",
    "    for key, value in analysis.items():\n",
    "        if key != 'full_content':  # Don't print the full content here\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎯 ENHANCED DECODED OUTPUT (limited to 500 chars):\")\n",
    "    print(\"-\" * 50)\n",
    "    enhanced_output = enhanced_decode_e2b_output(debug_result, max_display_length=500)\n",
    "    print(enhanced_output)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"\\n🔬 FULL OUTPUT AVAILABLE:\")\n",
    "    print(f\"  Full content length: {len(analysis['full_content'])} characters\")\n",
    "    print(f\"  Lines in full content: {analysis['line_count']}\")\n",
    "    print(f\"  No actual truncation: {not analysis['truncation_detected']}\")\n",
    "else:\n",
    "    print(\"⚠️  No debug_result available. Run the debug cell first.\")\n",
    "\n",
    "print(f\"\\n✅ Enhanced decoder ready for use!\")\n",
    "print(\"🎉 This should resolve any display-related 'truncation' issues.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}