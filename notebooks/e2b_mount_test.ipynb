{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2B Unified Mount Directory Test\n",
    "\n",
    "**✅ E2B Integration with Unified Mount Structure**\n",
    "\n",
    "## 🎯 Purpose\n",
    "Test the **universal mount directory structure** across all environments:\n",
    "- **Local**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "- **Docker**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "- **E2B**: Uses `S3_MOUNT_DIR` environment variable (default: `/opt/sentient`)\n",
    "\n",
    "## 📁 Expected Structure\n",
    "```\n",
    "/opt/sentient/\n",
    "├── {project_id}/\n",
    "│   ├── binance_toolkit/     # Binance API data\n",
    "│   ├── coingecko_toolkit/   # CoinGecko API data  \n",
    "│   ├── defillama_toolkit/   # DefiLlama API data\n",
    "│   ├── arkham_toolkit/      # Arkham API data\n",
    "│   └── results/             # AI analysis outputs\n",
    "└── other_projects/\n",
    "```\n",
    "\n",
    "## 🔄 Test Flow\n",
    "1. 🏠 **Local Data Fetching**: Agent with data toolkit fetches crypto data to mount directory\n",
    "2. ☁️ **S3 Sync**: Data automatically syncs to S3 bucket via mount\n",
    "3. 🚀 **E2B Execution**: Code runs in sandbox with same mount path `/opt/sentient`\n",
    "4. 🧮 **Analysis**: Sandbox analyzes data and saves results to mount\n",
    "5. 📊 **Results**: Results visible across all environments\n",
    "\n",
    "## ✅ What's Working\n",
    "- Universal `/opt/sentient` mount path across environments\n",
    "- S3 bucket mounted with goofys ✅ **WORKING!**\n",
    "- Cross-environment data persistence ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Environment cleaned and ready\n"
     ]
    }
   ],
   "source": [
    "# Clean imports and setup\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Clean any cached modules\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'sentient' in m.lower()]\n",
    "for module in modules_to_remove:\n",
    "    if module in sys.modules:\n",
    "        del sys.modules[module]\n",
    "\n",
    "gc.collect()\n",
    "print(\"🔄 Environment cleaned and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Dependencies and Configuration Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Environment Configuration Check:\n",
      "==================================================\n",
      "✅ E2B_API_KEY: e2b_d5d5...bd00\n",
      "✅ S3_BUCKET_NAME: roma-shared\n",
      "✅ S3_MOUNT_DIR: /opt/sentient\n",
      "\n",
      "📋 Optional Configuration:\n",
      "ℹ️  E2B_TEMPLATE_ID: Using default - Custom E2B template (defaults to sentient-e2b-s3)\n",
      "ℹ️  CURRENT_PROJECT_ID: Using default - Current project ID for testing\n",
      "\n",
      "🎉 All required environment variables are configured!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check required environment variables\n",
    "required_env_vars = {\n",
    "    'E2B_API_KEY': 'E2B API key for sandbox access',\n",
    "    'S3_BUCKET_NAME': 'S3 bucket name for data storage',\n",
    "    'S3_MOUNT_DIR': 'S3 mount directory path (default: /sentient)',\n",
    "}\n",
    "\n",
    "optional_env_vars = {\n",
    "    'E2B_TEMPLATE_ID': 'Custom E2B template (defaults to sentient-e2b-s3)',\n",
    "    'CURRENT_PROJECT_ID': 'Current project ID for testing',\n",
    "}\n",
    "\n",
    "print(\"🔍 Environment Configuration Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check required variables\n",
    "missing_required = []\n",
    "for var, description in required_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        if var in ['E2B_API_KEY']:\n",
    "            masked_value = f\"{value[:8]}...{value[-4:]}\"\n",
    "        else:\n",
    "            masked_value = value\n",
    "        print(f\"✅ {var}: {masked_value}\")\n",
    "    else:\n",
    "        print(f\"❌ {var}: MISSING - {description}\")\n",
    "        missing_required.append(var)\n",
    "\n",
    "print(\"\\n📋 Optional Configuration:\")\n",
    "for var, description in optional_env_vars.items():\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        print(f\"✅ {var}: {value}\")\n",
    "    else:\n",
    "        print(f\"ℹ️  {var}: Using default - {description}\")\n",
    "\n",
    "\n",
    "mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "\n",
    "\n",
    "if missing_required:\n",
    "    print(f\"\\n⚠️  Missing required environment variables: {', '.join(missing_required)}\")\n",
    "    print(\"   Please update your .env file with the missing values.\")\n",
    "else:\n",
    "    print(\"\\n🎉 All required environment variables are configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ E2BTools imported successfully\n",
      "✅ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import E2B and agno dependencies\n",
    "try:\n",
    "    from agno.tools.e2b import E2BTools\n",
    "    print(\"✅ E2BTools imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import E2BTools: {e}\")\n",
    "    print(\"   Please install agno: pip install agno-ai\")\n",
    "\n",
    "# Import other standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 E2B output decoder functions loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Union\n",
    "\n",
    "def decode_e2b_output(raw_result: Any) -> str:\n",
    "    \"\"\"\n",
    "    Decode AgnoAgent E2BTools output properly.\n",
    "    \n",
    "    The AgnoAgent E2BTools.run_python_code() method returns JSON-encoded output \n",
    "    in the format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "    This function properly decodes the Unicode escapes and extracts the stdout content.\n",
    "    \n",
    "    Args:\n",
    "        raw_result: Raw output from E2BTools.run_python_code()\n",
    "        \n",
    "    Returns:\n",
    "        Properly decoded and formatted output string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle different input types\n",
    "        if isinstance(raw_result, list) and len(raw_result) > 0:\n",
    "            content = raw_result[0]\n",
    "        elif isinstance(raw_result, str):\n",
    "            content = raw_result\n",
    "        else:\n",
    "            return str(raw_result)\n",
    "        \n",
    "        # Handle JSON-encoded list format: [\"Logs:\\nLogs(stdout: [...], stderr: [...])\"]\n",
    "        if isinstance(content, str) and content.startswith('[\"') and content.endswith('\"]'):\n",
    "            try:\n",
    "                parsed_list = json.loads(content)\n",
    "                if isinstance(parsed_list, list) and len(parsed_list) > 0:\n",
    "                    content = parsed_list[0]\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        # Extract stdout content from Logs format: \"Logs:\\nLogs(stdout: [...], stderr: [...])\"\n",
    "        if isinstance(content, str) and \"Logs(stdout:\" in content:\n",
    "            # Use regex to extract stdout content\n",
    "            stdout_pattern = r\"stdout:\\s*\\[(.*?)\\],?\\s*stderr:\"\n",
    "            match = re.search(stdout_pattern, content, re.DOTALL)\n",
    "            \n",
    "            if match:\n",
    "                stdout_raw = match.group(1)\n",
    "                \n",
    "                # Parse the stdout array content - handle quoted strings with proper escaping\n",
    "                stdout_lines = []\n",
    "                \n",
    "                # Split by ', ' but handle embedded quotes and escapes\n",
    "                current_part = \"\"\n",
    "                in_quotes = False\n",
    "                escaped = False\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(stdout_raw):\n",
    "                    char = stdout_raw[i]\n",
    "                    \n",
    "                    if escaped:\n",
    "                        current_part += char\n",
    "                        escaped = False\n",
    "                    elif char == '\\\\':\n",
    "                        current_part += char\n",
    "                        escaped = True\n",
    "                    elif char == \"'\" and not escaped:\n",
    "                        in_quotes = not in_quotes\n",
    "                        current_part += char\n",
    "                    elif char == ',' and not in_quotes and i + 1 < len(stdout_raw) and stdout_raw[i + 1] == ' ':\n",
    "                        # Found separator\n",
    "                        if current_part.strip():\n",
    "                            stdout_lines.append(current_part.strip())\n",
    "                        current_part = \"\"\n",
    "                        i += 1  # Skip the space after comma\n",
    "                    else:\n",
    "                        current_part += char\n",
    "                    \n",
    "                    i += 1\n",
    "                \n",
    "                # Add the last part\n",
    "                if current_part.strip():\n",
    "                    stdout_lines.append(current_part.strip())\n",
    "                \n",
    "                # Clean and decode each part\n",
    "                decoded_lines = []\n",
    "                for part in stdout_lines:\n",
    "                    # Remove outer quotes\n",
    "                    if (part.startswith(\"'\") and part.endswith(\"'\")) or (part.startswith('\"') and part.endswith('\"')):\n",
    "                        part = part[1:-1]\n",
    "                    \n",
    "                    # Decode JSON escapes properly\n",
    "                    try:\n",
    "                        # Handle JSON string encoding with proper Unicode support\n",
    "                        decoded_part = json.loads(f'\"{part}\"')\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Fallback: manual decode common escapes\n",
    "                        decoded_part = part\n",
    "                        decoded_part = decoded_part.replace('\\\\n', '\\n')\n",
    "                        decoded_part = decoded_part.replace('\\\\t', '\\t')\n",
    "                        decoded_part = decoded_part.replace('\\\\\"', '\"')\n",
    "                        decoded_part = decoded_part.replace(\"\\\\'\", \"'\")\n",
    "                        # Handle Unicode escapes manually\n",
    "                        decoded_part = decode_unicode_escapes(decoded_part)\n",
    "                        decoded_lines.append(decoded_part)\n",
    "                \n",
    "                # Join all stdout lines\n",
    "                full_output = ''.join(decoded_lines)\n",
    "                return full_output\n",
    "        \n",
    "        # If not in Logs format, try direct JSON decode\n",
    "        if isinstance(content, str):\n",
    "            try:\n",
    "                decoded = json.loads(f'\"{content}\"')\n",
    "                return decode_unicode_escapes(decoded)\n",
    "            except json.JSONDecodeError:\n",
    "                return decode_unicode_escapes(content)\n",
    "        \n",
    "        return str(content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"[Error decoding E2B output: {e}]\\n\\nRaw output:\\n{str(raw_result)}\"\n",
    "\n",
    "\n",
    "def decode_unicode_escapes(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Decode Unicode escape sequences in text.\n",
    "    \n",
    "    Args:\n",
    "        text: Text that may contain Unicode escapes like \\\\ud83c\\\\udfd7\\\\ufe0f\n",
    "        \n",
    "    Returns:\n",
    "        Text with Unicode escapes properly decoded\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "    \n",
    "    try:\n",
    "        # Handle Unicode escapes using encode/decode\n",
    "        return text.encode().decode('unicode_escape')\n",
    "    except (UnicodeDecodeError, UnicodeEncodeError):\n",
    "        # Fallback: manual decode using codecs\n",
    "        try:\n",
    "            import codecs\n",
    "            return codecs.decode(text, 'unicode_escape')\n",
    "        except:\n",
    "            # Final fallback: return as-is\n",
    "            return text\n",
    "\n",
    "print(\"🔧 E2B output decoder functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ MOUNT DIRECTORY VERIFICATION TEST\n",
      "=============================================\n",
      "🎯 Template: sentient-e2b-s3\n",
      "📁 Mount directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "✅ E2B toolkit initialized\n",
      "✅ Mount verification completed\n",
      "📊 Output:\n",
      "----------------------------------------\n",
      "🏗️ E2B MOUNT VERIFICATION\n",
      "==============================\n",
      "Platform: Linux-6.1.102-x86_64-with-glibc2.41\n",
      "Python: 3.12.11\n",
      "Working dir: /home/user\n",
      "\n",
      "📁 Checking mount directory: /opt/sentient\n",
      "   (S3_MOUNT_DIR env var: /opt/sentient)\n",
      "✅ Mount directory exists\n",
      "✅ Mount directory readable: 2 items\n",
      "📋 Current contents:\n",
      "   📁 shared: 0 items\n",
      "   📁 {shared}: 0 items\n",
      "✅ Mount directory is writable\n",
      "🧹 Test file cleaned up\n",
      "\n",
      "✅ Mount verification completed\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "🎯 Mount test result: ✅ PASSED\n",
      "\n",
      "🎉 SUCCESS: Mount directory is working correctly!\n",
      "📁 Using mount path: /opt/sentient\n"
     ]
    }
   ],
   "source": [
    "async def test_mount_directory():\n",
    "    \"\"\"Test the unified mount directory in E2B sandbox.\"\"\"\n",
    "    print(\"🏗️ MOUNT DIRECTORY VERIFICATION TEST\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        \n",
    "        print(f\"🎯 Template: {template_name}\")\n",
    "        print(f\"📁 Mount directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        print(\"✅ E2B toolkit initialized\")\n",
    "        \n",
    "        # Test mount directory and explore available directories\n",
    "        mount_test_code = f\"\"\"\n",
    "import os\n",
    "import platform\n",
    "\n",
    "print(\"🏗️ E2B MOUNT VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Platform: {{platform.platform()}}\")\n",
    "print(f\"Python: {{platform.python_version()}}\")\n",
    "print(f\"Working dir: {{os.getcwd()}}\")\n",
    "\n",
    "# Get mount directory from environment variable in E2B sandbox\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "print(f\"\\\\n📁 Checking mount directory: {{MOUNT_DIR}}\")\n",
    "print(f\"   (S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}})\")\n",
    "\n",
    "if os.path.exists(MOUNT_DIR):\n",
    "    print(\"✅ Mount directory exists\")\n",
    "    \n",
    "    try:\n",
    "        contents = os.listdir(MOUNT_DIR)\n",
    "        print(f\"✅ Mount directory readable: {{len(contents)}} items\")\n",
    "        \n",
    "        # Show contents\n",
    "        if contents:\n",
    "            print(\"📋 Current contents:\")\n",
    "            for item in contents[:10]:  # Show first 10 items\n",
    "                item_path = os.path.join(MOUNT_DIR, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    try:\n",
    "                        sub_items = len(os.listdir(item_path))\n",
    "                        print(f\"   📁 {{item}}: {{sub_items}} items\")\n",
    "                    except:\n",
    "                        print(f\"   📁 {{item}}: (cannot read)\")\n",
    "                else:\n",
    "                    size = os.path.getsize(item_path)\n",
    "                    print(f\"   📄 {{item}}: {{size}} bytes\")\n",
    "            \n",
    "            if len(contents) > 10:\n",
    "                print(f\"   ... and {{len(contents) - 10}} more items\")\n",
    "        else:\n",
    "            print(\"📁 Mount directory is empty\")\n",
    "            \n",
    "        # Test write access\n",
    "        test_file = os.path.join(MOUNT_DIR, \"mount_test.txt\")\n",
    "        try:\n",
    "            with open(test_file, 'w') as f:\n",
    "                f.write(\"Mount write test successful\")\n",
    "            \n",
    "            if os.path.exists(test_file):\n",
    "                print(\"✅ Mount directory is writable\")\n",
    "                os.remove(test_file)  # Clean up\n",
    "                print(\"🧹 Test file cleaned up\")\n",
    "            else:\n",
    "                print(\"❌ Write test file not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Cannot write to mount directory: {{e}}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cannot read mount directory: {{e}}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Mount directory does not exist\")\n",
    "    print(f\"   Expected path: {{MOUNT_DIR}}\")\n",
    "    print(\"   This indicates the E2B template mount is not working properly\")\n",
    "    \n",
    "    # Explore available directories to help with debugging\n",
    "    print(\"\\\\n🔍 EXPLORING AVAILABLE DIRECTORIES:\")\n",
    "    \n",
    "    # Check common mount points and symlinks from startup.sh\n",
    "    potential_mounts = [\n",
    "        \"/workspace\", \"/home/user\", \"/tmp\", \"/mnt\", \"/media\", \"/opt\",\n",
    "        \"/workspace/data\", \"/workspace/results\", \"/home/user/data\",\n",
    "        \"/home/user/s3-bucket\", \"/workspace/s3-bucket\"\n",
    "    ]\n",
    "    \n",
    "    found_dirs = []\n",
    "    for path in potential_mounts:\n",
    "        if os.path.exists(path):\n",
    "            found_dirs.append(path)\n",
    "            try:\n",
    "                contents = os.listdir(path)\n",
    "                print(f\"✅ {{path}}: {{len(contents)}} items\")\n",
    "                \n",
    "                # Check if it's a symlink (from startup.sh)\n",
    "                if os.path.islink(path):\n",
    "                    target = os.readlink(path)\n",
    "                    print(f\"   🔗 Symlink points to: {{target}}\")\n",
    "                \n",
    "                # Show contents\n",
    "                if contents and len(contents) <= 5:\n",
    "                    for item in contents:\n",
    "                        print(f\"   - {{item}}\")\n",
    "                elif contents:\n",
    "                    print(f\"   - {{contents[0]}}, {{contents[1]}} ... ({{len(contents)}} total)\")\n",
    "            except Exception as e:\n",
    "                print(f\"✅ {{path}}: exists but cannot list ({{e}})\")\n",
    "    \n",
    "    print(f\"\\\\n📋 Found {{len(found_dirs)}} potential directories\")\n",
    "    \n",
    "    # Check startup script environment info\n",
    "    print(\"\\\\n📋 E2B STARTUP ENVIRONMENT:\")\n",
    "    env_files = [\"/home/user/.env-info\", \"/workspace/.env-info\"]\n",
    "    for env_file in env_files:\n",
    "        if os.path.exists(env_file):\n",
    "            print(f\"Environment info from {{env_file}}:\")\n",
    "            try:\n",
    "                with open(env_file, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if not line.startswith('#') and '=' in line:\n",
    "                            print(f\"   {{line.strip()}}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"   Cannot read {{env_file}}: {{e}}\")\n",
    "    \n",
    "    # Check mount method used\n",
    "    mount_method_file = \"/tmp/mount-method\"\n",
    "    if os.path.exists(mount_method_file):\n",
    "        try:\n",
    "            with open(mount_method_file, 'r') as f:\n",
    "                method = f.read().strip()\n",
    "            print(f\"\\\\n🔧 S3 Mount method used: {{method}}\")\n",
    "        except:\n",
    "            print(\"\\\\n🔧 S3 Mount method: unknown\")\n",
    "    \n",
    "    print(\"\\\\n💡 DIAGNOSIS:\")\n",
    "    print(\"1. Check if S3_MOUNT_DIR env var is properly set in E2B template\")\n",
    "    print(\"2. Verify AWS credentials were passed as build arguments\")\n",
    "    print(\"3. Check startup.sh logs for mount failures\")\n",
    "    print(\"4. Alternative: Use working symlinked path if available\")\n",
    "\n",
    "print(\"\\\\n✅ Mount verification completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(mount_test_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"✅ Mount verification completed\")\n",
    "        print(\"📊 Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if mount exists for return value\n",
    "        mount_exists = \"✅ Mount directory exists\" in decoded_output\n",
    "        \n",
    "        # Extract working mount path if available\n",
    "        if not mount_exists:\n",
    "            lines = decoded_output.split('\\n')\n",
    "            for line in lines:\n",
    "                # Look for working symlinks that point to S3\n",
    "                if \"🔗 Symlink points to:\" in line and (\"/data\" in line or \"s3\" in line.lower()):\n",
    "                    potential_mount = line.split(\"points to: \")[-1].strip()\n",
    "                    print(f\"\\n💡 FOUND POTENTIAL WORKING MOUNT: {potential_mount}\")\n",
    "                    globals()['discovered_mount_dir'] = potential_mount\n",
    "                    break\n",
    "        \n",
    "        return mount_exists\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Mount directory test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "mount_success = await test_mount_directory()\n",
    "print(f\"\\n🎯 Mount test result: {'✅ PASSED' if mount_success else '❌ FAILED'}\")\n",
    "\n",
    "if not mount_success:\n",
    "    print(\"\\n🔧 TROUBLESHOOTING GUIDE:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The S3 mount directory is not working properly.\")\n",
    "    print()\n",
    "    print(\"📋 Root cause: E2B template was built without AWS credentials\")\n",
    "    print(\"🔧 Solution: Rebuild E2B template with proper build arguments:\")\n",
    "    print()\n",
    "    print(\"cd docker/e2b-sandbox\")\n",
    "    print(\"e2b template build \\\\\")\n",
    "    print(\"    --build-arg AWS_ACCESS_KEY_ID='your_key' \\\\\")\n",
    "    print(\"    --build-arg AWS_SECRET_ACCESS_KEY='your_secret' \\\\\")\n",
    "    print(\"    --build-arg S3_BUCKET_NAME='your_bucket' \\\\\")\n",
    "    print(\"    --build-arg S3_MOUNT_DIR='/opt/sentient' \\\\\")\n",
    "    print(\"    --name sentient-e2b-s3 --force\")\n",
    "    print()\n",
    "    print(\"Or run: ./setup.sh --e2b-only\")\n",
    "    \n",
    "    if 'discovered_mount_dir' in globals():\n",
    "        print(f\"\\n💡 TEMPORARY WORKAROUND:\")\n",
    "        print(f\"Update .env with: S3_MOUNT_DIR={globals()['discovered_mount_dir']}\")\n",
    "else:\n",
    "    print(\"\\n🎉 SUCCESS: Mount directory is working correctly!\")\n",
    "    print(f\"📁 Using mount path: {mount_dir}\")\n",
    "    globals()['working_mount_dir'] = mount_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Test 2: Project Structure Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 PROJECT STRUCTURE CREATION TEST\n",
      "=============================================\n",
      "🧪 Test Project ID: test_20250822_214954\n",
      "📁 Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "✅ Using confirmed working mount: /opt/sentient\n",
      "✅ Structure creation completed\n",
      "📊 Output:\n",
      "----------------------------------------\n",
      "📁 CREATING PROJECT STRUCTURE\n",
      "===================================\n",
      "Project: test_20250822_214954\n",
      "Mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient\n",
      "✅ Created project directory: /opt/sentient/test_20250822_214954\n",
      "\n",
      "🗂️  Creating 5 directories...\n",
      "✅ binance_toolkit: created directory + test file (261 bytes)\n",
      "✅ coingecko_toolkit: created directory + test file (263 bytes)\n",
      "✅ defillama_toolkit: created directory + test file (263 bytes)\n",
      "✅ arkham_toolkit: created directory + test file (260 bytes)\n",
      "✅ results: created directory + test file (253 bytes)\n",
      "\n",
      "📊 Structure Creation Summary:\n",
      "   Final mount path used: /opt/sentient\n",
      "   Project directory: /opt/sentient/test_20250822_214954\n",
      "   Toolkit directories created: 5/5\n",
      "\n",
      "📋 Final structure verification:\n",
      "   📁 arkham_toolkit: 1 files\n",
      "      📄 structure_test.json (260 bytes)\n",
      "   📁 binance_toolkit: 1 files\n",
      "      📄 structure_test.json (261 bytes)\n",
      "   📁 coingecko_toolkit: 1 files\n",
      "      📄 structure_test.json (263 bytes)\n",
      "   📁 defillama_toolkit: 1 files\n",
      "      📄 structure_test.json (263 bytes)\n",
      "   📁 results: 1 files\n",
      "      📄 structure_test.json (253 bytes)\n",
      "\n",
      "🎉 Project structure created successfully!\n",
      "✅ All expected directories and test files are present\n",
      "🔄 This structure should be visible across all environments\n",
      "🌐 S3 sync: Files should appear in S3 bucket via /opt/sentient mount\n",
      "\n",
      "✅ Structure creation test completed\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "🎯 Structure test result: ✅ PASSED\n"
     ]
    }
   ],
   "source": [
    "async def test_project_structure():\n",
    "    \"\"\"Test creating the expected project structure in mount directory.\"\"\"\n",
    "    print(\"📁 PROJECT STRUCTURE CREATION TEST\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "        \n",
    "        test_project_id = f\"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"🧪 Test Project ID: {test_project_id}\")\n",
    "        print(f\"📁 Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous test\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"🔄 Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"✅ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"🎯 Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        \n",
    "        # Create project structure\n",
    "        structure_test_code = f\"\"\"\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Use environment variable or fallback to discovered mount\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "PROJECT_ID = \"{test_project_id}\"\n",
    "\n",
    "print(\"📁 CREATING PROJECT STRUCTURE\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}}\")\n",
    "\n",
    "# Check if mount directory is available (could be symlinked)\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"❌ Mount directory not available: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"✅ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"❌ No alternative mount directories found\")\n",
    "        exit(1)\n",
    "\n",
    "# Create project directory\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "print(f\"✅ Created project directory: {{project_dir}}\")\n",
    "\n",
    "# Define expected toolkit directories\n",
    "toolkit_dirs = [\n",
    "    \"binance_toolkit\",\n",
    "    \"coingecko_toolkit\",\n",
    "    \"defillama_toolkit\", \n",
    "    \"arkham_toolkit\",\n",
    "    \"results\"\n",
    "]\n",
    "\n",
    "print(f\"\\\\n🗂️  Creating {{len(toolkit_dirs)}} directories...\")\n",
    "\n",
    "created_count = 0\n",
    "for toolkit_name in toolkit_dirs:\n",
    "    toolkit_dir = os.path.join(project_dir, toolkit_name)\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(toolkit_dir, exist_ok=True)\n",
    "        \n",
    "        # Create a test file in each directory\n",
    "        test_file = os.path.join(toolkit_dir, \"structure_test.json\")\n",
    "        test_data = {{\n",
    "            \"toolkit\": toolkit_name,\n",
    "            \"project_id\": PROJECT_ID,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"environment\": \"e2b_sandbox\",\n",
    "            \"mount_path\": MOUNT_DIR,\n",
    "            \"env_s3_mount_dir\": os.getenv('S3_MOUNT_DIR', 'not_set'),\n",
    "            \"test_type\": \"structure_creation\"\n",
    "        }}\n",
    "        \n",
    "        with open(test_file, 'w') as f:\n",
    "            json.dump(test_data, f, indent=2)\n",
    "        \n",
    "        file_size = os.path.getsize(test_file)\n",
    "        print(f\"✅ {{toolkit_name}}: created directory + test file ({{file_size}} bytes)\")\n",
    "        created_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {{toolkit_name}}: failed to create - {{e}}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\\\n📊 Structure Creation Summary:\")\n",
    "print(f\"   Final mount path used: {{MOUNT_DIR}}\")\n",
    "print(f\"   Project directory: {{project_dir}}\")\n",
    "print(f\"   Toolkit directories created: {{created_count}}/{{len(toolkit_dirs)}}\")\n",
    "\n",
    "# Verify structure\n",
    "if os.path.exists(project_dir):\n",
    "    contents = os.listdir(project_dir)\n",
    "    print(f\"\\\\n📋 Final structure verification:\")\n",
    "    for item in sorted(contents):\n",
    "        item_path = os.path.join(project_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = os.listdir(item_path)\n",
    "            print(f\"   📁 {{item}}: {{len(files)}} files\")\n",
    "            for f in files:\n",
    "                f_path = os.path.join(item_path, f)\n",
    "                f_size = os.path.getsize(f_path)\n",
    "                print(f\"      📄 {{f}} ({{f_size}} bytes)\")\n",
    "\n",
    "if created_count == len(toolkit_dirs):\n",
    "    print(\"\\\\n🎉 Project structure created successfully!\")\n",
    "    print(\"✅ All expected directories and test files are present\")\n",
    "    print(\"🔄 This structure should be visible across all environments\")\n",
    "    print(f\"🌐 S3 sync: Files should appear in S3 bucket via {{MOUNT_DIR}} mount\")\n",
    "else:\n",
    "    print(f\"\\\\n⚠️  Partial success: {{created_count}}/{{len(toolkit_dirs)}} directories created\")\n",
    "\n",
    "print(\"\\\\n✅ Structure creation test completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(structure_test_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"✅ Structure creation completed\")\n",
    "        print(\"📊 Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Save test project ID for later tests\n",
    "        globals()['test_project_id'] = test_project_id\n",
    "        \n",
    "        # Check if structure was created successfully\n",
    "        structure_success = \"🎉 Project structure created successfully!\" in decoded_output\n",
    "        \n",
    "        return structure_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Project structure test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "structure_success = await test_project_structure()\n",
    "print(f\"\\n🎯 Structure test result: {'✅ PASSED' if structure_success else '❌ FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💰 Test 3: Data Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 DATA ANALYSIS WORKFLOW TEST\n",
      "========================================\n",
      "🔄 Using existing project: test_20250822_012124\n",
      "📁 Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "✅ Using confirmed working mount: /opt/sentient\n",
      "✅ Analysis workflow completed\n",
      "📊 Output:\n",
      "----------------------------------------\n",
      "💰 DATA ANALYSIS WORKFLOW\n",
      "===================================\n",
      "Project: test_20250822_012124\n",
      "Mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient)\n",
      "✅ Project directory ready: /opt/sentient/test_20250822_012124\n",
      "\n",
      "📊 Step 1: Generating sample crypto data...\n",
      "✅ Generated 120 data points for 5 symbols\n",
      "\n",
      "🔍 Step 2: Performing market analysis...\n",
      "\n",
      "💰 Current Prices:\n",
      "   ADA: $0.49\n",
      "   BTC: $45,148.47\n",
      "   DOT: $24.36\n",
      "   ETH: $2,980.15\n",
      "   SOL: $100.54\n",
      "\n",
      "📈 24h Price Changes:\n",
      "   🔴 ADA: -3.69%\n",
      "   🔴 BTC: -0.66%\n",
      "   🔴 DOT: -6.06%\n",
      "   🔴 ETH: -2.11%\n",
      "   🟢 SOL: +0.98%\n",
      "\n",
      "📊 Price Volatility (24h):\n",
      "   ADA: 1.87%\n",
      "   BTC: 1.71%\n",
      "   DOT: 1.74%\n",
      "   ETH: 1.37%\n",
      "   SOL: 1.66%\n",
      "\n",
      "💾 Step 3: Saving analysis results...\n",
      "✅ Analysis results saved: /opt/sentient/test_20250822_012124/results/market_analysis.json (927 bytes)\n",
      "\"â Individual symbol data saved to toolkit directories\n",
      "\n",
      "ð Step 4: Verifying project structure...\n",
      "\n",
      "ð Project 'test_20250822_012124' structure:\n",
      "   ð ada_toolkit: 2 files1713 bytes total\n",
      "   📁 arkham_toolkit: 1 files260 bytes total\n",
      "   📁 binance_toolkit: 1 files261 bytes total\n",
      "   📁 btc_toolkit: 2 files1821 bytes total\n",
      "   📁 coingecko_toolkit: 1 files263 bytes total\n",
      "   📁 defillama_toolkit: 1 files263 bytes total\n",
      "   📁 dot_toolkit: 2 files1734 bytes total\n",
      "   📁 eth_toolkit: 2 files1793 bytes total\n",
      "   📁 results: 2 files1180 bytes total\n",
      "   📁 sol_toolkit: 2 files1750 bytes total\n",
      "\n",
      "ð Data analysis workflow completed successfully!\n",
      "â All data saved to unified mount directory\n",
      "ð Results should now be visible across all environments\n",
      "ð S3 sync: Files saved to /opt/sentient should sync to S3 bucket\n",
      "\n",
      "â Analysis workflow test completed\n",
      "\"\n",
      "----------------------------------------\n",
      "\n",
      "🎯 Analysis workflow result: ❌ FAILED\n"
     ]
    }
   ],
   "source": [
    "async def test_data_analysis_workflow():\n",
    "    \"\"\"Test a complete data analysis workflow using the mount directory.\"\"\"\n",
    "    print(\"💰 DATA ANALYSIS WORKFLOW TEST\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "        mount_dir = os.path.expandvars(mount_dir)  # Resolve $HOME etc.\n",
    "        \n",
    "        # Use existing test project or create new one\n",
    "        if 'test_project_id' in globals():\n",
    "            project_id = globals()['test_project_id']\n",
    "            print(f\"🔄 Using existing project: {project_id}\")\n",
    "        else:\n",
    "            project_id = f\"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "            print(f\"🆕 Creating new project: {project_id}\")\n",
    "        \n",
    "        print(f\"📁 Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous test\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"🔄 Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"✅ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"🎯 Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Initialize E2B\n",
    "        e2b_toolkit = E2BTools(\n",
    "            timeout=300,\n",
    "            sandbox_options={\"template\": template_name}\n",
    "        )\n",
    "        \n",
    "        # Run data analysis workflow\n",
    "        analysis_code = f\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Use environment variable or fallback to discovered mount\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/opt/sentient\")\n",
    "PROJECT_ID = \"{project_id}\"\n",
    "\n",
    "print(\"💰 DATA ANALYSIS WORKFLOW\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}})\")\n",
    "\n",
    "# Check if mount directory is available (could be symlinked)\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"❌ Mount directory not available: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"✅ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"❌ No alternative mount directories found - using /tmp\")\n",
    "        MOUNT_DIR = \"/tmp\"\n",
    "\n",
    "# Ensure project structure exists\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = os.path.join(project_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Project directory ready: {{project_dir}}\")\n",
    "\n",
    "# Step 1: Generate sample crypto data (simulating toolkit data)\n",
    "print(\"\\\\n📊 Step 1: Generating sample crypto data...\")\n",
    "\n",
    "symbols = ['BTC', 'ETH', 'SOL', 'ADA', 'DOT']\n",
    "base_prices = {{\n",
    "    'BTC': 45000,\n",
    "    'ETH': 3000, \n",
    "    'SOL': 100,\n",
    "    'ADA': 0.50,\n",
    "    'DOT': 25\n",
    "}}\n",
    "\n",
    "# Generate 24 hours of hourly data\n",
    "hours = 24\n",
    "timestamps = [datetime.now() - timedelta(hours=i) for i in range(hours, 0, -1)]\n",
    "\n",
    "np.random.seed(42)  # Reproducible results\n",
    "crypto_data = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    base_price = base_prices[symbol]\n",
    "    \n",
    "    for i, timestamp in enumerate(timestamps):\n",
    "        # Simulate realistic price movement\n",
    "        volatility = np.random.normal(0, 0.02)  # 2% volatility\n",
    "        trend = 0.001 * np.sin(i / 6)  # 6-hour cycle\n",
    "        price = base_price * (1 + trend + volatility)\n",
    "        \n",
    "        crypto_data.append({{\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'symbol': symbol,\n",
    "            'price': round(price, 6),\n",
    "            'volume': np.random.uniform(1000000, 5000000)\n",
    "        }})\n",
    "\n",
    "df = pd.DataFrame(crypto_data)\n",
    "print(f\"✅ Generated {{len(df)}} data points for {{len(symbols)}} symbols\")\n",
    "\n",
    "# Step 2: Perform analysis\n",
    "print(\"\\\\n🔍 Step 2: Performing market analysis...\")\n",
    "\n",
    "# Current prices\n",
    "current_prices = df.groupby('symbol')['price'].last()\n",
    "print(\"\\\\n💰 Current Prices:\")\n",
    "for symbol, price in current_prices.items():\n",
    "    print(f\"   {{symbol}}: ${{price:,.2f}}\")\n",
    "\n",
    "# Calculate 24h changes\n",
    "first_prices = df.groupby('symbol')['price'].first()\n",
    "changes_24h = ((current_prices - first_prices) / first_prices * 100)\n",
    "\n",
    "print(\"\\\\n📈 24h Price Changes:\")\n",
    "for symbol, change in changes_24h.items():\n",
    "    emoji = \"🟢\" if change > 0 else \"🔴\" if change < 0 else \"⚪\"\n",
    "    print(f\"   {{emoji}} {{symbol}}: {{change:+.2f}}%\")\n",
    "\n",
    "# Calculate volatility\n",
    "volatility = df.groupby('symbol')['price'].std() / df.groupby('symbol')['price'].mean() * 100\n",
    "print(\"\\\\n📊 Price Volatility (24h):\")\n",
    "for symbol, vol in volatility.items():\n",
    "    print(f\"   {{symbol}}: {{vol:.2f}}%\")\n",
    "\n",
    "# Step 3: Save results to mount directory\n",
    "print(\"\\\\n💾 Step 3: Saving analysis results...\")\n",
    "\n",
    "# Comprehensive analysis results\n",
    "analysis_results = {{\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'project_id': PROJECT_ID,\n",
    "    'analysis_type': 'crypto_market_analysis',\n",
    "    'data_period': '24h',\n",
    "    'symbols_analyzed': len(symbols),\n",
    "    'data_points': len(df),\n",
    "    'mount_path_used': MOUNT_DIR,\n",
    "    'env_s3_mount_dir': os.getenv('S3_MOUNT_DIR', 'not_set'),\n",
    "    'current_prices': dict(current_prices),\n",
    "    'price_changes_24h': dict(changes_24h),\n",
    "    'volatility_24h': dict(volatility),\n",
    "    'summary': {{\n",
    "        'best_performer': changes_24h.idxmax(),\n",
    "        'worst_performer': changes_24h.idxmin(),\n",
    "        'highest_volatility': volatility.idxmax(),\n",
    "        'lowest_volatility': volatility.idxmin()\n",
    "    }}\n",
    "}}\n",
    "\n",
    "# Save analysis results\n",
    "results_file = os.path.join(results_dir, 'market_analysis.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "results_size = os.path.getsize(results_file)\n",
    "print(f\"✅ Analysis results saved: {{results_file}} ({{results_size}} bytes)\")\n",
    "\n",
    "# Save raw data for each toolkit\n",
    "for symbol in symbols:\n",
    "    symbol_data = df[df['symbol'] == symbol].copy()\n",
    "    toolkit_name = f\"{{symbol.lower()}}_toolkit\"  # Not exactly the real toolkit names, but for demo\n",
    "    toolkit_dir = os.path.join(project_dir, toolkit_name)\n",
    "    os.makedirs(toolkit_dir, exist_ok=True)\n",
    "    \n",
    "    # Save symbol-specific data\n",
    "    symbol_file = os.path.join(toolkit_dir, f'{{symbol.lower()}}_24h_data.csv')\n",
    "    symbol_data.to_csv(symbol_file, index=False)\n",
    "    \n",
    "    # Save symbol summary\n",
    "    summary_file = os.path.join(toolkit_dir, f'{{symbol.lower()}}_summary.json')\n",
    "    symbol_summary = {{\n",
    "        'symbol': symbol,\n",
    "        'current_price': float(current_prices[symbol]),\n",
    "        'change_24h': float(changes_24h[symbol]),\n",
    "        'volatility_24h': float(volatility[symbol]),\n",
    "        'data_points': len(symbol_data),\n",
    "        'mount_path_used': MOUNT_DIR,\n",
    "        'price_range': {{\n",
    "            'min': float(symbol_data['price'].min()),\n",
    "            'max': float(symbol_data['price'].max()),\n",
    "            'avg': float(symbol_data['price'].mean())\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(symbol_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Individual symbol data saved to toolkit directories\")\n",
    "\n",
    "# Step 4: Verify final structure\n",
    "print(\"\\\\n📋 Step 4: Verifying project structure...\")\n",
    "\n",
    "if os.path.exists(project_dir):\n",
    "    contents = os.listdir(project_dir)\n",
    "    print(f\"\\\\n📁 Project '{{PROJECT_ID}}' structure:\")\n",
    "    for item in sorted(contents):\n",
    "        item_path = os.path.join(project_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            files = os.listdir(item_path)\n",
    "            total_size = sum(os.path.getsize(os.path.join(item_path, f)) \n",
    "                           for f in files if os.path.isfile(os.path.join(item_path, f)))\n",
    "            print(f\"   📁 {{item}}: {{len(files)}} files, {{total_size}} bytes total\")\n",
    "\n",
    "print(\"\\\\n🎉 Data analysis workflow completed successfully!\")\n",
    "print(\"✅ All data saved to unified mount directory\")\n",
    "print(\"🔄 Results should now be visible across all environments\")\n",
    "print(f\"🌐 S3 sync: Files saved to {{MOUNT_DIR}} should sync to S3 bucket\")\n",
    "\n",
    "print(\"\\\\n✅ Analysis workflow test completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        result = e2b_toolkit.run_python_code(analysis_code)\n",
    "        decoded_output = decode_e2b_output(result)\n",
    "        print(\"✅ Analysis workflow completed\")\n",
    "        print(\"📊 Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_output)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Check if analysis was successful\n",
    "        analysis_success = \"🎉 Data analysis workflow completed successfully!\" in decoded_output\n",
    "        \n",
    "        return analysis_success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data analysis workflow test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "analysis_success = await test_data_analysis_workflow()\n",
    "print(f\"\\n🎯 Analysis workflow result: {'✅ PASSED' if analysis_success else '❌ FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Test 4: Real Crypto Agent + E2B Analysis Workflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:19.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_cache_system\u001b[0m:\u001b[36m270\u001b[0m - \u001b[34m\u001b[1mInitialized generic cache system with TTL: 3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[34m\u001b[1mInitialized DataHTTPClient with 30.0s timeout\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.451\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_standard_configuration\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1mInitialized standard configuration: timeout=30.0s, retries=3, cache_ttl=3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:19.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing S3 mounted directory for binance: /opt/sentient\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 CRYPTO AGENT + E2B ANALYSIS WORKFLOW\n",
      "==================================================\n",
      "🧪 Project ID: agent_test_20250822_012919\n",
      "📁 Mount Directory (from S3_MOUNT_DIR): /opt/sentient\n",
      "✅ Using confirmed working mount: /opt/sentient\n",
      "\n",
      "📊 Step 1: Initializing real crypto data toolkits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:20.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_detect_e2b_context\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mS3 integration detected with bucket: roma-shared\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mData helpers initialized - Project: agent_test_20250822_012919, Toolkit: binance, Dir: /opt/sentient/agent_test_20250822_012919/binance, S3: True\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m300\u001b[0m - \u001b[34m\u001b[1mInitialized Multi-Market BinanceToolkit with default market 'spot' and 2 symbols\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m319\u001b[0m - \u001b[34m\u001b[1mUsing CoinGecko Pro API with API key\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_cache_system\u001b[0m:\u001b[36m270\u001b[0m - \u001b[34m\u001b[1mInitialized generic cache system with TTL: 3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[34m\u001b[1mInitialized DataHTTPClient with 30.0s timeout\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.440\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_init_standard_configuration\u001b[0m:\u001b[36m559\u001b[0m - \u001b[34m\u001b[1mInitialized standard configuration: timeout=30.0s, retries=3, cache_ttl=3600s\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1mUsing S3 mounted directory for coingecko: /opt/sentient\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.780\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_detect_e2b_context\u001b[0m:\u001b[36m131\u001b[0m - \u001b[1mS3 integration detected with bucket: roma-shared\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_data\u001b[0m:\u001b[36m_init_data_helpers\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mData helpers initialized - Project: agent_test_20250822_012919, Toolkit: coingecko, Dir: /opt/sentient/agent_test_20250822_012919/coingecko, S3: True\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m370\u001b[0m - \u001b[34m\u001b[1mInitialized CoinGeckoToolkit with default currency 'usd' (type: <enum 'VsCurrency'>) and all coins\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'spot' with base URL: https://api.binance.us\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'usdm' with base URL: https://fapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36madd_endpoint\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mAdded endpoint 'coinm' with base URL: https://dapi.binance.com\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.782\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36msetup_endpoints\u001b[0m:\u001b[36m419\u001b[0m - \u001b[34m\u001b[1mSetup 3 authenticated endpoints\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_get_client\u001b[0m:\u001b[36m175\u001b[0m - \u001b[34m\u001b[1mCreated HTTP client for endpoint 'spot'\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:20.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/exchangeInfo (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Real crypto toolkits initialized\n",
      "   BinanceToolkit: BinanceToolkit\n",
      "   CoinGeckoToolkit: CoinGeckoToolkit\n",
      "   Configured S3_MOUNT_DIR: /opt/sentient\n",
      "\n",
      "💰 Step 2: Fetching real crypto data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-22 01:29:21.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.base.base_api\u001b[0m:\u001b[36m_cache_data\u001b[0m:\u001b[36m311\u001b[0m - \u001b[34m\u001b[1mCached set data (246 items) for key 'symbols_spot'\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit\u001b[0m:\u001b[36mreload_symbols\u001b[0m:\u001b[36m509\u001b[0m - \u001b[1mLoaded 246 symbols for Binance Spot Trading\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.324\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/ticker/price (attempt 1)\u001b[0m\n",
      "\u001b[32m2025-08-22 01:29:21.570\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msentientresearchagent.hierarchical_agent_framework.toolkits.utils.http_client\u001b[0m:\u001b[36m_make_request\u001b[0m:\u001b[36m317\u001b[0m - \u001b[34m\u001b[1mMaking GET request to spot/api/v3/klines (attempt 1)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Binance data fetched:\n",
      "   BTC Price: $N/A\n",
      "   ETH Price: $N/A\n",
      "   BTC 24h data points: 24\n",
      "⚠️  CoinGecko data fetch error: 'CoinGeckoToolkit' object has no attribute 'get_coin_data'\n",
      "\n",
      "🚀 Step 3: Initializing E2B for advanced analysis...\n",
      "✅ Crypto analyzer agent with E2B initialized\n",
      "\n",
      "🔍 Step 4: Running comprehensive crypto analysis...\n",
      "🤖 Agent query (preview): \n",
      "Perform a comprehensive cryptocurrency market analysis using the E2B sandbox environment.\n",
      "\n",
      "PROJECT SETUP:\n",
      "- Project ID: agent_test_20250822_012919\n",
      "- Mount directory: Use S3_MOUNT_DIR environment vari...\n",
      "\n",
      "⏳ Running analysis (this may take 2-3 minutes)...\n",
      "⏰ Analysis timed out after 5 minutes\n",
      "\n",
      "🔍 Step 5: Verifying results in mount directory...\n",
      "✅ Verification completed\n",
      "📊 Verification Output:\n",
      "----------------------------------------\n",
      "🔍 RESULTS VERIFICATION\n",
      "==============================\n",
      "Project: agent_test_20250822_012919\n",
      "Expected mount: /opt/sentient\n",
      "S3_MOUNT_DIR env var: /opt/sentient\n",
      "\"â Project directory exists: /opt/sentient/agent_test_20250822_012919\n",
      "â Results directory exists: /opt/sentient/agent_test_20250822_012919/results\n",
      "\n",
      "ð Results files (3):\n",
      "   ð btc_advanced_analysis.png: 726,382 bytes\n",
      "   ð btc_comprehensive_dashboard.png: 1,146,585 bytes\n",
      "   ð crypto_price_comparison.png: 98,849 bytes\n",
      "\n",
      "ð Total results size: 1,971,816 bytes\n",
      "â Missing expected file: crypto_analysis_report.json\n",
      "â Missing expected file: processed_data.csv\n",
      "â Missing expected file: analysis_summary.md\n",
      "â Found 3 visualization(s): ['btc_advanced_analysis.png'btc_comprehensive_dashboard.png'crypto_price_comparison.png']\n",
      "\n",
      "ð Success rate: 0/3 expected files found\n",
      "ð Final mount directory used: /opt/sentient\n",
      "\n",
      "â Verification completed\n",
      "\"\n",
      "----------------------------------------\n",
      "\n",
      "🎯 Crypto Agent + E2B Workflow Result: ❌ FAILED\n",
      "\n",
      "🎯 Final result: ❌ FAILED\n"
     ]
    }
   ],
   "source": [
    "async def test_crypto_agent_e2b_workflow():\n",
    "    \"\"\"Test real crypto agent data fetching followed by E2B analysis.\"\"\"\n",
    "    print(\"🤖 CRYPTO AGENT + E2B ANALYSIS WORKFLOW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Configuration - Use environment variable for mount directory\n",
    "        template_name = os.getenv(\"E2B_TEMPLATE_ID\", \"sentient-e2b-s3\")\n",
    "        mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "        \n",
    "        project_id = f\"agent_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        print(f\"🧪 Project ID: {project_id}\")\n",
    "        print(f\"📁 Mount Directory (from S3_MOUNT_DIR): {mount_dir}\")\n",
    "        \n",
    "        # Use discovered working mount if available from previous tests\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            working_mount = globals()['discovered_mount_dir']\n",
    "            print(f\"🔄 Using discovered working mount: {working_mount}\")\n",
    "        elif 'working_mount_dir' in globals():\n",
    "            working_mount = globals()['working_mount_dir']\n",
    "            print(f\"✅ Using confirmed working mount: {working_mount}\")\n",
    "        else:\n",
    "            working_mount = mount_dir\n",
    "            print(f\"🎯 Using configured mount: {working_mount}\")\n",
    "        \n",
    "        # Step 1: Initialize crypto data toolkits (real data fetching)\n",
    "        print(\"\\n📊 Step 1: Initializing real crypto data toolkits...\")\n",
    "        \n",
    "        try:\n",
    "            from sentientresearchagent.hierarchical_agent_framework.toolkits.data.binance_toolkit import BinanceToolkit\n",
    "            from sentientresearchagent.hierarchical_agent_framework.toolkits.data.coingecko_toolkit import CoinGeckoToolkit\n",
    "            \n",
    "            # Initialize with mount directory for data persistence\n",
    "            os.environ[\"CURRENT_PROJECT_ID\"] = project_id\n",
    "            os.environ[\"S3_MOUNT_ENABLED\"] = \"true\"\n",
    "            os.environ[\"S3_MOUNT_DIR\"] = working_mount  # Use working mount\n",
    "            \n",
    "            # Initialize toolkits - they should use BaseDataToolkit and save to mount\n",
    "            binance_toolkit = BinanceToolkit(\n",
    "                symbols=['BTCUSDT', 'ETHUSD'],\n",
    "                default_market_type=\"spot\"\n",
    "            )\n",
    "            \n",
    "            coingecko_toolkit = CoinGeckoToolkit()\n",
    "            \n",
    "            print(\"✅ Real crypto toolkits initialized\")\n",
    "            print(f\"   BinanceToolkit: {binance_toolkit.__class__.__name__}\")\n",
    "            print(f\"   CoinGeckoToolkit: {coingecko_toolkit.__class__.__name__}\")\n",
    "            print(f\"   Configured S3_MOUNT_DIR: {working_mount}\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            print(f\"⚠️  Could not import toolkits: {e}\")\n",
    "            print(\"   Using simulated data instead\")\n",
    "            binance_toolkit = None\n",
    "            coingecko_toolkit = None\n",
    "        \n",
    "        # Step 2: Fetch real crypto data using toolkits\n",
    "        print(\"\\n💰 Step 2: Fetching real crypto data...\")\n",
    "        \n",
    "        fetched_data = {}\n",
    "        \n",
    "        if binance_toolkit:\n",
    "            try:\n",
    "                # Fetch real current prices\n",
    "                btc_price = await binance_toolkit.get_current_price('BTCUSDT')\n",
    "                eth_price = await binance_toolkit.get_current_price('ETHUSD')\n",
    "                \n",
    "                # Fetch some historical data\n",
    "                btc_klines = await binance_toolkit.get_klines('BTCUSDT', interval='1h', limit=24)\n",
    "                \n",
    "                fetched_data['binance'] = {\n",
    "                    'btc_price': btc_price,\n",
    "                    'eth_price': eth_price,\n",
    "                    'btc_24h_data': btc_klines\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ Binance data fetched:\")\n",
    "                print(f\"   BTC Price: ${btc_price.get('price', 'N/A')}\")\n",
    "                print(f\"   ETH Price: ${eth_price.get('price', 'N/A')}\")\n",
    "                print(f\"   BTC 24h data points: {len(btc_klines.get('data', [])) if isinstance(btc_klines, dict) else 'N/A'}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Binance data fetch error: {e}\")\n",
    "                fetched_data['binance_error'] = str(e)\n",
    "        \n",
    "        if coingecko_toolkit:\n",
    "            try:\n",
    "                # Get market data from CoinGecko\n",
    "                btc_market = await coingecko_toolkit.get_coin_data('bitcoin')\n",
    "                market_overview = await coingecko_toolkit.get_global_market_data()\n",
    "                \n",
    "                fetched_data['coingecko'] = {\n",
    "                    'btc_market': btc_market,\n",
    "                    'global_market': market_overview\n",
    "                }\n",
    "                \n",
    "                print(f\"✅ CoinGecko data fetched:\")\n",
    "                print(f\"   Bitcoin market data: Available\")\n",
    "                print(f\"   Global market data: Available\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  CoinGecko data fetch error: {e}\")\n",
    "                fetched_data['coingecko_error'] = str(e)\n",
    "        \n",
    "        # Step 3: Initialize E2B for advanced analysis\n",
    "        print(\"\\n🚀 Step 3: Initializing E2B for advanced analysis...\")\n",
    "        \n",
    "        from agno.tools.e2b import E2BTools\n",
    "        from agno.tools.reasoning import ReasoningTools\n",
    "        from agno.agent import Agent as AgnoAgent\n",
    "        from agno.models.litellm import LiteLLM\n",
    "        \n",
    "        # Create E2B-enabled crypto analyzer agent\n",
    "        crypto_analyzer_agent = AgnoAgent(\n",
    "            model=LiteLLM(id=\"openrouter/anthropic/claude-sonnet-4\"),\n",
    "            tools=[\n",
    "                E2BTools(sandbox_options={\"template\": template_name}),\n",
    "                ReasoningTools()\n",
    "            ],\n",
    "            name=\"CryptoAnalyzerWithE2B\",\n",
    "            system_message=f\"\"\"\n",
    "You are an expert cryptocurrency analyst with access to a secure E2B sandbox environment for advanced data analysis.\n",
    "\n",
    "Key capabilities:\n",
    "1. E2B sandbox for secure Python code execution\n",
    "2. Access to unified mount directory for data persistence\n",
    "3. Advanced data analysis libraries: pandas, numpy, matplotlib, seaborn\n",
    "4. Cross-environment data sharing\n",
    "\n",
    "IMPORTANT MOUNT DIRECTORY CONFIGURATION:\n",
    "- The mount directory is configured via S3_MOUNT_DIR environment variable\n",
    "- In E2B sandbox, check: os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "- Use this path consistently for all file operations\n",
    "- If mount directory doesn't exist, check alternative paths like /workspace/data\n",
    "\n",
    "When analyzing data:\n",
    "1. Get mount directory: MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "2. Save all analysis results to MOUNT_DIR/{{project_id}}/results/\n",
    "3. Use the E2B sandbox for all computations and visualizations\n",
    "4. Create comprehensive reports with actionable insights\n",
    "5. Ensure data persists across environments\n",
    "\n",
    "Always provide thorough analysis with clear explanations and save results for future access.\n",
    "\"\"\",\n",
    "            show_tool_calls=True,\n",
    "            markdown=True\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Crypto analyzer agent with E2B initialized\")\n",
    "        \n",
    "        # Step 4: Run comprehensive analysis using both real data and E2B\n",
    "        print(\"\\n🔍 Step 4: Running comprehensive crypto analysis...\")\n",
    "        \n",
    "        analysis_query = f\"\"\"\n",
    "Perform a comprehensive cryptocurrency market analysis using the E2B sandbox environment.\n",
    "\n",
    "PROJECT SETUP:\n",
    "- Project ID: {project_id}\n",
    "- Mount directory: Use S3_MOUNT_DIR environment variable (fallback: /sentient)\n",
    "- Check available paths: /workspace/data, /home/user/data if primary mount fails\n",
    "\n",
    "ANALYSIS TASKS:\n",
    "1. Get mount directory from environment: MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "2. If MOUNT_DIR doesn't exist, try alternatives: /workspace/data, /home/user/data\n",
    "3. Create project directory structure in MOUNT_DIR/{project_id}/\n",
    "4. Process and analyze this real market data: {json.dumps(fetched_data, indent=2, default=str)}\n",
    "5. Perform technical analysis on the price data\n",
    "6. Calculate key metrics: volatility, returns, moving averages\n",
    "7. Generate market insights and trading signals\n",
    "8. Create visualizations (save as PNG files)\n",
    "9. Save comprehensive analysis report as JSON\n",
    "10. Verify all files are saved to the mount directory\n",
    "\n",
    "Deliverables:\n",
    "- Save analysis report to: MOUNT_DIR/{project_id}/results/crypto_analysis_report.json\n",
    "- Save processed data to: MOUNT_DIR/{project_id}/results/processed_data.csv  \n",
    "- Save visualizations to: MOUNT_DIR/{project_id}/results/*.png\n",
    "- Create summary file: MOUNT_DIR/{project_id}/results/analysis_summary.md\n",
    "\n",
    "IMPORTANT: Always check if the mount directory exists first, and include debugging info about paths used.\n",
    "\n",
    "Use the E2B sandbox for all computations and file operations. Ensure data persists in the unified mount directory.\n",
    "\"\"\"\n",
    "        \n",
    "        print(f\"🤖 Agent query (preview): {analysis_query[:200]}...\")\n",
    "        print(\"\\n⏳ Running analysis (this may take 2-3 minutes)...\")\n",
    "        \n",
    "        try:\n",
    "            # Run the analysis with timeout\n",
    "            analysis_response = await asyncio.wait_for(\n",
    "                crypto_analyzer_agent.arun(analysis_query),\n",
    "                timeout=300  # 5 minute timeout\n",
    "            )\n",
    "            \n",
    "            print(\"✅ Analysis completed successfully!\")\n",
    "            print(\"\\n📄 Agent Response:\")\n",
    "            print(\"-\" * 60)\n",
    "            # Truncate very long responses for readability\n",
    "            if len(str(analysis_response)) > 2000:\n",
    "                print(f\"{str(analysis_response)[:2000]}...\\n[Response truncated - full response was {len(str(analysis_response))} characters]\")\n",
    "            else:\n",
    "                print(analysis_response)\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except asyncio.TimeoutError:\n",
    "            print(\"⏰ Analysis timed out after 5 minutes\")\n",
    "            analysis_response = \"TIMEOUT\"\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Analysis failed: {e}\")\n",
    "            analysis_response = f\"ERROR: {e}\"\n",
    "        \n",
    "        # Step 5: Verify results in E2B mount directory\n",
    "        print(\"\\n🔍 Step 5: Verifying results in mount directory...\")\n",
    "        \n",
    "        e2b_toolkit = E2BTools(sandbox_options={\"template\": template_name})\n",
    "        \n",
    "        verification_code = f\"\"\"\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Get mount directory from environment variable\n",
    "MOUNT_DIR = os.getenv(\"S3_MOUNT_DIR\", \"/sentient\")\n",
    "PROJECT_ID = \"{project_id}\"\n",
    "\n",
    "print(\"🔍 RESULTS VERIFICATION\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Project: {{PROJECT_ID}}\")\n",
    "print(f\"Expected mount: {{MOUNT_DIR}}\")\n",
    "print(f\"S3_MOUNT_DIR env var: {{os.getenv('S3_MOUNT_DIR', 'not set')}}\")\n",
    "\n",
    "# Check if mount directory exists, try alternatives if not\n",
    "if not os.path.exists(MOUNT_DIR):\n",
    "    print(f\"❌ Primary mount directory not found: {{MOUNT_DIR}}\")\n",
    "    \n",
    "    # Try alternative paths from startup.sh symlinks\n",
    "    alternatives = [\"/workspace/data\", \"/home/user/data\"]\n",
    "    for alt in alternatives:\n",
    "        if os.path.exists(alt):\n",
    "            print(f\"✅ Using alternative mount: {{alt}}\")\n",
    "            MOUNT_DIR = alt\n",
    "            break\n",
    "    else:\n",
    "        print(\"❌ No alternative mount directories found\")\n",
    "\n",
    "# Check project directory\n",
    "project_dir = os.path.join(MOUNT_DIR, PROJECT_ID)\n",
    "if os.path.exists(project_dir):\n",
    "    print(f\"✅ Project directory exists: {{project_dir}}\")\n",
    "    \n",
    "    # Check results directory\n",
    "    results_dir = os.path.join(project_dir, \"results\")\n",
    "    if os.path.exists(results_dir):\n",
    "        print(f\"✅ Results directory exists: {{results_dir}}\")\n",
    "        \n",
    "        # List all files in results\n",
    "        results_files = os.listdir(results_dir)\n",
    "        print(f\"\\\\n📋 Results files ({{len(results_files)}}):\")\n",
    "        \n",
    "        total_size = 0\n",
    "        for f in sorted(results_files):\n",
    "            f_path = os.path.join(results_dir, f)\n",
    "            if os.path.isfile(f_path):\n",
    "                size = os.path.getsize(f_path)\n",
    "                total_size += size\n",
    "                print(f\"   📄 {{f}}: {{size:,}} bytes\")\n",
    "                \n",
    "                # Show content preview for key files\n",
    "                if f.endswith('.json'):\n",
    "                    try:\n",
    "                        with open(f_path, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "                        print(f\"      🔍 JSON keys: {{list(data.keys())[:5]}}\")\n",
    "                    except:\n",
    "                        pass\n",
    "            elif os.path.isdir(f_path):\n",
    "                sub_files = len(os.listdir(f_path))\n",
    "                print(f\"   📁 {{f}}: {{sub_files}} items\")\n",
    "        \n",
    "        print(f\"\\\\n📊 Total results size: {{total_size:,}} bytes\")\n",
    "        \n",
    "        # Check for expected files\n",
    "        expected_files = [\n",
    "            'crypto_analysis_report.json',\n",
    "            'processed_data.csv',\n",
    "            'analysis_summary.md'\n",
    "        ]\n",
    "        \n",
    "        found_files = 0\n",
    "        for expected in expected_files:\n",
    "            if expected in results_files:\n",
    "                print(f\"✅ Found expected file: {{expected}}\")\n",
    "                found_files += 1\n",
    "            else:\n",
    "                print(f\"❌ Missing expected file: {{expected}}\")\n",
    "        \n",
    "        # Check for visualizations\n",
    "        png_files = [f for f in results_files if f.endswith('.png')]\n",
    "        if png_files:\n",
    "            print(f\"✅ Found {{len(png_files)}} visualization(s): {{png_files}}\")\n",
    "        else:\n",
    "            print(\"❌ No PNG visualizations found\")\n",
    "        \n",
    "        print(f\"\\\\n📈 Success rate: {{found_files}}/{{len(expected_files)}} expected files found\")\n",
    "        print(f\"📁 Final mount directory used: {{MOUNT_DIR}}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Results directory not found: {{results_dir}}\")\n",
    "else:\n",
    "    print(f\"❌ Project directory not found: {{project_dir}}\")\n",
    "    print(f\"   Looking for: {{MOUNT_DIR}}/{{PROJECT_ID}}\")\n",
    "\n",
    "print(\"\\\\n✅ Verification completed\")\n",
    "\"\"\"\n",
    "        \n",
    "        verification_result = e2b_toolkit.run_python_code(verification_code)\n",
    "        decoded_verification = decode_e2b_output(verification_result)\n",
    "        print(\"✅ Verification completed\")\n",
    "        print(\"📊 Verification Output:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(decoded_verification)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Success determination\n",
    "        success = (\n",
    "            analysis_response != \"TIMEOUT\" and\n",
    "            \"ERROR\" not in str(analysis_response) and\n",
    "            \"✅\" in str(decoded_verification)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎯 Crypto Agent + E2B Workflow Result: {'✅ PASSED' if success else '❌ FAILED'}\")\n",
    "        \n",
    "        if success:\n",
    "            print(\"🎉 Complete workflow successful!\")\n",
    "            print(f\"   📁 Real data fetched from crypto APIs\")\n",
    "            print(f\"   🤖 Agent analysis completed in E2B sandbox\")\n",
    "            print(f\"   💾 Results saved to unified mount directory\")\n",
    "            print(f\"   🔄 Data accessible across all environments\")\n",
    "            print(f\"   🌐 Using environment-variable mount: {working_mount}\")\n",
    "        \n",
    "        return success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Crypto agent + E2B workflow failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "crypto_agent_success = await test_crypto_agent_e2b_workflow()\n",
    "print(f\"\\n🎯 Final result: {'✅ PASSED' if crypto_agent_success else '❌ FAILED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_summary():\n",
    "    \"\"\"Print a comprehensive summary of all tests.\"\"\"\n",
    "    print(\"📊 E2B UNIFIED MOUNT TEST SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get mount directory from environment variable\n",
    "    configured_mount_dir = os.getenv(\"S3_MOUNT_DIR\", \"/data/sentient\")\n",
    "    configured_mount_dir = os.path.expandvars(configured_mount_dir)\n",
    "    \n",
    "    print(f\"🎯 Configured Mount Directory (S3_MOUNT_DIR): {configured_mount_dir}\")\n",
    "    \n",
    "    # Show discovered working mount if different\n",
    "    if 'discovered_mount_dir' in globals():\n",
    "        print(f\"🔄 Discovered Working Mount: {globals()['discovered_mount_dir']}\")\n",
    "    if 'working_mount_dir' in globals():\n",
    "        print(f\"✅ Confirmed Working Mount: {globals()['working_mount_dir']}\")\n",
    "    \n",
    "    # Collect test results\n",
    "    tests = [\n",
    "        (\"Mount Directory Verification\", globals().get('mount_success', False)),\n",
    "        (\"Project Structure Creation\", globals().get('structure_success', False)),\n",
    "        (\"Data Analysis Workflow\", globals().get('analysis_success', False)),\n",
    "        (\"Crypto Agent + E2B Analysis\", globals().get('crypto_agent_success', False)),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"\\n🧪 Test Results:\")\n",
    "    for test_name, result in tests:\n",
    "        if result is None:\n",
    "            status = \"⏭️  SKIPPED\"\n",
    "        elif result:\n",
    "            status = \"✅ PASSED\"\n",
    "            passed += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            status = \"❌ FAILED\"\n",
    "            total += 1\n",
    "        \n",
    "        print(f\"  {status} {test_name}\")\n",
    "    \n",
    "    print(f\"\\n📈 Overall Results: {passed}/{total} tests passed\")\n",
    "    \n",
    "    # Results analysis\n",
    "    if passed == total and total > 0:\n",
    "        print(\"\\n🎉 SUCCESS: All tests passed!\")\n",
    "        print(\"✅ Unified mount directory system is working correctly\")\n",
    "        print(\"✅ Environment variable configuration (S3_MOUNT_DIR) is functional\")\n",
    "        print(\"✅ Cross-environment data persistence is operational\")\n",
    "        print(\"✅ Project structure creation and data workflows are working\")\n",
    "        print(\"✅ Real crypto agent + E2B analysis integration is functional\")\n",
    "    elif passed > 0:\n",
    "        print(f\"\\n⚠️  PARTIAL SUCCESS: {passed}/{total} tests passed\")\n",
    "        print(\"   Check the failed tests above for specific issues\")\n",
    "    else:\n",
    "        print(f\"\\n❌ FAILURE: No tests passed\")\n",
    "        print(\"   Check E2B template configuration and mount setup\")\n",
    "    \n",
    "    # Key information\n",
    "    print(\"\\n💡 Key System Information:\")\n",
    "    print(f\"  📁 Mount Directory: {configured_mount_dir} (from S3_MOUNT_DIR env var)\")\n",
    "    print(f\"  🏗️  E2B Template: {os.getenv('E2B_TEMPLATE_ID', 'sentient-e2b-s3')}\")\n",
    "    print(f\"  🗂️  Project Structure: MOUNT_DIR/{{project_id}}/{{toolkit_name|results}}/\")\n",
    "    print(f\"  🔄 Data Flow: Local → S3 → E2B sandbox (consistent paths)\")\n",
    "    print(f\"  🤖 Agent Integration: Real API data → E2B analysis → Unified results\")\n",
    "    print(f\"  🌐 Environment Variable: All components use S3_MOUNT_DIR for consistency\")\n",
    "    \n",
    "    # Usage guidance\n",
    "    print(\"\\n🔗 Usage Instructions:\")\n",
    "    print(\"  1. Set project ID: export CURRENT_PROJECT_ID='your_project'\")\n",
    "    print(\"  2. Enable S3 mount: export S3_MOUNT_ENABLED='true'\")\n",
    "    print(f\"  3. Configure mount path: export S3_MOUNT_DIR='{configured_mount_dir}'\")\n",
    "    print(\"  4. BaseDataToolkit will auto-create: $S3_MOUNT_DIR/your_project/{toolkit_name}/\")\n",
    "    print(\"  5. AI analysis results go to: $S3_MOUNT_DIR/your_project/results/\")\n",
    "    print(\"  6. All data syncs automatically across environments\")\n",
    "    print(\"  7. Environment variable ensures consistency across all components\")\n",
    "    \n",
    "    # Troubleshooting\n",
    "    if passed < total:\n",
    "        print(\"\\n🔧 Troubleshooting:\")\n",
    "        if not globals().get('mount_success', False):\n",
    "            print(\"  📁 Mount issues: E2B template lacks proper S3 mount or AWS credentials\")\n",
    "            print(\"    ◦ Root cause: Template built without AWS build arguments\")\n",
    "            print(\"    ◦ Solution: Rebuild template with: ./setup.sh --e2b-only\")\n",
    "        if not globals().get('structure_success', False):\n",
    "            print(\"  🗂️  Structure issues: Check write permissions to mount directory\")\n",
    "        if not globals().get('analysis_success', False):\n",
    "            print(\"  📊 Analysis issues: Verify pandas/numpy are available in E2B template\")\n",
    "        if not globals().get('crypto_agent_success', False):\n",
    "            print(\"  🤖 Agent issues: Check crypto toolkit imports and E2B agent configuration\")\n",
    "        \n",
    "        print(\"\\n  🛠️  Root Cause & Fix:\")\n",
    "        print(\"    ❌ PROBLEM: E2B template was built without AWS credentials as build arguments\")\n",
    "        print(\"    ✅ SOLUTION: Rebuild E2B template with proper AWS credentials:\")\n",
    "        print(\"       cd docker/e2b-sandbox\")\n",
    "        print(\"       e2b template build \\\\\")\n",
    "        print(\"         --build-arg AWS_ACCESS_KEY_ID='your_key' \\\\\")\n",
    "        print(\"         --build-arg AWS_SECRET_ACCESS_KEY='your_secret' \\\\\")\n",
    "        print(\"         --build-arg S3_BUCKET_NAME='your_bucket' \\\\\")\n",
    "        print(f\"         --build-arg S3_MOUNT_DIR='{configured_mount_dir}' \\\\\")\n",
    "        print(\"         --name sentient-e2b-s3 --force\")\n",
    "        print()\n",
    "        print(\"    Or run automated fix: ./setup.sh --e2b-only\")\n",
    "        \n",
    "        # Show temporary workaround if available\n",
    "        if 'discovered_mount_dir' in globals():\n",
    "            print(f\"\\n  🔧 TEMPORARY WORKAROUND:\")\n",
    "            print(f\"    Update .env with working mount: S3_MOUNT_DIR={globals()['discovered_mount_dir']}\")\n",
    "\n",
    "print_test_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Next Steps\n",
    "\n",
    "### ✅ If all tests passed:\n",
    "Your unified mount directory system is working perfectly! You can now:\n",
    "\n",
    "1. **Use in production**: Data saved in any environment will be visible in all others\n",
    "2. **Run AI agents**: They will automatically use the correct project structure\n",
    "3. **Scale across environments**: Local development → Docker deployment → E2B execution\n",
    "\n",
    "### ⚠️ If some tests failed:\n",
    "1. **Check the error messages** in the test outputs above\n",
    "2. **Verify E2B template**: Ensure `/data/sentient` is properly mounted\n",
    "3. **Check S3 setup**: Verify AWS credentials and bucket permissions\n",
    "4. **Rebuild template**: `cd docker/e2b-sandbox && e2b template build -n sentient-e2b-s3`\n",
    "\n",
    "### 🎯 Production Usage:\n",
    "\n",
    "```bash\n",
    "# Set your project context\n",
    "export CURRENT_PROJECT_ID=\"crypto_analysis_2024\"\n",
    "export S3_MOUNT_ENABLED=\"true\"\n",
    "\n",
    "# Run your agents - they'll automatically create:\n",
    "# /data/sentient/crypto_analysis_2024/binance_toolkit/\n",
    "# /data/sentient/crypto_analysis_2024/coingecko_toolkit/\n",
    "# /data/sentient/crypto_analysis_2024/results/\n",
    "```\n",
    "\n",
    "### 📁 Expected Final Structure:\n",
    "```\n",
    "/data/sentient/\n",
    "├── crypto_analysis_2024/\n",
    "│   ├── binance_toolkit/          # Real API data\n",
    "│   │   ├── BTCUSDT_data.parquet\n",
    "│   │   └── current_prices.json\n",
    "│   ├── coingecko_toolkit/        # Market data\n",
    "│   ├── results/                  # AI analysis outputs\n",
    "│   │   ├── market_analysis.json\n",
    "│   │   ├── trading_signals.csv\n",
    "│   │   └── risk_assessment.png\n",
    "│   └── ...\n",
    "└── other_projects/\n",
    "```\n",
    "\n",
    "### 🎉 Benefits Achieved:\n",
    "- ✅ **Unified file paths** across all environments\n",
    "- ✅ **Automatic data sync** via S3 mount\n",
    "- ✅ **Project isolation** with clear directory structure\n",
    "- ✅ **Cross-environment persistence** for AI workflows\n",
    "- ✅ **Scalable architecture** for production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
