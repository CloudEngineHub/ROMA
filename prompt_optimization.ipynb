{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bca8d96",
   "metadata": {},
   "source": [
    "### Enable logging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fde04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm.litellm_core_utils import logging_worker\n",
    "from roma_dspy.utils import log_async_execution\n",
    "import dspy\n",
    "dspy.settings.provide_traceback = True\n",
    "import asyncio\n",
    "dspy.provide_traceback = True\n",
    "import os\n",
    "os.environ[\"LITELLM_LOG\"] = \"ERROR\"\n",
    "\n",
    "dspy.disable_litellm_logging()\n",
    "def _run_logging_inline(async_coroutine):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        asyncio.run(async_coroutine)\n",
    "    else:\n",
    "        loop.create_task(async_coroutine)\n",
    "\n",
    "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "# mlflow.set_experiment(\"dspy_experiment_test\")\n",
    "# mlflow.dspy.autolog(\n",
    "#     # Log the optimization progress\n",
    "#     log_compiles=True,\n",
    "#     # Log the evaluation results\n",
    "#     log_evals=True,\n",
    "#     # Log traces from module executions\n",
    "#     log_traces=True\n",
    "# )\n",
    "\n",
    "# log_async_execution(verbose=True)  # DEBUG level\n",
    "# import logging\n",
    "# for name in [\"openai\", \"openai._base_client\", \"httpx\", \"httpcore\"]:\n",
    "#     logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "logging_worker.GLOBAL_LOGGING_WORKER.start = lambda: None\n",
    "logging_worker.GLOBAL_LOGGING_WORKER.enqueue = _run_logging_inline\n",
    "logging_worker.GLOBAL_LOGGING_WORKER.ensure_initialized_and_enqueue = _run_logging_inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32398c67",
   "metadata": {},
   "source": [
    "### Configuring DSPy stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37adeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy \n",
    "from prompt_optimization.seed_prompts import ATOMIZER_PROMPT, PLANNER_PROMPT, AGGREGATOR_PROMPT, ATOMIZER_DEMOS, PLANNER_DEMOS\n",
    "from roma_dspy import RecursiveSolverModule, RecursiveSolverFactory, Executor, Atomizer, Planner, Aggregator\n",
    "\n",
    "executor_lm = dspy.LM(\"cerebras/gpt-oss-120b\", temperature=0.5, max_tokens=128000, cache=False)\n",
    "atomizer_lm = dspy.LM(\"cerebras/qwen-3-235b-a22b-instruct-2507\", temperature=0.5, max_tokens=128000,  cache=False)\n",
    "planner_lm = dspy.LM(\"cerebras/qwen-3-235b-a22b-instruct-2507\", temperature=0.5, max_tokens=128000, cache=False)\n",
    "aggregator_lm = dspy.LM(\"cerebras/gpt-oss-120b\", temperature=0.5, max_tokens=128000, cache=False)\n",
    "\n",
    "# Initialize modules\n",
    "atomizer = Atomizer(lm=atomizer_lm)\n",
    "planner = Planner(lm=planner_lm)\n",
    "executor = Executor(lm=executor_lm)\n",
    "aggregator = Aggregator(lm=aggregator_lm)\n",
    "\n",
    "#Add few-shot examples + prompts\n",
    "atomizer.signature.instructions = ATOMIZER_PROMPT\n",
    "atomizer._predictor.predict.demos.extend(ATOMIZER_DEMOS)\n",
    "planner.signature.instructions = PLANNER_PROMPT\n",
    "planner._predictor.predict.demos.extend(PLANNER_DEMOS)\n",
    "aggregator.signature.instructions = AGGREGATOR_PROMPT\n",
    "\n",
    "# Create solver\n",
    "solver = RecursiveSolverFactory(\n",
    "    atomizer,\n",
    "    planner,\n",
    "    executor,\n",
    "    aggregator,\n",
    "    max_depth=1,\n",
    "    enable_logging=False\n",
    ")\n",
    "\n",
    "dspy_module = RecursiveSolverModule(solver_factory=solver, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53270a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_optimization.datasets import load_aimo_datasets\n",
    "\n",
    "train_set, val_set, test_set = load_aimo_datasets(\n",
    "    train_size=32,\n",
    "    val_size=8,\n",
    "    test_size=8,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c09ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_optimization import ComponentJudge, MetricWithFeedback\n",
    "\n",
    "judge = ComponentJudge()\n",
    "metric = MetricWithFeedback(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde0b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "optimizer = GEPA(\n",
    "    metric=metric,\n",
    "    # auto=\"light\",\n",
    "    component_selector=\"round_robin\",\n",
    "    max_metric_calls=225,\n",
    "    add_format_failure_as_feedback=True,\n",
    "    num_threads=12,\n",
    "    track_stats=True,\n",
    "    log_dir=\"logs/aime_test\",\n",
    "    use_wandb=True,\n",
    "    wandb_init_kwargs={\"project\": \"aime_test\"},\n",
    "    reflection_minibatch_size=8,\n",
    "    reflection_lm=dspy.LM(model=\"openrouter/anthropic/claude-sonnet-4.5\", temperature=.75, max_tokens=128000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 11:18:01 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 225 metric calls of the program. This amounts to 5.62 full evals on the train+val set.\n",
      "2025/10/09 11:18:01 INFO dspy.teleprompt.gepa.gepa: Using 8 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget. GEPA requires you to provide the smallest valset that is just large enough to match your downstream task distribution, while providing as large trainset as possible.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msalzubi\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/salahalzubi/cursor_projects/ROMA-DSPy/wandb/run-20251009_111802-3qtbfp2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/salzubi/aime_test/runs/3qtbfp2y' target=\"_blank\">charmed-violet-22</a></strong> to <a href='https://wandb.ai/salzubi/aime_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/salzubi/aime_test' target=\"_blank\">https://wandb.ai/salzubi/aime_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/salzubi/aime_test/runs/3qtbfp2y' target=\"_blank\">https://wandb.ai/salzubi/aime_test/runs/3qtbfp2y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [dspy, litellm, openai] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Weave is installed but not imported. Add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "GEPA Optimization:   0%|          | 0/225 [00:00<?, ?rollouts/s]2025/10/09 11:18:02 INFO dspy.teleprompt.gepa.gepa: Loading gepa state from run dir\n",
      "2025/10/09 11:18:02 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Base program full valset score: 0.25\n",
      "2025/10/09 11:18:02 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Selected program 4 score: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.00 / 8 (100.0%): 100%|██████████| 8/8 [00:20<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 11:18:23 INFO dspy.evaluate.evaluate: Average Metric: 8 / 8 (100.0%)\n",
      "2025/10/09 11:18:23 INFO dspy.teleprompt.gepa.gepa: Iteration 10: All subsample scores perfect. Skipping.\n",
      "2025/10/09 11:18:23 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Reflective mutation did not propose a new candidate\n",
      "GEPA Optimization:  92%|█████████▏| 208/225 [00:20<00:01, 10.20rollouts/s]2025/10/09 11:18:23 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Selected program 4 score: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Metric: 4.00 / 8 (50.0%): 100%|██████████| 8/8 [01:20<00:00, 10.00s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 11:19:43 INFO dspy.evaluate.evaluate: Average Metric: 4 / 8 (50.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback: None\n",
      "Score: 0\n",
      "Feedback: None\n",
      "Score: 0\n",
      "Feedback: None\n",
      "Score: 1\n",
      "Feedback: None\n",
      "Score: 0\n",
      "Feedback: None\n",
      "Score: 1\n",
      "Feedback: None\n",
      "Score: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 11:23:05 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Proposed new text for planner._predictor.predict: You are a mathematical problem decomposition assistant. Your task is to break down complex competition mathematics problems into structured, executable subtasks.\n",
      "\n",
      "**Input Format:**\n",
      "- You will receive a mathematical problem statement in the `goal` field\n",
      "- The problem will typically be from mathematical competitions (AMC, AIME, IMO, etc.)\n",
      "\n",
      "**Your Task:**\n",
      "Generate a complete problem decomposition consisting of:\n",
      "\n",
      "1. **Reasoning Section**: \n",
      "   - Analyze the problem structure and mathematical domain\n",
      "   - Identify key insights, theorems, or techniques needed\n",
      "   - Outline the overall solution strategy\n",
      "   - Explain how subtasks will fit together\n",
      "   - Note any important mathematical properties or constraints\n",
      "   - Work through key derivations and intermediate steps in detail\n",
      "   - For problems involving sequences, recurrences, or periodic behavior, explicitly compute initial terms to detect patterns\n",
      "   - For problems requiring counting or enumeration, identify the underlying combinatorial structure\n",
      "   - Show all algebraic manipulations and simplifications explicitly\n",
      "   - Verify solutions by substituting back into original equations when possible\n",
      "\n",
      "2. **Subtasks List**:\n",
      "   Each subtask must include:\n",
      "   - `goal`: Clear, specific objective (what to accomplish)\n",
      "   - `task_type`: One of:\n",
      "     - `THINK`: Mathematical reasoning, deriving relationships, applying theorems, setting up equations, analyzing structures, algebraic manipulation\n",
      "     - `CODE_INTERPRET`: Numerical computation, iteration, solving equations, simulating recurrences, counting solutions, verifying calculations\n",
      "     - `WRITE`: Formatting final answer, expressing results\n",
      "   - `dependencies`: List of subtask indices this depends on (use string indices like '0', '1')\n",
      "   - `result`: Always None (placeholder)\n",
      "   - `context_input`: Description of what information/results flow from dependency tasks (or None if no dependencies)\n",
      "\n",
      "3. **Dependencies Graph**: \n",
      "   - Dictionary mapping each subtask index to its list of dependencies\n",
      "   - Ensures proper execution order\n",
      "\n",
      "**Key Guidelines:**\n",
      "\n",
      "- **Logical Flow**: Subtasks should progress from problem understanding → setup → computation → final answer\n",
      "- **Specificity**: Each subtask goal should be concrete and actionable\n",
      "- **Dependencies**: Clearly track which results feed into later subtasks via context_input\n",
      "- **Mathematical Rigor**: Include steps for verifying conditions, checking edge cases, and validating results\n",
      "- **Task Types**: \n",
      "  - Use THINK for conceptual work (setting up equations, applying geometric properties, modular arithmetic reasoning, deriving recurrences, analyzing periodicity, algebraic simplification)\n",
      "  - Use CODE_INTERPRET for computational work (solving systems, iterating over cases, numerical integration, simulating sequences, counting solutions to Diophantine equations, numerical verification)\n",
      "  - Use WRITE for final formatting (reducing fractions, computing m+n, expressing in required form)\n",
      "\n",
      "- **Domain-Specific Patterns**:\n",
      "  - For Diophantine equations: reduce via modular arithmetic to constrain variables, then iterate or use generating functions\n",
      "  - For geometric problems: set up coordinates, use symmetry, apply circle theorems or trigonometry, consider vector methods, use complex numbers for cyclic configurations\n",
      "  - For combinatorial problems: identify periodicity/cycles, use generating functions or inclusion-exclusion, recognize standard structures (ultrafilters, intersecting families), compute independent sets in graphs\n",
      "  - For probability: define sample space, characterize favorable outcomes, use dynamic programming for optimal strategies\n",
      "  - For recurrence relations: compute initial terms explicitly to detect periodicity, then use the period to count efficiently over large ranges\n",
      "  - For polynomial coefficients: use cyclotomic polynomial factorizations, generating function expansions, or roots of unity filters\n",
      "  - For problems involving equal angles or concurrent lines: consider trigonometric Ceva, coordinate geometry, or complex number representations\n",
      "  - For cyclic/rotation problems: use periodicity with lcm of cycle lengths, apply inclusion-exclusion for union of events, exploit symmetry to group cases\n",
      "\n",
      "- **Context Flow**: The context_input should explicitly state what values, equations, or insights from previous subtasks are being used\n",
      "\n",
      "- **Detailed Reasoning**: In the reasoning section, show key algebraic manipulations, compute several initial values for sequences, verify small cases, and explain why certain approaches work\n",
      "\n",
      "- **Special Techniques**:\n",
      "  - For maximal/extremal set families: recall results like Erdős–Ko–Rado theorem or properties of ultrafilters on finite sets\n",
      "  - For expressions involving x^n - 1: use cyclotomic polynomial factorization and Möbius inversion\n",
      "  - For generating functions with products of (x^k - 1) terms: expand as geometric series for |x| < 1, then extract coefficients by solving resulting Diophantine equations\n",
      "  - For angle conditions in geometry: use dot products, cross products, and tan²θ = |u × v|² / (u · v)² for vectors u, v\n",
      "  - For distance problems in cyclic configurations: use complex numbers with |e^{iα} - e^{iβ}| = 2|sin((α-β)/2)|\n",
      "  - For product-to-sum identities: sin A sin B = ½[cos(A-B) - cos(A+B)]\n",
      "  - For rhombus/parallelogram properties: sum of distances from interior point to parallel sides equals the distance between those sides (for tangential quadrilaterals)\n",
      "  - For independent sets in graphs: use recurrence relations, identify cycle structure, apply inclusion-exclusion for constraint satisfaction\n",
      "  - For problems with rotational symmetry: exploit lcm-based periodicity, count within one fundamental period, then scale appropriately\n",
      "\n",
      "- **Verification Steps**: \n",
      "  - Always include subtasks to verify intermediate results when possible\n",
      "  - Check boundary conditions and special cases\n",
      "  - Substitute solutions back into original constraints\n",
      "  - Verify units, signs, and reasonableness of answers\n",
      "\n",
      "**Output Format:**\n",
      "Provide three sections exactly as shown in the examples:\n",
      "1. ### reasoning (detailed mathematical analysis with explicit calculations)\n",
      "2. ### subtasks (Python list of SubTask objects)\n",
      "3. ### dependencies_graph (Python dictionary)\n",
      "2025/10/09 11:23:43 INFO dspy.evaluate.evaluate: Average Metric: 5 / 8 (62.5%)\n",
      "2025/10/09 11:23:43 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New subsample score 5 is better than old score 4. Continue to full eval and add to candidate pool.\n",
      "2025/10/09 11:24:15 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n",
      "2025/10/09 11:24:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n",
      "2025/10/09 11:25:25 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n",
      "✗ [PLANNER] Failed | Error: Adapter JSONAdapter failed to parse the LM response. \n",
      "\n",
      "LM Response: {0,1} \n",
      "\n",
      "Expected to find output \n",
      "2025/10/09 11:26:17 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n",
      "2025/10/09 11:28:10 INFO dspy.evaluate.evaluate: Average Metric: 7 / 8 (87.5%)\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Full valset score for new program: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Full train_val score for new program: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Individual valset scores for new program: [0, 1, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New valset pareto front scores: [0, 1, 1, 1, 1, 1, 1, 1]\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Full valset pareto front score: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Updated valset pareto front programs: [{0, 1, 2, 3, 4, 5, 6, 7}, {0, 1, 2, 3, 4, 6, 7}, {1, 2, 3, 4, 5, 6, 7}, {0, 1, 2, 3, 4, 6, 7}, {1, 2, 4, 5, 7}, {1, 2, 3, 4, 5, 6, 7}, {1, 2, 3, 4, 5, 6, 7}, {1, 4, 5, 6, 7}]\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Best valset aggregate score so far: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Best program as per aggregate score on train_val: 1\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Best program as per aggregate score on valset: 1\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Best score on valset: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Best score on train_val: 0.875\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Linear pareto front program index: 1\n",
      "2025/10/09 11:28:10 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New program candidate index: 7\n",
      "GEPA Optimization:  92%|█████████▏| 208/225 [10:08<00:49,  2.92s/rollouts]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>base_program_full_valset_score</td><td>▁</td></tr><tr><td>best_program_as_per_agg_score</td><td>▁</td></tr><tr><td>best_program_as_per_agg_score_valset</td><td>▁</td></tr><tr><td>best_score_on_train_val</td><td>▁</td></tr><tr><td>best_score_on_valset</td><td>▁</td></tr><tr><td>best_valset_agg_score</td><td>▁</td></tr><tr><td>iteration</td><td>▁▅█</td></tr><tr><td>linear_pareto_front_program_idx</td><td>▁</td></tr><tr><td>new_program_idx</td><td>▁</td></tr><tr><td>new_subsample_score</td><td>▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>base_program_full_valset_score</td><td>0.25</td></tr><tr><td>best_program_as_per_agg_score</td><td>1</td></tr><tr><td>best_program_as_per_agg_score_valset</td><td>1</td></tr><tr><td>best_score_on_train_val</td><td>0.875</td></tr><tr><td>best_score_on_valset</td><td>0.875</td></tr><tr><td>best_valset_agg_score</td><td>0.875</td></tr><tr><td>iteration</td><td>11</td></tr><tr><td>linear_pareto_front_program_idx</td><td>1</td></tr><tr><td>new_instruction_planner._predictor.predict</td><td>You are a mathematic...</td></tr><tr><td>new_program_idx</td><td>7</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-violet-22</strong> at: <a href='https://wandb.ai/salzubi/aime_test/runs/3qtbfp2y' target=\"_blank\">https://wandb.ai/salzubi/aime_test/runs/3qtbfp2y</a><br> View project at: <a href='https://wandb.ai/salzubi/aime_test' target=\"_blank\">https://wandb.ai/salzubi/aime_test</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251009_111802-3qtbfp2y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 11:28:23 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n",
      "2025/10/09 11:29:45 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=128000. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.5)  if the reason for truncation is repetition.\n"
     ]
    }
   ],
   "source": [
    "optimized_program = optimizer.compile(\n",
    "    dspy_module,\n",
    "    trainset=train_set,\n",
    "    valset=val_set,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6efd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "atomizer._predictor.predict = Predict(StringSignature(goal -> reasoning, is_atomic, node_type\n",
       "    instructions='Signature for task atomization.'\n",
       "    goal = Field(annotation=str required=True description='Task to atomize' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'Task to atomize', 'prefix': 'Goal:'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    is_atomic = Field(annotation=bool required=True description='True if task can be executed directly' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'True if task can be executed directly', 'prefix': 'Is Atomic:'})\n",
       "    node_type = Field(annotation=NodeType required=True description='Type of node to process (PLAN or EXECUTE)' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'Type of node to process (PLAN or EXECUTE)', 'prefix': 'Node Type:'})\n",
       "))\n",
       "planner._predictor.predict = Predict(StringSignature(goal -> reasoning, subtasks, dependencies_graph\n",
       "    instructions=\"You are a mathematical problem decomposition assistant. Your task is to break down complex competition mathematics problems into structured, executable subtasks.\\n\\n**Input Format:**\\n- You will receive a mathematical problem statement in the `goal` field\\n- The problem will typically be from mathematical competitions (AMC, AIME, IMO, etc.)\\n\\n**Your Task:**\\nGenerate a complete problem decomposition consisting of:\\n\\n1. **Reasoning Section**: \\n   - Analyze the problem structure and mathematical domain\\n   - Identify key insights, theorems, or techniques needed\\n   - Outline the overall solution strategy\\n   - Explain how subtasks will fit together\\n   - Note any important mathematical properties or constraints\\n\\n2. **Subtasks List**:\\n   Each subtask must include:\\n   - `goal`: Clear, specific objective (what to accomplish)\\n   - `task_type`: One of:\\n     - `THINK`: Mathematical reasoning, deriving relationships, applying theorems\\n     - `CODE_INTERPRET`: Numerical computation, iteration, solving equations\\n     - `WRITE`: Formatting final answer, expressing results\\n   - `dependencies`: List of subtask indices this depends on (use string indices like '0', '1')\\n   - `result`: Always None (placeholder)\\n   - `context_input`: Description of what information/results flow from dependency tasks (or None if no dependencies)\\n\\n3. **Dependencies Graph**: \\n   - Dictionary mapping each subtask index to its list of dependencies\\n   - Ensures proper execution order\\n\\n**Key Guidelines:**\\n\\n- **Logical Flow**: Subtasks should progress from problem understanding → setup → computation → final answer\\n- **Specificity**: Each subtask goal should be concrete and actionable\\n- **Dependencies**: Clearly track which results feed into later subtasks via context_input\\n- **Mathematical Rigor**: Include steps for verifying conditions, checking edge cases, and validating results\\n- **Task Types**: \\n  - Use THINK for conceptual work (setting up equations, applying geometric properties, modular arithmetic reasoning)\\n  - Use CODE_INTERPRET for computational work (solving systems, iterating over cases, numerical integration)\\n  - Use WRITE for final formatting (reducing fractions, computing m+n, expressing in required form)\\n\\n- **Domain-Specific Patterns**:\\n  - For Diophantine equations: reduce via modular arithmetic, then iterate over constrained variables\\n  - For geometric problems: set up coordinates, use symmetry, apply circle theorems or trigonometry\\n  - For combinatorial problems: identify periodicity/cycles, use generating functions or inclusion-exclusion\\n  - For probability: define sample space, characterize favorable outcomes, integrate or count\\n\\n- **Context Flow**: The context_input should explicitly state what values, equations, or insights from previous subtasks are being used\\n\\n**Output Format:**\\nProvide three sections exactly as shown in the examples:\\n1. ### reasoning (detailed mathematical analysis)\\n2. ### subtasks (Python list of SubTask objects)\\n3. ### dependencies_graph (Python dictionary)\"\n",
       "    goal = Field(annotation=str required=True description='Task that needs to be decomposed into subtasks through planner' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'Task that needs to be decomposed into subtasks through planner', 'prefix': 'Goal:'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    subtasks = Field(annotation=List[SubTask] required=True description='List of generated subtasks from planner' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'List of generated subtasks from planner', 'prefix': 'Subtasks:'})\n",
       "    dependencies_graph = Field(annotation=Union[Dict[str, List[str]], NoneType] required=False default=None description='Task dependency mapping, should map subtask ids by int only' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'Task dependency mapping, should map subtask ids by int only', 'prefix': 'Dependencies Graph:'})\n",
       "))\n",
       "executor._predictor.predict = Predict(StringSignature(goal, context -> reasoning, output, sources\n",
       "    instructions='Executor execution result.\\n\\nContains the output of atomic task execution.'\n",
       "    goal = Field(annotation=str required=True description='Task that needs to be executed' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'Task that needs to be executed', 'prefix': 'Goal:'})\n",
       "    context = Field(annotation=Union[str, NoneType] required=False default=None description='Context from dependent tasks (left-to-right flow)' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'Context from dependent tasks (left-to-right flow)', 'prefix': 'Context:'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    output = Field(annotation=str required=True description='Execution result' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'Execution result', 'prefix': 'Output:'})\n",
       "    sources = Field(annotation=Union[List[str], NoneType] required=False default_factory=list description='Information sources used' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'Information sources used', 'prefix': 'Sources:'})\n",
       "))\n",
       "aggregator._predictor.predict = Predict(StringSignature(original_goal, subtasks_results -> reasoning, synthesized_result\n",
       "    instructions='Aggregator synthesis result.\\n\\nContains the synthesis of multiple subtask results into a cohesive output.'\n",
       "    original_goal = Field(annotation=str required=True description='Original goal of the task' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'Original goal of the task', 'prefix': 'Original Goal:'})\n",
       "    subtasks_results = Field(annotation=List[SubTask] required=True description='List of subtask results to synthesize' json_schema_extra={'__dspy_field_type': 'input', 'desc': 'List of subtask results to synthesize', 'prefix': 'Subtasks Results:'})\n",
       "    reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "    synthesized_result = Field(annotation=str required=True description='Final synthesized output' json_schema_extra={'__dspy_field_type': 'output', 'desc': 'Final synthesized output', 'prefix': 'Synthesized Result:'})\n",
       "))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921ad5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
